<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分布式 on LHR</title>
    <link>https://chinalhr.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/</link>
    <description>Recent content in 分布式 on LHR</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 08 Sep 2018 23:02:55 +0800</lastBuildDate>
    
	<atom:link href="https://chinalhr.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>基于SnowFlake的分布式UID生成服务</title>
      <link>https://chinalhr.github.io/post/uidgenerateserver/</link>
      <pubDate>Sat, 08 Sep 2018 23:02:55 +0800</pubDate>
      
      <guid>https://chinalhr.github.io/post/uidgenerateserver/</guid>
      <description>背景 公司原本订单ID的生成是基于Java Random与Hash算法的单机UID生成策略，但是后端系统的生成环境是基于SLB，Nginx与Java多进程的分布式环境，导致出现了UID 碰撞,产生线上问题。现准备基于Twitter的SnowFlake，将UID生成独立出来做一个统一的全局ID生成服务。
SnowFlake概述 SnowFlake算法用来生成64位的ID，刚好可以用long整型存储，能够用于分布式系统中生产唯一的ID， 并且生成的ID有大致的顺序。 生成的64位ID可以分成5个部分：
0 - 41位时间戳 - 5位数据中心标识 - 5位机器标识 - 12位序列号  5位数据中心标识跟5位机器标识这样的分配仅仅是当前实现中分配的，可以按其它的分配比例分配，如10位机器标识，不需要数据中心标识&amp;hellip;
 1位标识部分，在java中由于long的最高位是符号位，正数是0，负数是1，一般生成的ID为正数，所以为0 41位时间戳部分，这个是毫秒级的时间，一般实现上不会存储当前的时间戳，而是时间戳的差值（当前时间-固定的开始时间），这样可以使产生的ID从更小值开始；41位的时间戳可以使用69年，41位可以表示241−1个毫秒的值，转化成单位年则是(241−1)/(1000∗60∗60∗24∗365)=69年 10位节点部分，Twitter实现中使用前5位作为数据中心标识，后5位作为机器标识，可以部署1024个节点(5位可支持2^5 = 0~31整型,32*32=1024节点)； 12位序列号部分，支持同一毫秒内同一个节点可以生成4096个ID(2^12=4096)；  简单实现 结构 0 - 41位时间戳 - 10位机器标识 - 12位序列号  集群环境下机器标识获取  IP后三位
启动系统的时候获取IP地址(或者截取IP后三位)作为机器标识(0~255)。缺陷：会浪费256~1024位的机器标识
 Redis Set
①在服务启动时通过Redis setnx ,for 1024对机器标识进行获取，如果Key不存在则设置当前IP地址，并使用key作为当前服务的机器标识。
②在服务运行过程中使用ScheduleThreadPool或者DeamonThread为当前服务的Redis Key续时,避免其他服务获取到相同的机器标识
  其他 1. 服务鉴权|IP白名单 2. 服务限流  基于Netty的分布式UID生成服务 https://github.com/ChinaLHR/Gungnir/tree/master/gungnir-uid-generate
参考  http://www.wolfbe.com/detail/201701/386.html http://www.wolfbe.com/detail/201611/381.html https://segmentfault.com/a/1190000011282426 </description>
    </item>
    
    <item>
      <title>分布式锁实现(基于Redis|Zookeeper)</title>
      <link>https://chinalhr.github.io/post/distributedlocks/</link>
      <pubDate>Sun, 15 Jul 2018 18:38:26 +0800</pubDate>
      
      <guid>https://chinalhr.github.io/post/distributedlocks/</guid>
      <description>分布式锁  关于： 当多个进程(集群)不在同一个系统中，用分布式锁控制多个进程对资源的访问 分布式锁对比线程锁：线程锁可以利用共享堆内存标记存储位置达到目的；分布式锁因为进程不在同一台机器上，需要采取对所有进程可见的中间件标记存储位置达到目的 问题：需要考虑锁对所有进程可见，锁与进程间网络问题 实现：基于数据库，缓存，分布式协调中间件(Zookeeper|Chubby)  设计  可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行 设计为可重入锁，避免死锁 阻塞与非阻塞获取锁方式  基于Redis实现  思路  加锁： 使用set命令(key，value，time，NX) 释放锁:使用del(key)  问题  线程a执行时间超过锁wait时间，导致锁自动释放，①线程b获取了锁和线程a并发访问代码块，②线程a执行结束释放了线程b的锁：  ①避免并发问题：给获取锁的线程开启一个守护线程，给快超时的锁增加wait时间 ②避免锁误删：加锁的时候把当前的线程ID当做value，并在删除之前验证key对应的value是不是自己线程的ID(使用lua脚本确保判断和释放锁的原子性)    基于Zookeeper实现</description>
    </item>
    
  </channel>
</rss>