[
    
    
    
        
            {
                "id": 0,
                "href": "https://chinalhr.github.io/about/",
                "title": "About",
                "section": "page",
                "date" : "0001.01.01",
                "body": " 关于本博客\n 关于博客 本博客基于Github Pages,Hugo搭建,使用了harbor主题，感谢开源平台与开源软件开发者。本博客主要用于记录软件开发过程中的分析总结,书籍阅读笔记。\n技术标签  Java、Kotlin、Go Cloud Native、Distributed System Linux、Docker、Kubernetes Database、Middleware Agile、DevOps  联系我  Github : https://github.com/ChinaLHR Email : 13435500980@163.com "
            }
    
        ,
            {
                "id": 1,
                "href": "https://chinalhr.github.io/post/gitops-argo-cd/",
                "title": "GitOps实践，基于Gitlab CI、Kustomize 与Argo CD",
                "section": "post",
                "date" : "2022.07.20",
                "body": " GitOps总结与实践\n GitOps 什么是GitOps GitOps是Weaveworks公司于2017年首创的一种进行Kubernetes集群管理和应用交付的方式。GitOps通过使用Git作为声明性基础设施和应用程序的单一事实来源进行工作。 GitOps的核心是将应用的声明性基础架构描述、应用源码与自动化流程存放在Git Repository中，将Git作为交付流水线的核心。GitOps会Diff Repository中声明信息与Kubernetes集群中实际运行的内容之间的差异，其实现的Kubernetes Operator会根据情况进行自动更新或回滚集群；通过将Git作为交付流水线的核心，开发人员可以使用push、pull request等Git操作，以加速和简化Kubernetes集群应用程序的部署和操作任务。\nGitOps是用于构建云原生应用程序的操作模型\n GitOps为Kubernetes或其他云原生技术的操作模型，提供了一组最佳实践，统一容器化集群和应用程序基于Git部署、管理和监控。 优化开发人员管理应用程序的方式，包括应用 CI/CD pipeline 和 Git Flow 的操作和开发。  使用GitOps的前置条件   声明式描述整个应用系统，并使用Git进行版本化控制 任何能够被描述的内容都必须存储于Git仓库中，如Kubernetes资源对象描述；声明性意味着配置由一组事实而不是一组指令来保证，基于Git对应用程序的声明。描述进行版本控制后，就有了单一的事实来源，应用程序可以轻松部署到Kubernetes，并进行升级、回滚，而且当容灾处理时也可以快速、可靠地复制基础设施。\n  Git Approved Changes后自动更新 允许对声明信息的变更，自动化更新于集群中，并且不需要集群凭证来更改系统，在隔离的环境中使用GitOps，将状态定义位于外部，使得可以将\u0026quot;所做的事情\u0026quot;和\u0026quot;将要如何做的事情\u0026quot;分开。\n  通过GitOps确保正确性以及分歧报警 一旦系统状态被声明并受版本控制，可以通过GitOps提供的工具对真实的系统状态与版本控制中声明的系统状态进行Diff，在真实的系统状态与版本控制中声明的系统状态不一致时进行通知；以及通过GitOps工具支持系统在故障、人为错误时进行自我修复。\n  不可变基础设施、IaC与GitOps  不可变基础设施（Immutable Infrastructure）：不可变基础设施是由Chad Fowler于 2013 年提出的一个构想：在这种模式中任何基础设施的实例一旦创建之后便成为一种只读状态，不可对其进行任何更改。如果需要修改或升级某些实例，唯一的方式就是创建一批新的实例以替换。 传统可变基础设施是将应用打包好部署在不同的机器上，需要确保环境的统一，并通过修改、补丁的方式持续更新应用，随着时间的推移很难再确保所有的机器处于相同的状态；而不可变基础架构，是将整个机器环境打包成一个单一的不可变单元，这个单元包含了整个环境和应用本身，以解决传统可变基础架构的问题。\n  基础架构即代码（IaC）是通过代码（而非手动流程）来管理和置备基础架构的方法。\n 通过使用容器技术可以实现基础设施的不变性，而Kubernetes的声明性容器编排可以实现基础架构即代码。GitOps基于不可变基础设施和IaC，结合Git进行应用系统整个配置文件集版本控制。\nGitOps的工作模式  Git对环境配置进行版本控制  GitOps以Git为核心进行工作，除了应用程序的仓库外，还需要对环境配置的仓库进行版本控制；其中应用程序仓库含应用程序的源码与部署相关的manifests，环境配置仓库包含应用所需基础架构与部署相关的manifests。\n GitOps基于pull的工作模式   GitOps大都基于Pull模式去工作，开发人员推送应用代码到应用仓库后，经过Pull Request合并到目标分支后，触发CI Pipeline，构建推送应用容器镜像与推送更新环境配置仓库；在CD阶段，通过引入Operator，不断观察环境配置库的声明信息并与当前集群的应用环境进行Diff，当存在差异时进行集群应用环境的部署、更新或者还原。\n GitOps基于push的工作模式   GitOps也可以基于Push模式去工作，对比基于Pull的工作模式，Push模式在CI Pipeline完成阶段会去通知Operator进行环境配置信息的拉取与部署、更新。Push模式有着传统CI/CD流程的直观性，对比Pull模式有着更好的实时性与性能，但却缺少了对 端（环境配置信息）到端（集群应用信息）一致性的保证。\nGitOps的优势 通过GitOps，当我们对Git Repository进行更改时，GitOps的自动化交付流水线会自动将变更部署到基础架构中。而且，GitOps还会使用工具将应用程序的实际状态与Git Repository中的声明状态进行比较，以通知集群Git Repository中声明状态与实际环境的Diff。\n通过应用GitOps，基础设施和应用程序代码都有一个“真实来源”，提高了开发团队速度并提高系统可靠性。\n将GitOps理论应用在持续交付流水线上有着深远的好处：\n 提高生产力：具有集成反馈控制回路的自动化持续部署可以显著加快部署时间。 增加开发体验：开发人员可以使用熟悉的版本控制工具（Git）更快速、方便地管理Kubernetes的更新和功能。 更加稳定与可靠：可以使用Git Flow来管理集群应用，而且可以方便地获得审计日志，记录集群应用的更改；借助Git的revert/rollback，可以稳定且可重现地进行回滚、恢复操作。 一致性和标准化：GitOps提供了标准化的模型以进行基础设施、应用程序与Kubernetes的附加更改，可以保持组织中端到端工作流的已执行状态。  Argo CD Argo CD是一个用于Kubernetes的声明式GitOps持续交付工具；Argo CD遵循GitOps模式，使用Git存储库作为定义所需应用程序状态的真实来源, Argo CD支持多种Kubernetes manifests 如kustomize、helm charts、jsonnet files\u0026hellip;。Argo CD可以在指定的目标环境中自动部署所需的应用程序状态，应用程序部署可以跟踪分支、标签的更新或在Git提交时固定到特定版本的清单。\nArgo CD特性  将应用程序自动部署到指定的目标环境 支持多种配置管理/模板工具（Kustomize、Helm、Jsonnet、plain-YAML） 支持管理和部署到多个集群 SSO 集成（OIDC、OAuth2、LDAP、SAML 2.0、GitHub、GitLab、Microsoft、LinkedIn） 用于授权的多租户和 RBAC 策略 基于Git版本控制，支持回滚到Git存储库中提交的任何应用程序状态 持续监控与分析应用资源的健康状况 支持自动或手动（基于Pull/Push模式）触发应用程序同步到所需状态 提供可视化的Web UI与用于自动化与CI集成的CLI Webhook 集成（GitHub、BitBucket、GitLab），以及提供用于自动化的访问令牌 PreSync、Sync、PostSync hooks 以支持复杂的应用程序的部署策略（如blue/green 、canary upgrades） 应用程序事件和API调用的审计跟踪 Prometheus指标  Argo CD架构 Argo CD 通过kubernetes控制器实现，通过持续监控集群中正在运行的应用程序并将当前的状态信息与所需的目标状态（环境配置仓库）进行比较。通过Diff实时状态与目标状态，如果存在差异则应用进入OutOfSync状态；Argo CD会报告和可视化差异，同时提供工具自动或手动的方式将实时状态同步回所需目标状态。在 Git 存储库中对所需目标状态所做的任何修改都可以自动应用并反映在指定的目标环境中。\nArgo CD主要由API Server、Repository Server、Application Controller组成。\nAPI Server Argo CD的 API Server是一个gRPC/REST服务，它公开了Argo CD的Web UI、CLI API 与CI/CD系统使用的API，主要职责为：\n 应用程序管理和状态报告 调用应用程序进行相关的操作，如同步、回滚、自定义操作等 存储库和集群凭证管理(使用K8s secrets进行存储) 对外部身份提供者的身份验证和授权 基于RBAC的角色访问权限控制 Git Webhook 事件的listener/forwarder  Repository Server 存储库服务器是一个内部服务，维护保存应用程序Git存储库的本地缓存，当应用设置如下输入信息时，它负责生成和返回 Kubernetes manifests：\n 存储库URL git revision（commit, tag, branch） 应用程序设置的路径 模板配置：parameters、helm values.yaml  Application Controller 应用程序控制器是一个Kubernetes Controller，它持续监控正在运行的应用程序并将当前的状态信息与所需的目标状态（环境配置仓库）进行比较，对检测到的OutOfSync状态的应用程序选择对应的纠正措施进行纠正。它还负责调用任何用户定义的生命周期事件（PreSync、Sync、PostSync）对应的hooks。\nArgo CD部署 Argo CD支持两种部署方式：多租户（multi-tenant）、核心（core）。\n  多租户（multi-tenant） 多租户模式是安装Argo CD最常见的方式，通常用于为公司内多个开发团队提供服务，并由平台团队进行维护；用户可以使用Web UI或CLI 通过 API Server访问 Argo CD；支持HA与非HA方式进行部署。\n  核心（core） 核心模式适合独立使用Argo CD且不需要多租户特性的集群管理员；核显模式包含更少的组件并且更易于设置，bundle不包含API Server或Web UI，并只安装每个组件的轻量级（非 HA）版本；用户需要 Kubernetes 访问权限来管理 Argo CD。\n  具体的部署方式可以参考：https://argo-cd.readthedocs.io/en/stable/operator-manual/installation/\n使用Kustomize定制/管理Kubernetes资源 关于Kustomize Kustomize是一个通过kustomization文件定制Kubernetes对象的工具；kustomize允许您自定义原始的、无模板的Kubernetes YAML文件以用于多种用途，而原始Kubernetes YAML保持不变且可按原样使用。\nKustomize可以解析并修补Kubernetes API对象，如同make在一个文件中进行声明，如同sed可以发出解析并修补完成的YAML文本。\n可以通过kustomize build命令生成自定义YAML，从1.14版本开始kubectl也支持使用kustomization文件来管理Kubernetes对象，可以直接通过kubectl kustomize命令生成自定义YAML。\n使用Kustomize定制/管理Kubernetes资源 kustomize提供以下功能特性定制/管理Kubernetes应用配置文件  从其他来源生成资源  Kustomize提供secretGenerator和configMapGenerator，可以基于文件或字面值来生成Secret和ConfigMap。如下，从.env文件生成ConfigMap。\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - service.yaml configMapGenerator: - envs: - .env name: echo-config  为资源设置贯穿性（Cross-Cutting）字段 如同模板中的变量，为在项目中所有的Kubernetes对象设置贯穿性字段，如为所有资源设置相同的名字空间；为所有对象添加相同的前缀后缀、标签、注解等。  apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - service.yaml namespace: dev namePrefix: dev- nameSuffix: \u0026#34;-test\u0026#34; commonLabels: app: echo commonAnnotations: sidecar: monitoring resources: - deployment.yaml - service.yaml  组织和定制资源集合 在项目中构造资源集合并将其放到同一个文件或目录中管理。Kustomize提供基于不同文件来组织资源并向其应用补丁或者其他定制的能力。  kustomization.yaml文件的resources字段定义配置中要包含的资源列表，达到资源的组织。\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - service.yaml 补丁文件（Patches）可以用来对资源执行不同的定制。Kustomize通过patchesStrategicMerge和patchesJson6902支持不同的打补丁机制。 patchesStrategicMerge的内容是一个文件路径的列表，其中每个文件都应可解析为策略性合并补丁（Strategic Merge Patch）。\n# deployment replace patch file  apiVersion: apps/v1 kind: Deployment metadata: name: echo spec: replicas: 3 # deployment memory patch file apiVersion: apps/v1 kind: Deployment metadata: name: echo spec: template: spec: containers: - name: echo resources: limits: memory: 512Mi # kustomization file apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - service.yaml patchesStrategicMerge: - deployment_replicas_patch.yaml - deployment_memory_patch.yaml 也可以通过patchesJson6902来使用JSON补丁的能力，对比patchesStrategicMerge的合并机制，patchesJson6902支持对任何资源的任何字段进行修改。\n# deployment replace patch file  - op: replace path: /spec/replicas value: 3 # kustomization file apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - service.yaml patchesJson6902: - target: group: apps version: v1 kind: Deployment name: echo path: deployment_replicas_patch.yaml kustomize也提供了对象字段值注入的功能，例如通过images字段设置新的镜像。\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - deployment.yaml - service.yaml images: - name: echo newName: my.image.registry/echo newTag: 1.1.0 基准（Bases）与覆盖（Overlays） Kustomize中有基准（bases）和覆盖（overlays）的概念，基准是包含kustomization.yaml文件的本地目录或者远程仓库目录，包含一组资源及其相关的定制；覆盖也是一个目录，其中包含将其他kustomization目录当做bases来引用的kustomization.yaml文件。 基准与覆盖是多对多的关系，覆盖可以引用多个基准，基准也可以被多个覆盖引用，提高了kustomize的高度定制性。\n├── base │ ├── deployment.yaml │ ├── kustomization.yaml │ └── service.yaml └── overlays ├── development │ ├── cpu_count.yaml │ ├── kustomization.yaml │ └── replica_count.yaml └── production ├── cpu_count.yaml ├── kustomization.yaml └── replica_count.yaml kustomize的详细用法可以参考：\n https://kubectl.docs.kubernetes.io/guides/introduction/kustomize/ https://kubernetes.io/zh-cn/docs/tasks/manage-kubernetes-objects/kustomization/  Kustomize vs Helm Kustomize对比Helm，可以通过如下表格来对比两者的特性：\n   特征 Helm Kustomize     模板支持 √ ×   覆盖支持 × √   打包支持 √ ×   验证hooks √ ×   回滚支持 √ ×   原生 K8s 集成 × √   声明性 √ √   可见性和透明度 弱 强    对比两者，比较显著的差别是Helm的流程主要基于模板，类似于瀑布式的模板定义-\u0026gt;模板填充-\u0026gt;生成资源文件；Kustomize的流程基于覆盖，类似于迭代式的Base和Overlay独立迭代变更。相比之下，选择使用更契合GitOps 版本化控制思想的Kustomize进行实践。\n基于Gitlab CI、Kustomize、 Argo CD的GitOps实践 前置条件  Docker：使用镜像构建，打包实现基础设施的不变性；实现对不可变基础设施（Immutable Infrastructure）的支持。 Kubernetes：声明性容器编排，实现基础架构即代码（IaC）的支持。 Gitlab：Merge Request与Approval 作为所有基础架构更新的更改机制（MRs），Git Repository实现对源码版本控制支持，- - Gitlab CI作为对GitOps CI阶段的支持。 Kustomize：对Kubernetes资源对象的定制。 Argo CD：声明式GitOps CD支持。  GitOps流程 这里我们选择基于pull的工作模式去实现GitOps，具体流程如下所示： 项目结构 ├── .backup │ ├── ci-base │ │ ├── Dockerfile │ │ └── Makefile │ └── gitlab │ └── note.md ├── Dockerfile ├── .gitignore ├── .gitlab-ci.yml ├── go.mod ├── go.sum ├── .kubernetes │ ├── base │ │ ├── deployment.yaml │ │ ├── kustomization.yaml │ │ └── service.yaml │ └── overlays │ ├── production │ │ ├── deployment_patch.yaml │ │ ├── .env │ │ └── kustomization.yaml │ └── staging │ ├── deployment_patch.yaml │ ├── .env │ └── kustomization.yaml ├── main.go ├── Makefile └── README.md 这里我们为了更简单直观的展示流程，将应用仓库与环境配置仓库合并为一个，使用Golang、Gin简单实现了一个应用服务；其中.backup目录为基础CI镜像的manifests，.kubernetesm目录是基于Kustomize的manifests。\nGitOps流程实践 应用Docker镜像构建 使用makefile定义了三个phony target ，配合Dockerfile实现Docker镜像构建、镜像远程仓库登录与上传镜像\n.PHONY: build-release-image build-release-image: docker build . \\ \t--no-cache \\ \t--force-rm \\ \t-t $(release-image) \\ \t-f Dockerfile .PHONY: login-docker-registry login-docker-registry: docker login -u $(registry-user) -p $(registry-password) $(registry-host) .PHONY: push-release-image push-release-image: docker push $(release-image) FROMgolang:1.18 as buildENV GO111MODULE=on ENV GOPROXY=https://goproxy.cn,directWORKDIR/go/releaseADD . .RUN GOOS=linux CGO_ENABLED=0 GOARCH=amd64 go build -ldflags=\u0026#34;-s -w\u0026#34; -installsuffix cgo -o app main.goFROMscratch as prodCOPY --from=build /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeCOPY --from=build /go/release/app /CMD [\u0026#34;/app\u0026#34;]Kustomize定制声明式Kubernetes资源对象 这里使用Kustomize Base/Overlay来管理Kubernetes多环境资源配置与使用Patches特性来进行迭代更新;\n│ ├── base │ │ ├── deployment.yaml │ │ ├── kustomization.yaml │ │ └── service.yaml │ └── overlays │ ├── production │ │ ├── deployment_patch.yaml │ │ ├── .env │ │ └── kustomization.yaml │ └── staging │ ├── deployment_patch.yaml │ ├── .env │ └── kustomization.yaml # base kustomization.yaml 定制应用的基础kubernetes资源 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization commonLabels: app: saken namespace: default resources: - deployment.yaml - service.yaml # overlay kustomization.yaml 定制应用不同环境（staging/production/...）特定kubernetes资源属性， # 基于configMapGenerator从不同环境的env文件生成不同资源的configMap # 基于patchesStrategicMerge对不同环境的deployment-pod资源进行控制 # 基于Cross-Cutting对不同环境的namespace、name-prefix进行定制 apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - ../../base namePrefix: production- namespace: production configMapGenerator: - envs: - .env name: saken-config patchesStrategicMerge: - deployment_patch.yaml images: - name: my-registry.com/shovel/shovel-gitops newTag: 940638fd88cf1c030e3c02b21f2df738db8544f3 通过kustomize edit 命令，支持我们在CI阶段对基础配置仓库的kustomization文件进行命令式的编辑。\nGitlab CI流程 首先需要配置好CI流程所需要的环境变量，通过Gitlab settings-\u0026gt;CI/CD-\u0026gt;Variables进行配置如下变量\nDOCKER_REGISTRY_HOST DOCKER_REGISTRY_IMAGE DOCKER_REGISTRY_PWD DOCKER_REGISTRY_USER GITLAB_EMAIL GITLAB_ID_RSA GITLAB_REMOTE_URL GITLAB_USERNAME 如下所示，Gitlab-CI流程，定义了两个stage:\n build-publish-image：Docker镜像构建并推送到远程仓库 update-kustomize：更新对应分支的kustomize overlay下kustomization文件，并推送到环境配置仓库  stages: - build-publish-image - update-kustomize build-publish-image: image: ccr.ccs.tencentyun.com/shovel/ci-base:0.0.7 stage: build-publish-image tags: - docker-runner only: - staging - main services: - name: docker:20-dind before_script: - make login-docker-registry registry-user=$DOCKER_REGISTRY_USER registry-password=$DOCKER_REGISTRY_PWD registry-host=$DOCKER_REGISTRY_HOST script: - make build-release-image release-image=$DOCKER_REGISTRY_HOST/$DOCKER_REGISTRY_IMAGE:$CI_COMMIT_SHA - make push-release-image release-image=$DOCKER_REGISTRY_HOST/$DOCKER_REGISTRY_IMAGE:$CI_COMMIT_SHA update-kustomize: image: ccr.ccs.tencentyun.com/shovel/ci-base:0.0.7 stage: update-kustomize tags: - docker-runner only: - staging - main services: - name: docker:20-dind before_script: - eval $(ssh-agent -s) - echo \u0026#34;${GITLAB_ID_RSA}\u0026#34; | tr -d \u0026#39;\\r\u0026#39; | ssh-add - \u0026gt; /dev/null - mkdir -p ~/.ssh - ssh-keyscan gitlab.com \u0026gt;\u0026gt; ~/.ssh/known_hosts - git remote set-url origin $GITLAB_REMOTE_URL - git config --global user.email $GITLAB_EMAIL - git config --global user.name $GITLAB_USERNAME - if [ \u0026#34;$CI_COMMIT_BRANCH\u0026#34; == \u0026#34;main\u0026#34; ]; then KUSTOMIZE_OVERLAY=\u0026#34;production\u0026#34;; fi - if [ \u0026#34;$CI_COMMIT_BRANCH\u0026#34; == \u0026#34;staging\u0026#34; ]; then KUSTOMIZE_OVERLAY=\u0026#34;staging\u0026#34;; fi script: - git checkout -B ${CI_COMMIT_BRANCH} - cd .kubernetes/overlays/${KUSTOMIZE_OVERLAY} - kustomize edit set image $DOCKER_REGISTRY_HOST/$DOCKER_REGISTRY_IMAGE:$CI_COMMIT_SHA - kustomize build . - git commit -am \u0026#39;[skip ci] staging kustomize update\u0026#39; - git push origin ${CI_COMMIT_BRANCH} 进行到这里就完成了GitOps CI阶段的流程。\nArgo CD流程  安装  由于是实验环境，我们这里直接使用轻量级（非 HA）版本进行安装。\nkubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml 安装完成后Deployment状态如下：\n$kubectl get deploy --namespace argocd NAME READY UP-TO-DATE AVAILABLE AGE argocd-redis 1/1 1 1 1h argocd-server 1/1 1 1 1h argocd-notifications-controller 1/1 1 1 1h argocd-applicationset-controller 1/1 1 1 1h argocd-dex-server 1/1 1 1 1h argocd-repo-server 1/1 1 1 1h web服务配置：可以通过配置Ingress路由映射到argocd-server443/80端口；或者简单通过port-forward实现。\n获取admin password：第一次安装完成后，需要获取argocd命名空间下secret-argocd-initial-admin-secret获取到初始化的admin password进行登录；完成密码修改后即可删除该secret。\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d; echo 使用CLI进行操作：\n# 安装CLI VERSION=$(curl --silent \u0026#34;https://api.github.com/repos/argoproj/argo-cd/releases/latest\u0026#34; | grep \u0026#39;\u0026#34;tag_name\u0026#34;\u0026#39; | sed -E \u0026#39;s/.*\u0026#34;([^\u0026#34;]+)\u0026#34;.*/\\1/\u0026#39;) curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64 chmod +x /usr/local/bin/argocd # 使用CLI登录 argocd login my.argocd.k8s.com # argocd -h 查看argocd的一些基本操作帮助  配置Kubernetes Cluster  通过CLI添加本地kube config中的cluster到argo cd：\nargocd cluster add production-cluster  创建Projects  Projects提供了应用程序的逻辑分组，一般为公司内部的多个团队、多个项目组做区分与提供资源限制。Projects对应的CRD是appprojects.argoproj.io ,对应的spec规范了资源限制条件，定义项目角色以提供应用程序RBAC，如下是默认default projects；\nspec: # 限制可以部署的Git源存储库 sourceRepos: - \u0026#39;*\u0026#39; destinations: #制应用程序可以部署的目标集群和命名空间 - namspce: \u0026#39;/果应用程序清单于私有存储库中，则必须配置存储库凭据。Argo CD 支持 HTTPS 和 SSH Git 凭证。 server: \u0026#39;*\u0026#39; # 限制可以部署或不可以部署的Kubernetes资源类型(Deployment,DaemonSets...) clusterResourceWhitelist: - group: \u0026#39;*\u0026#39; kind: \u0026#39;*\u0026#39; 通过CLI配置Projects可以使用如下命令\nargocd proj create --help Create a project Usage: argocd proj create PROJECT [flags] 通过Web UI配置Projects:通过在settings-\u0026gt;projects下创建并编辑使用。\n 配置Repository  如果应用程序位于私有存储库中，则必须配置存储库凭据。Argo CD支持HTTPS/SSH Git凭证。 通过CLI配置Repositories可以使用如下命令\nargocd repo add --help Add git repository connection parameters Usage: argocd repo add REPOURL [flags] Examples: # Add a Git repository via SSH using a private key for authentication, ignoring the server\u0026#39;s host key: argocd repo add git@git.example.com:repos/repo --insecure-ignore-host-key --ssh-private-key-path ~/id_rsa 通过Web UI配置Repositories:通过在Settings/Repositories下CONNECT REPO进行连接测试与创建。\n 配置Application  接下来就可以创建Application，创建后即创建了对应的Application Kubernetes CRD资源applications.argoproj.io，Argo CD实现的GitOps Controller会自动将Application Git仓库里的配置清单部署到指定的Kubernetes集群上。\n通过CLI配置Application如下所示：\nargocd app create --help Create an application Usage: argocd app create APPNAME [flags] Examples: # Create a Kustomize app argocd app create kustomize-guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path kustomize-guestbook --dest-namespace default --dest-server https://kubernetes.default.svc --kustomize-image gcr.io/heptio-images/ks-guestbook-demo:0.1 其中一些参数\u0026hellip;\n \u0026ndash;repo 指定部署应用对应的Git仓库地址 \u0026ndash;path 部署应用对应的 manifest 位置 \u0026ndash;dest-server 目标 Kubernetes 集群地址  通过Web UI创建Application可以通过NEW APP进行创建，如下所示： 创建完成后，如果选择的同步策略为automated，Argo CD会自动同步项目的manifest部署到对应的集群中，也可以在WEB UI通过SYNC或者CLI通过argocd app sync shovel-gitops-production进步手动同步。\n同步完成后可以在WEB UI上看到对应应用的同步状态，如下所示：  自动同步策略（SYNC POLICY）  Argo CD能够在检测到 Git 中所需的清单与集群中的实时状态之间存在差异时自动同步应用程序。自动同步是GitOps Pull模式的核心，好处是 CI/CD Pipeline 不再需要直接访问Argo CD API服务器来执行部署，可以通过在WEB UI的Application-SYNC POLICY中启用AUTOMATED或CLIargocd app set \u0026lt;APPNAME\u0026gt; --sync-policy automated 进行配置。\n 自动修剪（Automatic Pruning）  自动修剪作为一种安全机制，当Argo CD检测到资源不再存在于Git中的环境配置定义时，自动同步不会删除资源。可以通过手动同步（检查修剪）或者通过WEB UI的Application-SYNC POLICY中启用PRUNE RESOURCES或CLIargocd app set \u0026lt;APPNAME\u0026gt; --auto-prune进行配置。\n 自动修复（Automatic Self-Healing）  自动修复作为一种安全机制，当我们直接对集群进行操作导致实时集群的状态偏离Git中的环境配置定义定义的状态时，Argo CD会自动进行修复，保持集群的状态与Git manifest的一致性。可以通过WEB UI的Application-SYNC POLICY中启用SELF HEAL或CLIargocd app set \u0026lt;APPNAME\u0026gt; --self-heal进行配置。\n更多Argo CD的配置可以参考：https://argo-cd.readthedocs.io/en/stable/user-guide/\n项目源码 可以在Github上获取到此项目的源码：https://github.com/ChinaLHR/shovel-kustomize-argocd-gitops\n参考  https://argo-cd.readthedocs.io/en/stable/ https://www.weave.works/technologies/gitops/ https://harness.io/blog/helm-vs-kustomize "
            }
    
        ,
            {
                "id": 2,
                "href": "https://chinalhr.github.io/post/kubernetes-crd-operator-2/",
                "title": "Kubernetes-Operator：扩展Kubernetes API Resource与Custom Controller (下)",
                "section": "post",
                "date" : "2022.01.23",
                "body": " 本文介绍了如何使用Operator-SDK开发Kubernetes Operator\n Operator 什么是Operator  参考：https://www.redhat.com/zh/topics/containers/what-is-a-kubernetes-operator\n Kubernetes Operator 是一种特定于应用的控制器，可扩展 Kubernetes API 的功能，来代表 Kubernetes 用户创建、配置和管理复杂应用的实例。\n它基于基本 Kubernetes Resource与Controller概念构建，又涵盖了特定于域或应用的知识，用于实现其所管理软件的整个生命周期的自动化；可以看作是CRD与Custom Controller的组合模式的封装及最佳实践。\n对比通过定义CRD与基于client-go去实现Custom Controller；Operator在这个基础上进行了标准化的流程规范、高级API抽象、快速启动脚手架与代码生成工具、RBAC控制、持续监控、数据备份、故障恢复、自动化升级Operator的CICD等。\n目前比较主流的Operator框架是如下两个：\n operator-framework 由CoreOS开发和维护，包含operator-sdk与operator-lifecycle-manager kubebuilder由k8s SIG开发和维护  Operator-SDK vs kubebuilder 目前来看，operator-sdk与kubebuilder都是类似的支持快速创建和管理 Operator 的框架，这两个项目都广泛使用了controller-runtime 和 controller-tools 项目，因此有着类似的 Go 源文件、包结构，而且目前operator-sdk也与kubebuilder进行了深度整合，可以参考kubebuilder的github projects 与 operator-sdk的faq。\n operator-sdk-go 底层基于kuberbuilder进行操作。 operator-sdk在Kubebuilder提供的基本项目脚手架之上提供了其他功能，如operator-sdk init会生成。  Operator Lifecycle Manager operator的安装和运行管理系统 支持发布OperatorHub Operator SDK scorecard 用于确保operator最佳实践和开发集群测试的工具   operator-sdk支持 Go 以外的 operator types，如Ansible 和 Helm operator-sdk与kubebuilder的文档是通用的  Operator 工作模式  用户创建一个CRD，并提交给到apiServer apiserver 根据自己注册的一个 pass 列表，把该 CRD 的请求转发给 webhook webhook完成该CRD 的缺省值与参数检验，处理完成后，相应的 CR 会被持久化到etcd，后响应用户 controller会在后台监测该自定义资源，按照业务逻辑，处理与该自定义资源相关联的特殊操作  可以看到，与前文所示的sample-controller方式的处理流程大致相同。\nOperator 应用场景  分布式应用的自动化运维：etcd-operator、 mongodb-enterprise-kubernetes、prometheus-operator、kafka-operator\u0026hellip; 扩展Kubernetes：istio、kruise、kubeedge\u0026hellip;  Kubernetes Admission Controller  参考：\nhttps://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\nhttps://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/\n Admission Controller（准入控制器）\n准入控制器是在对象持久化之前用于对 Kubernetes API Server 的请求进行拦截的代码段。准入控制器可能是validating、mutating或者两者都有，Mutating 控制器可以修改他们的处理的资源对象，Validating 控制器不会，如果任何一个阶段中的任何控制器拒绝了请求，则会立即拒绝整个请求，并将错误返回给最终的用户。\n我们可以编写自己的准入控制器，但是准入控制器需要被编译进 kube-apiserver，并且在 apiserver 启动时启动。为了增加扩展性，而不是和 apiserver 耦合在一起，便有了Admission webhooks这种通过一种动态配置webhook的方式。\n**admission webhooks **\n在 Kubernetes apiserver 中包含两个特殊的准入控制器：MutatingAdmissionWebhook和ValidatingAdmissionWebhook。这两个控制器支持webhook模式，将发送准入请求到外部的 HTTP 服务并接收一个准入响应；创建admission webhook的流程可以参考官方文档。\nOperator开发实践  参考：\nhttps://sdk.operatorframework.io/docs/\nhttps://olm.operatorframework.io/docs/\nhttps://book.kubebuilder.io/\n Operator-Framework Operator-Framework主要包含如下两个部分：\nOperator-SDK 编写Operator的核心工具包，提供高级API，可以更直观地编写Controller逻辑；提供用于快速启动新项目的脚手架与代码生成工具。\nGo-Operator工作流程：\n 使用 SDK 命令行界面 (CLI) 创建Operator项目 通过添加CRD 来定义新的资源 API 定义控制器来监视和协调资源 使用 SDK 和 controller-runtime APIs 为控制器编写协调逻辑 使用 SDK CLI 构建和生成Operator部署清单  Operator能力等级：\n标准化了Operator的生命周期管理能力方面不同的成熟度级别（基本安装、无缝升级、全生命周期、深度可观测、自动伸缩扩展），如下图所示：\nGo-Operator-SDK架构图\nOperator-Lifecycle-Manager（OLM） OLM 扩展了 Kubernetes，以提供一种声明式的方式来安装、管理和升级 Operator 及其在集群中的依赖项；提供的功能如下：\n Over-the-Air Updates and Catalogs：OLM 有一个 Catalogs概念，Operator 可以从中安装并保持更新。 Dependency Model：使用 OLMs 打包格式，可以表达对平台其他Operator的依赖关系。 Discoverability：可发现的Operator，OLM 将已安装的 Operator 及其服务通告到租户的命名空间中，以达到自动发现可安装的Operator。 Cluster Stability ：集群稳定性保证，Operator声明对其API的所有权，OLM 将防止拥有相同 API 的 Operator 发生冲突，确保集群稳定性。 声明式 UI 控件  基于Operator-SDK开发Sidecar-Operator Sidecar-Operator的功能 与Istio的自动注入Envoy代理方式类似，sidcar-operator支持自动为集群中符合条件的Pod注入sidecar容器，并负责更新sidecarSet自定义资源的状态。\n安装Operator-SDK 这里使用的是Ubuntu系统进行开发，安装可以参考官方文档：\nexport ARCH=$(case $(uname -m) in x86_64) echo -n amd64 ;; aarch64) echo -n arm64 ;; *) echo -n $(uname -m) ;; esac) export OS=$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;) export OPERATOR_SDK_DL_URL=https://github.com/operator-framework/operator-sdk/releases/download/v1.16.0 curl -LO ${OPERATOR_SDK_DL_URL}/operator-sdk_${OS}_${ARCH} chmod +x operator-sdk_${OS}_${ARCH} \u0026amp;\u0026amp; sudo mv operator-sdk_${OS}_${ARCH} /usr/local/bin/operator-sdk 初始化项目 # 创建项目 mkdir -p sidecar-operator \u0026amp;\u0026amp; cd sidecar-operator # 初始化项目，这里的domain指定了后续注册CRD对象的Group域名，init初始化后会生成代码框架、Makefile工具文件、拉取依赖代码库 operator-sdk init --domain chinalhr.github.io --repo github.com/ChinaLHR/sidecar-operator # 查看项目结构 tree -L 2 . ├── Dockerfile ├── LICENSE ├── Makefile ├── PROJECT ├── README.md ├── config │ ├── default │ ├── manager │ ├── manifests │ ├── prometheus │ ├── rbac │ └── scorecard ├── go.mod ├── go.sum ├── hack │ └── boilerplate.go.txt └── main.go  go.mod ：项目基本依赖，client-go，controller-runtime\u0026hellip; Makefile：构建，部署Operator PROJECT：用于搭建新组件的 Kubebuilder 元数据 config目录：启动配置，包含在集群上启动控制器所需的Kustomize YAML 定义，还会保存CRD、RBAC与Webhook配置 main.go：初始化并运行Manager  创建API资源与Custom Controller模板代码 operator-sdk create api --group apps --version v1alpha1 --kind SidecarSet --resource --controller 通过operator-sdk tool生成 SidecarSet API资源 api/v1alpha1/sidecarset_types.go和控制器controllers/sidecarset_controller.go 模板。\n自定义资源结构修改\n这里我们需要根据我们自身的需求去定义我们自定义资源的结构，即修改controllers/sidecarset_controller.go，如下所示：\n/* Copyright 2022. Licensed under the Apache License, Version 2.0 (the \u0026#34;License\u0026#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \u0026#34;AS IS\u0026#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. */ package v1alpha1 import ( v1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; ) // EDIT THIS FILE! THIS IS SCAFFOLDING FOR YOU TO OWN! // NOTE: json tags are required. Any new fields you add must have json tags for the fields to be serialized.  // SidecarSetSpec defines the desired state of SidecarSet type SidecarSetSpec struct { // INSERT ADDITIONAL SPEC FIELDS - desired state of cluster \t// Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file  //query over pods that should be injected sidecar \tSelector *metav1.LabelSelector `json:\u0026#34;selector,omitempty\u0026#34;` Containers []SidecarContainer `json:\u0026#34;containers,omitempty\u0026#34;` } type SidecarContainer struct { v1.Container `json:\u0026#34;,inline\u0026#34;` } // SidecarSetStatus defines the observed state of SidecarSet type SidecarSetStatus struct { // INSERT ADDITIONAL STATUS FIELD - define observed state of cluster \t// Important: Run \u0026#34;make\u0026#34; to regenerate code after modifying this file  //the number of Pods whose labels are matched with this SidecarSet\u0026#39;s selector \tMatchedPods int32 `json:\u0026#34;matchedPods\u0026#34;` //the number of matched Pods that are injected with the latest SidecarSet\u0026#39;s \tUpdatedPods int32 `json:\u0026#34;updatedPods\u0026#34;` //the number of matched Pods that have a ready condition \tReadyPods int32 `json:\u0026#34;readyPods\u0026#34;` } // +genclient // +genclient:nonNamespaced // +kubebuilder:object:root=true // +kubebuilder:subresource:status // +kubebuilder:resource:scope=Cluster // +kubebuilder:printcolumn:name=\u0026#34;MATCHED\u0026#34;,type=\u0026#34;integer\u0026#34;,JSONPath=\u0026#34;.status.matchedPods\u0026#34;,description=\u0026#34;The number of pods matched.\u0026#34; // +kubebuilder:printcolumn:name=\u0026#34;UPDATED\u0026#34;,type=\u0026#34;integer\u0026#34;,JSONPath=\u0026#34;.status.updatedPods\u0026#34;,description=\u0026#34;The number of pods matched and updated.\u0026#34; // +kubebuilder:printcolumn:name=\u0026#34;READY\u0026#34;,type=\u0026#34;integer\u0026#34;,JSONPath=\u0026#34;.status.readyPods\u0026#34;,description=\u0026#34;the number of matched Pods that have a ready condition.\u0026#34;  // SidecarSet is the Schema for the sidecarsets API type SidecarSet struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ObjectMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Spec SidecarSetSpec `json:\u0026#34;spec,omitempty\u0026#34;` Status SidecarSetStatus `json:\u0026#34;status,omitempty\u0026#34;` } //+kubebuilder:object:root=true  // SidecarSetList contains a list of SidecarSet type SidecarSetList struct { metav1.TypeMeta `json:\u0026#34;,inline\u0026#34;` metav1.ListMeta `json:\u0026#34;metadata,omitempty\u0026#34;` Items []SidecarSet `json:\u0026#34;items\u0026#34;` } func init() { //将Go类型添加到API组中 \tSchemeBuilder.Register(\u0026amp;SidecarSet{}, \u0026amp;SidecarSetList{}) } 这里主要是更新了SidecarSet的Spec结构，增加了Selector与Containers属性，用于匹配需要注入sidecar的Pod以及sidecar注入容器的定义。更新了SidecarSet的Status结构，增加了MatchedPods、UpdatedPods与ReadyPods描述匹配的Pod数量与已注入sidecar的Pod数量与已注入成功并且就绪的Pod数量。\n这里还补充了code generator依赖注释，如：+genclient:nonNamespaced生成非namespace对象、+kubebuilder:resource:scope=Cluster指定资源的范围、kubebuilder:printcolumn... 用于kubectl获取数据展示\u0026hellip;\n最后，我们需要运行代码生成命令make generate 为资源类型更新\\生成代码，该命令会调用controller-gen程序来更新api/v1alpha1/zz_generated.deepcopy.go文件。\nmake generate 自定义控制器模板\n生成的controllers/sidecarset_controller.go模板\n SetupWithManager()函数：为构建控制器以监视 CR 和该控制器拥有和管理的其他资源。  // SetupWithManager sets up the controller with the Manager. func (r *SidecarSetReconciler) SetupWithManager(mgr ctrl.Manager) error { return ctrl.NewControllerManagedBy(mgr).\t//控制器构建器 \tFor(\u0026amp;appsv1alpha1.SidecarSet{}).\t//监控的主资源,对于每个SidecarSet类型的Add/Update/Delete事件，将发送给协调循环（Reconcile） \tComplete(r)\t}  Reconcile()函数：协调循环（Reconcile loop），接受Request参数，该参数为（命名空间/名称键），用于从缓存中查找主要资源对象如Sidecarset；实现特定类型协调逻辑（如repliceset的reconcile即在其创建的时候去创建关联的pod）并返回结果、异常或者重试。  func (r *SidecarSetReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { _ = log.FromContext(ctx) // TODO(user): your logic here \treturn ctrl.Result{}, nil } 创建CR Admission Webhook模板代码 Admission Webhooks分为如下两种：\n Validating admission webhook： 用于执行超出 OpenAPI 模式验证功能的验证，注意这里可以拒绝请求，但不能修改在请求中接收的对象。 Mutating admission webhook： 用于默认设置（在创建时为资源中未设置的字段添加默认值），也可以通过创建一个补丁来修改对象。  通过如下命令创建Validating Admission Webhook模板代码\n# --programmatic-validation参数创建Validating admission webhook所需要的资源 # --defaulting 参数创建Mutating admission webhook所需要的资源 operator-sdk create webhook --group apps --version v1alpha1 --kind SidecarSet --programmatic-validation 生成的api/v1alpha1/sidecarset_webhook.go模板\nfunc (r *SidecarSet) ValidateCreate() error { sidecarsetlog.Info(\u0026#34;validate create\u0026#34;, \u0026#34;name\u0026#34;, r.Name) // TODO(user): fill in your validation logic upon object creation. \treturn nil } func (r *SidecarSet) ValidateUpdate(old runtime.Object) error { sidecarsetlog.Info(\u0026#34;validate update\u0026#34;, \u0026#34;name\u0026#34;, r.Name) // TODO(user): fill in your validation logic upon object update. \treturn nil } func (r *SidecarSet) ValidateDelete() error { sidecarsetlog.Info(\u0026#34;validate delete\u0026#34;, \u0026#34;name\u0026#34;, r.Name) // TODO(user): fill in your validation logic upon object deletion. \treturn nil } 可以看到生成了ValidateCreate、ValidateUpdate、ValidateDelete三个模板方法，分别在CR创建、更新、删除时进行回调。\n核心逻辑梳理  在用户创建SidecarSet CR的时候进行配置项校验：通过实现SidecarSet Validating admission webhook 校验逻辑进行处理。 在用户创建Pod的时候，自动匹配对应的SidecarSet，如果匹配到SidecarSet的话解析出SidecarSet对应的sidecar容器注入到Pod中：这里就需要通过实现Pod的Mutating admission webhook达到目的，由于 kubebuilder 不支持核心类型的 webhook 自动生成，所以需要使用 controller-runtime 的库来处理。 实时监测Pod信息，更新SidecarSet Status： 这里就需要实现sidecarset_controller的Reconcile逻辑进行处理。  完整流程如下图所示：\n实现Pod类型的Mutating Admission Webhook //+kubebuilder:webhook:path=/mutate-sidecar-pod,mutating=true,failurePolicy=fail,groups=\u0026#34;\u0026#34;,resources=pods,verbs=create;update,versions=v1,name=mpod.kb.io,admissionReviewVersions=v1,sideEffects=None  type podHandler struct { Client client.Client Decoder *admission.Decoder } var ( SidecarEnvKey = \u0026#34;IS_INJECTED\u0026#34; SidecarAnnotationHashKey = \u0026#34;chinalhr.github.io/sidecar-hash\u0026#34; ) func (r *SidecarSet) Register(mgr ctrl.Manager) error { mgr.GetWebhookServer().Register(\u0026#34;/mutate-sidecar-pod\u0026#34;, \u0026amp;webhook.Admission{Handler: \u0026amp;podHandler{Client: mgr.GetClient()}}) return nil } func (handler *podHandler) Handle(ctx context.Context, request admission.Request) admission.Response { log := log.FromContext(ctx) pod := \u0026amp;v1.Pod{} err := handler.Decoder.Decode(request, pod) copy := pod.DeepCopy() log.V(0).Info(\u0026#34;begin to process sidecarSet webhook\u0026#34;) if err != nil { return admission.Errored(http.StatusInternalServerError, err) } log.V(0).Info(\u0026#34;sidecarSet webhook hand pod %v\u0026#34;, \u0026#34;pod\u0026#34;, pod.Name) err = handler.mutatingPodFunc(ctx, copy) if err != nil { return admission.Errored(http.StatusInternalServerError, err) } marshaledPod, err := json.Marshal(copy) return admission.PatchResponseFromRaw(request.Object.Raw, marshaledPod) } func (handler *podHandler) mutatingPodFunc(ctx context.Context, pod *v1.Pod) error { //1. get sidecarSet resource list \tsidecarSets := \u0026amp;SidecarSetList{} if err := handler.Client.List(ctx, sidecarSets); err != nil { return err } var sidecarContainers []v1.Container sidecarSetHash := make(map[string]string) matchNothing := true for _, sidecarSet := range sidecarSets.Items { needInject, err := PodSidecarSetMatch(pod, sidecarSet) if err != nil { return err } if !needInject { continue } matchNothing = false sidecarSetHash[sidecarSet.Name] = sidecarSet.Annotations[SidecarAnnotationHashKey] for i := range sidecarSet.Spec.Containers { sidecarContainer := \u0026amp;sidecarSet.Spec.Containers[i] sidecarContainer.Env = append(sidecarContainer.Env, v1.EnvVar{Name: SidecarEnvKey, Value: \u0026#34;true\u0026#34;}) sidecarContainers = append(sidecarContainers, sidecarContainer.Container) } } if matchNothing { return nil } pod.Spec.Containers = append(pod.Spec.Containers, sidecarContainers...) if pod.Annotations == nil { pod.Annotations = make(map[string]string) } if len(sidecarSetHash) != 0 { encodedStr, err := json.Marshal(sidecarSetHash) if err != nil { return err } pod.Annotations[SidecarAnnotationHashKey] = string(encodedStr) } return nil } func PodSidecarSetMatch(pod *v1.Pod, sidecarSet SidecarSet) (bool, error) { selector, err := metav1.LabelSelectorAsSelector(sidecarSet.Spec.Selector) if err != nil { return false, err } if !selector.Empty() \u0026amp;\u0026amp; selector.Matches(labels.Set(pod.Labels)) { return true, nil } return false, nil } // InjectDecoder injects the decoder. func (handler *podHandler) InjectDecoder(d *admission.Decoder) error { handler.Decoder = d return nil } 这里实现包括：\n InjectDecoder方法注入解码器，用以解析admission request；实现 admission.Handler 接口的Handle方法，解析出Pod 资源的信息，获取当前sidecarSet，根据sidecarSet的selector匹配Pod的labels，匹配成功则进行sidecar的注入，将sidecarSet的Container注入到当前的Pod中，并对已注入的Pod进行annotations标识（后续SidecarSet Controller进行识别监控）；最后通过PatchResponseFromRaw 合并更新Pod的信息相应admission controller。 Register方法注册当前的mutating webhook。 注释kubebuilder标记，让controller-gen可以生成对应的webhook 配置  实现SidecarSet Validating admission webhook逻辑 // log is for logging in this package. var sidecarsetlog = logf.Log.WithName(\u0026#34;sidecarset-resource\u0026#34;) func (r *SidecarSet) SetupWebhookWithManager(mgr ctrl.Manager) error { return ctrl.NewWebhookManagedBy(mgr). For(r). Complete() } //+kubebuilder:webhook:path=/validate-apps-chinalhr-github-io-v1alpha1-sidecarset,mutating=false,failurePolicy=fail,sideEffects=None,groups=apps.chinalhr.github.io,resources=sidecarsets,verbs=create;update,versions=v1alpha1,name=vsidecarset.kb.io,admissionReviewVersions=v1  var _ webhook.Validator = \u0026amp;SidecarSet{} // ValidateCreate implements webhook.Validator so a webhook will be registered for the type func (r *SidecarSet) ValidateCreate() error { sidecarsetlog.Info(\u0026#34;validate create\u0026#34;, \u0026#34;name\u0026#34;, r.Name) return r.validateSpec() } // ValidateUpdate implements webhook.Validator so a webhook will be registered for the type func (r *SidecarSet) ValidateUpdate(old runtime.Object) error { sidecarsetlog.Info(\u0026#34;validate update\u0026#34;, \u0026#34;name\u0026#34;, r.Name) return r.validateSpec() } func (r *SidecarSet) validateSpec() *apierrors.StatusError { spec := r.Spec var allErrs field.ErrorList if spec.Selector == nil { allErrs = append(allErrs, field.Required(field.NewPath(\u0026#34;spec\u0026#34;).Child(\u0026#34;selector\u0026#34;), \u0026#34;no selector defined for sidecarset\u0026#34;)) } else { if len(spec.Selector.MatchLabels)+len(spec.Selector.MatchExpressions) == 0 { allErrs = append(allErrs, field.Invalid(field.NewPath(\u0026#34;spec\u0026#34;).Child(\u0026#34;selector\u0026#34;), spec.Selector, \u0026#34;empty selector is not valid for sidecarset.\u0026#34;)) } } if len(allErrs) == 0 { return nil } return apierrors.NewInvalid( schema.GroupKind{Group: \u0026#34;apps.chinalhr.github.io\u0026#34;, Kind: \u0026#34;SidecarSet\u0026#34;}, r.Name, allErrs) } // ValidateDelete implements webhook.Validator so a webhook will be registered for the type func (r *SidecarSet) ValidateDelete() error { sidecarsetlog.Info(\u0026#34;validate delete\u0026#34;, \u0026#34;name\u0026#34;, r.Name) // TODO(user): fill in your validation logic upon object deletion. \treturn nil } 这里主要是实现了validateSpec方法，支持对SidecarSet创建/更新的请求，进行spec 中Selector属性的校验；如果校验失败，则返回异常拒绝资源的创建/更新。\n实现控制器协调逻辑 //+kubebuilder:rbac:groups=apps.chinalhr.github.io,resources=sidecarsets,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=apps.chinalhr.github.io,resources=sidecarsets/status,verbs=get;update;patch //+kubebuilder:rbac:groups=apps.chinalhr.github.io,resources=sidecarsets/finalizers,verbs=update //+kubebuilder:rbac:groups=\u0026#34;\u0026#34;,resources=pods,verbs=get;list;watch;create;update;patch;delete //+kubebuilder:rbac:groups=\u0026#34;\u0026#34;,resources=pods/status,verbs=get  func (r *SidecarSetReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) { log := log.FromContext(ctx) //next reconcile \treconcileResult := ctrl.Result{RequeueAfter: time.Second * 5, Requeue: true} //fetch the SidecarSet instance \tsidecarSet := \u0026amp;appsv1alpha1.SidecarSet{} err := r.Get(context.TODO(), req.NamespacedName, sidecarSet) if err != nil { if errors.IsNotFound(err) { log.Error(err, \u0026#34;process sidecarSet error : object not found\u0026#34;) return reconcileResult, nil } log.Error(err, \u0026#34;process sidecarSet error : reading object error\u0026#34;) return reconcile.Result{}, err } log.V(0).Info(\u0026#34;begin to process sidecarSet [%s]\u0026#34;, sidecarSet.Name) //list matched pod \tselector, err := metav1.LabelSelectorAsSelector(sidecarSet.Spec.Selector) if err != nil { log.Error(err, \u0026#34;process sidecarSet error\u0026#34;) return reconcile.Result{}, err } listOpts := \u0026amp;client.ListOptions{LabelSelector: selector} matchedPods := \u0026amp;v1.PodList{} if err := r.Client.List(context.TODO(), matchedPods, listOpts); err != nil { log.Error(err, \u0026#34;list matched pods error\u0026#34;) return reconcile.Result{}, err } log.V(0).Info(\u0026#34;process sidecarSet matchedPods\u0026#34;, \u0026#34;pods\u0026#34;, matchedPods) // ignore inactive pods \tvar filteredPods []*v1.Pod for i := range matchedPods.Items { pod := \u0026amp;matchedPods.Items[i] if IsPodActive(pod) { filteredPods = append(filteredPods, pod) } } //calculateSidecarSetStatus \tstatus, err := calculateSidecarSetStatus(sidecarSet, filteredPods) if err != nil { log.Error(err, \u0026#34;calculateSidecarSetStatus error\u0026#34;) return reconcile.Result{}, err } //updateSidecarSetStatus \terr = r.updateSidecarSetStatus(log, sidecarSet, status) if err != nil { log.Error(err, \u0026#34;updateSidecarSetStatus error\u0026#34;) return reconcile.Result{}, err } return reconcileResult, nil } 这里实现包括：\n 获取到sidecarSet资源，根据sidecarSet的selector获取匹配的Pod集合，根据Pod的状态计算出sidecarSet的Status，即已匹配的Pod数量更新MatchedPods ，已经注入sidecar容器的Pod数量更新UpdatedPods，已经注入sidecar容器并且处于Running状态的Pod数量更新ReadyPods。 注释kubebuilder标记，增加对Pod,Pod Status对应的权限。  部署Operator  生成CRD/Webhook manifests  # 使用spec/status字段与标记和CRD validation标记定义API，以及新增了其他资源的Webhook，需要使用make manifests生成/更新CRD与webhook的 manifests make manifests  部署cert-manager  因为需要为webhook server配置证书，这里需要在Kubernetes集群中部署cert manager作证书管理，可以参考https://cert-manager.io/docs/。\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.7.1/cert-manager.yaml cert-manager的CA injector组件会将 CA 包注入到 Mutating|ValidatingWebhookConfiguration 中，operator-sdk已经为我们生成了对应的kustomize patch文件在config/default/webhookcainjection_patch.yaml。\napiVersion: admissionregistration.k8s.io/v1 kind: MutatingWebhookConfiguration metadata: name: mutating-webhook-configuration annotations: cert-manager.io/inject-ca-from: $(CERTIFICATE_NAMESPACE)/$(CERTIFICATE_NAME) --- apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: name: validating-webhook-configuration annotations: cert-manager.io/inject-ca-from: $(CERTIFICATE_NAMESPACE)/$(CERTIFICATE_NAME)  通过 kustomize 启用 webhook 和 cert-manager 配置config/default/kustomization.yaml与config/crd/kustomization.yaml，根据注释进行配置 通过make docker-build docker-push进行容器镜像打包与推送；通过make deploy部署Operator到集群中  make docker-build docker-push IMG=ccr.ccs.tencentyun.com/open-operator/sidecar-operator:1.0.0 make deploy IMG=ccr.ccs.tencentyun.com/open-operator/sidecar-operator:1.0.0 代码仓库 https://github.com/ChinaLHR/sidecar-operator\n"
            }
    
        ,
            {
                "id": 3,
                "href": "https://chinalhr.github.io/post/kubernetes-crd-operator-1/",
                "title": "Kubernetes-Operator：扩展Kubernetes API Resource与Custom Controller (上)",
                "section": "post",
                "date" : "2022.01.22",
                "body": " 本文介绍了如何对Kubernetes 核心组件、扩展机制与API Resource设计概念，以及如何使用定制资源（Custom Resource）与定制控制器（Custom Controller）实现对Kubernetes API Resource的扩展\n Kubernetes架构设计 kubernetes核心组件 Kubernetes系统架构整体采用的是C/S 的架构，其中Master节点作为 Server，各Worker 节点作为 Client。\n Master Node：  kube-apiserver：提供了资源操作的唯一入口，提供REST API接口，并提供认证、授权、访问控制、API 注册和发现等机制；常见的与kube-apiserver进行交互可以通过kubectl 、client-go kube-scheduler：负责kubernetes内资源的调度，按照调度策略将Pod调度到相应的Node上 kube-controller-manager：控制器管理器，提供了一些内置的Controller，自动化地管理集群状态，包括了资源状态，节点状态等；核心功能是确保集群始终处于预期状态，控制Kubernetes中的资源它们向 spec 配置的期望状态进行收敛 etcd：持久化集群状态、元数据、集群资源对象 kube-proxy：负责 Kubernetes 中 Service 的服务发现和负载均衡功能实现   Worker Node：  kubelet：负责管理Worker Node上Pod资源的生命周期，kubelet相当于一个代理执行器，接收到kube-apiserver的请求后执行Pod的管理逻辑，定期监控资源的使用状态上报kube-apiserver，以及如下接口的管理  CRI（Container Runtime Interface）：容器运行时接口，提供计算资源 CNI（Container Network Interface）：容器网络接口，提供网络资源 CSI（Container Storage Interface）：容器存储接口，提供存储资源   Container Runtime： 负责镜像管理及 Pod 和容器运行时接口实现（CRI）    kubernetes扩展性  参考：https://kubernetes.io/zh/docs/concepts/extend-kubernetes/\n Kubernetes的架构设计是高度可配置、可扩展的，这里主要关注Kubernetes自身的扩展性，即扩充 Kubernetes 的能力并深度集成软件组件。\n扩展模式：\n 控制器模式：编写Kubernetes 的客户端程序的一种特定模式，控制器通常读取一个对象的 spec 字段，可能做出一些处理，然后更新对象的 status 字段。 Webhook模式：Kubernetes作为客户端调用远程服务的模式。 Binary Plugin模式：Kubernetes作为客户端执行一个二进制插件程序。  扩展kubernetes的核心组件：\n kube-controller-manager与API Resource扩展：通过使用crd与custom controller、operator framework实现，本文主要讲解这方面的扩展实现。 kube-apiserver扩展：通过使用API Aggregation layer在不修改 Kubernetes 核心代码的同时扩展 Kubernetes api-server，即将第三方服务注册成Kubernetes api-server提供服务。 kube-scheduler扩展：通过Kubernetes Scheduling Framework扩展调度机制。  扩展Kubernetes API Resource相关概念 资源（Resource）： Resource是 Kubernetes API中的一个端点，用于存储某个类别的API对象的一个集合；如YAML中的kind：Pod、CronJob\u0026hellip;\n定制资源（Custom Resource）： 自定义 API 资源，Custom Resource是对 Kubernetes API 的扩展，定制资源所代表的是对特定Kubernetes安装的一种定制。\n定制控制器（Custom Controller）： Custom Resource本身只能用来存取结构化的数据，需要将Custom Resource与Custom Controller结合，Custom Controller负责监控Customer Resource的变化（创建、删除\u0026hellip;）并执行具体的动作。\nCRD（CustomResourceDefinitions）： Custom Resource的定义，Kubernetes CustomResourceDefinition API资源允许自定义Custom Resource。 定义CRD对象的操作会使用你所设定的名字和模式定义（Schema）创建一个新的Custom Resource，Kubernetes API 会为Custom Resource提供存储和访问服务。\n简单的说，可以通过CRD定制自定义API资源即Custom Resource，动态注册到Kubernetes集群中；注册完成后用户可以通过 kubectl 来创建访问自定义API资源对象，类似于操作 Pod 一样。CRD 仅仅只是做资源的定义，需要配合控制器即Custom Controller 去监听Custom Resource的事件触发执行对应的处理逻辑。\nKubernetes 的 Resource 设计概念  通过上文的介绍不难看出，Kubernetes是一个以资源为中心容器编排平台，核心组件kube-api-server通过REST API暴漏资源操作接口、kube-controller-manager控制管理资源的状态、kube-scheduler进行资源的调度。\n Group/Version/Resource kubernetes在资源的概念上进行了分组与版本化，，一个 API对象在Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成，如下图所示。 资源间的关系如下：\n Kubernetes支持多个 Group（资源组） 每个Group支持多个Version（资源版本） 每个Version支持多种Resource（资源），部分资源还拥有自己的子资源 Kind 与 Resource 属于同一级概念，Kind 用于描述 Resource 的种类，一般情况下Kind与Resource是一一对应的关系，例如pods Resource 对应于 Pod Kind  定位资源的形式如下：\n\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt;[/\u0026lt;SUBSOURCE\u0026gt;] # 以Deployment为例子 apps/v1/deployments/status 资源对象（资源描述）即Resource Object描述如下：\n\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;, Kind=\u0026lt;RESOURCE_NAME\u0026gt; # 以Deployment为例子 apps/v1, Kind=Deployment Group  资源组的划分依据是资源的功能，Kubernetes支持不同资源组中拥有不同资源版本，方便组内资源迭代升级 对于 Kubernetes 里的核心 API 对象（如：Pod\u0026hellip;）是无组名Group（即：Group 是“”）,在 /api 这个层级下；对于Kubernetes里的非核心 API 对象（如：CronJob\u0026hellip;）是有组名Group，在 /apis 这个层级下  有组名 Group 资源: .../apis/\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt; 无组名 Group 资源: .../api/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt; Version Kubernetes 的资源版本 Version 采用语义化的版本号\n Alpha 阶段：内部测试版本，Alpha 版本中的功能默认情况下会被禁用，常见命名方式如 v1alpha1。 Beta 阶段：相对稳定版本，经过了官方和社区的测试，Beta 阶段下的功能默认是开启的，常见命名方式如 v2beta1。 Stable 阶段：正式发布版本，命名方式如 v1、v2 。  Resouce Resource 是 Kubernetes 中的核心概念\n Resource 实例化后称为一个 Resource Object。 Kubernetes 中所有的 Resource Object 都称为 Entity。 可以通过 Kubenetes API Server 去操作 Resource Object。  Kubernetes 目前的 Entity 分为两大类：\n Persistent Entity：持久化实体，Resource Object 创建后会持久存在，如 Deployment / Service。 Ephemeral Entity: 短暂实体，Resource Object 创建后不稳定，如出现故障/调度失败后不再重建，如Pod。  资源操作方法 Kubernetes资源YAML文件提交给kube-apiserver后，会被转换为Resouce Object，序列化后持久化到Etcd中；对资源的操作方法主要有如下8种：\n  create：Resource Object 创建\n  delete：Resource Object 删除\n  deletecollection：多个 Resource Objects 删除\n  patch：Resource Object 局部字段更新\n  update：Resource Object 整体更新\n  get：Resource Object 获取\n  list：多个 Resource Objects 获取\n  watch：Resource Objects 监控\n  如何创建CRD  参考文档：\nhttps://kubernetes.io/zh/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/\n apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: # 名字必需与下面的 spec 字段匹配，并且格式为 \u0026#39;\u0026lt;名称的复数形式\u0026gt;.\u0026lt;组名\u0026gt;\u0026#39; name: crontabs.stable.example.com spec: # 组名称，用于 REST API: /apis/\u0026lt;组\u0026gt;/\u0026lt;版本\u0026gt; group: stable.example.com # 列举此 CustomResourceDefinition 所支持的版本 versions: - name: v1 # 每个版本都可以通过 served 标志来独立启用或禁止 served: true # 其中一个且只有一个版本必需被标记为存储版本 storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: cronSpec: type: string image: type: string replicas: type: integer # 可以是 Namespaced 或 Cluster scope: Namespaced names: # 名称的复数形式，用于 URL：/apis/\u0026lt;组\u0026gt;/\u0026lt;版本\u0026gt;/\u0026lt;名称的复数形式\u0026gt; plural: crontabs # 名称的单数形式，作为命令行使用时和显示时的别名 singular: crontab # kind 通常是单数形式的帕斯卡编码（PascalCased）形式。你的资源清单会使用这一形式。 kind: CronTab # shortNames 允许你在命令行使用较短的字符串来匹配资源 shortNames: - ct 如上CRD所示， 指定group为stable.example.com，version为v1，CustomResource为CronTab，scope为Namespaced（CronTab属于Namespace的对象）,然后需要设置CustomResource的对象描述，包括：Spec、Status \u0026hellip;;\n通过以下命令创建CRD后，会创建一个新的 namespace 级别的 RESTful API 就会被创建：/apis/stable.example.com/v1/namespaces/*/crontabs/...，用以创建和管理CustomResource CronTab。在创建CRD时，Kubernetes 会对我们提交的声明文件进行校验（基于 OpenAPI v3 schem 进行规范）。如果想要更加复杂的校验，需要通过 Kubernetes 的 admission webhook 进行实现。\nkubectl apply -f crd.yaml 创建完CRD后，可以通过如下方式定义一个CronTab资源对象\napiVersion: \u0026#34;stable.example.com/v1\u0026#34; kind: CronTab metadata: name: my-cron-object spec: cronSpec: \u0026#34;* * * * */5\u0026#34; image: my-image 创建完CronTab资源对象后，就可以通过kubectl来管理自定义资源对象\n$ kubectl get crontab NAME AGE my-cron-tab 93s $ kubectl get crontab -o yaml ... 如何创建Custom Controller（sample-controller示例）  参考：\nkubernetes官方Custom Controller示例：https://github.com/kubernetes/sample-controller\n 示例演示了：\n  如何使用 CustomResourceDefinition注册类型的新自定义资源（自定义资源类型）Foo\n  如何创建/获取/列出新资源类型的实例Foo\n  如何基于控制器处理资源Foo创建/更新/删除事件\n  示例控制器基于client-go 库 进行Controller开发；该项目的tools/cache目录下包含了开发Custom Controller 使用的各种工具、机制。\nclient-go client对象  RESTClient：client-go 中最基础的客户端，其它 client 都基于 RESTClient 实现，RESTClient 实现了 RESTful 风格的 API 请求封装，可以实现对任意 Kubernetes 资源（包括内置资源及 CRDs）的 RESTful 风格交互，如 Post() / Delete() / Put() / Get()，同时支持 Json 和 protobuf； ClientSet：与 Kubernetes 内置资源对象交互最常用的 Client，强调，只能处理 Kubernetes 内置资源，不包括 CRD 自定义资源，使用时需要指定 Group、指定 Version，然后根据 Resource 获取。ClientSet 的操作代码是通过 client-gen 代码生成器自动生成的； DynamicClient：DynamicClient 能处理包括 CRD 自定义资源在内的任意 kubernetes 资源。如果一个 Controller 中需要控制所有的 API，可以使用Dynamic Client，DynamicClient 只支持JSON； DiscoveryClient：用于发现 kube-apiserver 支持的 Group / Version / Resource 信息；  client-go 组件工作流程与以及与Custom Controller的交互点 client-go组件：\n Reflector：通过 Kubernetes API 监控 Kubernetes 的资源类型，通过List/Watch 机制, 获取\u0026amp;监听资源对象实例的变化，添加 object 对象到 FIFO 队列，后续Informer 会从队列中进行数据获取。 Informer：controller 机制的基础，控制循环（Control Loop）处理，从队列中取出数据，添加到 Indexer 进行数据缓存，提供对象监听事件回调处理的 handler 接口，通过 给Informer 添加 ResourceEventHandler 实例的回调函数，通过实现OnAdd(obj interface{})、 OnUpdate(oldObj, newObj interface{}) 和 OnDelete(obj interface{}) 方法处理资源的创建、更新和删除操作。 Indexer：提供 object 对象的索引，缓存对象信息，indexer是线程安全的存储。  Custom Controller组件：\n Informer与Indexer的reference，通过client-go 提供的NewIndexerInformer函数进行创建。 Resource Event Handlers：Informer在要将object 对象传递给Custom Controller 时将调用的回调函数，Resource Event Handlers 被回调后会将Object Key写入到Work queue中。 Process Item：从Work queue中取出Object key（事件通知） 进行后续处理。  simaple-controller核心逻辑  项目结构  └── sample-controller ├── artifacts # yaml示例，如crd.yaml、example-foo.yaml │ └── examples ├── code-of-conduct.md ├── CONTRIBUTING.md ├── controller.go\t# custom controller实现，核心逻辑 ├── controller_test.go ├── docs │ ├── controller-client-go.md │ └── images ├── go.mod ├── go.sum ├── hack\t# code generation 工具类 │ ├── boilerplate.go.txt │ ├── custom-boilerplate.go.txt │ ├── tools.go │ ├── update-codegen.sh │ └── verify-codegen.sh ├── LICENSE ├── main.go\t# 启动函数，参数配置与初始化逻辑 ├── OWNERS ├── pkg\t# 资源定义文件与自动生成的代码 │ ├── apis │ ├── generated │ └── signals ├── README.md └── SECURITY_CONTACTS  代码生成  此项目利用k8s.io/code-generator中的生成器 来生成typed client、informers、listers 和deep-copy functions。自动生成了如下文件与目录\npkg/apis/samplecontroller/v1alpha1/zz_generated.deepcopy.go\npkg/generated/\n 核心逻辑-main.go（pseudocode）  func main() { ... // 创建clientset，kubeClient(操作除自定义资源组外的其他资源)、exampleClient(操作自定义资源组) \tcfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if err != nil { klog.Fatalf(\u0026#34;Error building kubeconfig: %s\u0026#34;, err.Error()) } kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil { klog.Fatalf(\u0026#34;error building kubernetes clientset: %s\u0026#34;, err.Error()) } exampleClient, err := clientset.NewForConfig(cfg) if err != nil { klog.Fatalf(\u0026#34;Error building example clientset: %s\u0026#34;, err.Error()) } // 创建Informer \tkubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30) exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30) // 创建 controller，传入 clientset 和 informer \tcontroller := NewController(kubeClient, exampleClient, kubeInformerFactory.Apps().V1().Deployments(), exampleInformerFactory.Samplecontroller().V1alpha1().Foos()) // 运行 Informer，Start方法非阻塞，运行在单独的 goroutine 中 \tkubeInformerFactory.Start(stopCh) exampleInformerFactory.Start(stopCh) //运行Custom Controller \tif err = controller.Run(2, stopCh); err != nil { klog.Fatalf(\u0026#34;Error running controller: %s\u0026#34;, err.Error()) } }  核心逻辑controller.go （pseudocode）  /* *** main.go */ // 创建 clientset kubeClient, err := kubernetes.NewForConfig(cfg)\t// k8s clientset, \u0026#34;k8s.io/client-go/kubernetes\u0026#34; exampleClient, err := clientset.NewForConfig(cfg)\t// sample clientset, \u0026#34;k8s.io/sample-controller/pkg/generated/clientset/versioned\u0026#34;  // 创建 Informer kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)\t// k8s informer, \u0026#34;k8s.io/client-go/informers\u0026#34; exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30)\t// sample informer, \u0026#34;k8s.io/sample-controller/pkg/generated/informers/externalversions\u0026#34;  // 创建 controller，传入 clientset 和 informer controller := NewController(kubeClient, exampleClient, kubeInformerFactory.Apps().V1().Deployments(), exampleInformerFactory.Samplecontroller().V1alpha1().Foos()) // 运行 Informer，Start 方法为非阻塞，会运行在单独的 goroutine 中 kubeInformerFactory.Start(stopCh) exampleInformerFactory.Start(stopCh) // 运行 controller controller.Run(2, stopCh) /* *** controller.go */ NewController() *Controller {} // 将 CRD 资源类型定义加入到 Kubernetes 的 Scheme 中，以便 Events 可以记录 CRD 的事件 \tutilruntime.Must(samplescheme.AddToScheme(scheme.Scheme)) //创建 Event Broadcaster \teventBroadcaster := record.NewBroadcaster() // ... ...  // 监听 CRD 类型Foo变化并注册 ResourceEventHandler方法，当Foo的实例变化时获取Foo资源并将其转换为 namespace/name字符(Key)，然后将其放入工作队列中。 \tfooInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueFoo, UpdateFunc: func(old, new interface{}) { controller.enqueueFoo(new) }, }) // 监听Deployment变化并注册ResourceEventHandler方法， \t// 当它的 ownerReferences 为 Foo 类型实例时，将该Foo资源加入 work queue \tdeploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.handleObject, UpdateFunc: func(old, new interface{}) { newDepl := new.(*appsv1.Deployment) oldDepl := old.(*appsv1.Deployment) if newDepl.ResourceVersion == oldDepl.ResourceVersion { return } controller.handleObject(new) }, DeleteFunc: controller.handleObject, }) func (c *Controller) Run(threadiness int, stopCh \u0026lt;-chan struct{}) error {} // 在启动 worker 前等待缓存同步 \tif ok := cache.WaitForCacheSync(stopCh, c.deploymentsSynced, c.foosSynced); !ok { return fmt.Errorf(\u0026#34;failed to wait for caches to sync\u0026#34;) } // 运行两个 worker 来处理资源 \tfor i := 0; i \u0026lt; workers; i++ { go wait.Until(c.runWorker, time.Second, stopCh) } // 无限循环，不断的调用 processNextWorkItem 处理下一个对象 \tfunc (c *Controller) runWorker() { for c.processNextWorkItem() { } } // 从workqueue中获取下一个对象并进行处理，通过调用 syncHandler \tfunc (c *Controller) processNextWorkItem() bool { //从workqueue获取obj \tobj, shutdown := c.workqueue.Get() if shutdown { return false } err := func(obj interface{}) error { // 调用 workqueue.Done(obj) 方法告诉 workqueue 当前项已经处理完毕， \t// 如果我们不想让当前项重新入队，一定要调用 workqueue.Forget(obj)。 \t// 当我们没有调用Forget时，当前项会重新入队 workqueue 并在一段时间后重新被获取。 \tdefer c.workqueue.Done(obj) var key string var ok bool // 格式校验，我们期望的是 key \u0026#39;namespace/name\u0026#39; 格式的 string \tif key, ok = obj.(string); !ok { // 无效的项调用Forget方法，避免重新入队。 \tc.workqueue.Forget(obj) utilruntime.HandleError(fmt.Errorf(\u0026#34;expected string in workqueue but got %#v\u0026#34;, obj)) return nil } //运行 syncHandler，传递Foo资源的namespace/name字符串 \tif err := c.syncHandler(key); err != nil { // 放回workqueue避免偶发的异常 \tc.workqueue.AddRateLimited(key) return fmt.Errorf(\u0026#34;error syncing \u0026#39;%s\u0026#39;: %s, requeuing\u0026#34;, key, err.Error()) } // 如果没有异常，Forget当前项，同步成功 \tc.workqueue.Forget(obj) klog.Infof(\u0026#34;Successfully synced \u0026#39;%s\u0026#39;\u0026#34;, key) return nil }(obj) if err != nil { utilruntime.HandleError(err) return true } return true } // 将实际状态与期望的状态进行比较，然后尝试将两者收敛，然后它用资源的当前状态更新Foo资源的Status块。 \tfunc (c *Controller) syncHandler(key string) error { // 通过 workqueue 中的 key 解析出 namespace 和 name \tnamespace, name, err := cache.SplitMetaNamespaceKey(key) // 调用 lister 接口通过 namespace 和 name 获取 Foo 实例 \tfoo, err := c.foosLister.Foos(namespace).Get(name) deploymentName := foo.Spec.DeploymentName // 获取 Foo 实例中定义的 deploymentname \tdeployment, err := c.deploymentsLister.Deployments(foo.Namespace).Get(deploymentName) //如果没有发现对应的 deployment，创建一个新的deployment。并还在资源上设置适当的 OwnerReferences，以便 handleObject 可以发现“拥有”它的 Foo 资源。 \tif errors.IsNotFound(err) { deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Create(newDeployment(foo)) } // deployment OwnerReferences 不是 Foo 实例，warning并返回错误 \tif !metav1.IsControlledBy(deployment, foo) { msg := fmt.Sprintf(MessageResourceExists, deployment.Name) c.recorder.Event(foo, corev1.EventTypeWarning, ErrResourceExists, msg) return fmt.Errorf(msg) } // deployment 中 的配置和 Foo 实例中 Spec 的配置不一致，即更新 deployment \tif foo.Spec.Replicas != nil \u0026amp;\u0026amp; *foo.Spec.Replicas != *deployment.Spec.Replicas { deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Update(newDeployment(foo)) } // 更新 Foo 实例状态 \terr = c.updateFooStatus(foo, deployment) c.recorder.Event(foo, corev1.EventTypeNormal, SuccessSynced, MessageResourceSynced) }  运行  # assumes you have a working kubeconfig, not required if operating in-cluster go build -o sample-controller . ./sample-controller -kubeconfig=$HOME/.kube/config # create a CustomResourceDefinition kubectl create -f artifacts/examples/crd-status-subresource.yaml # create a custom resource of type Foo kubectl create -f artifacts/examples/example-foo.yaml # check deployments created through the custom resource kubectl get deployments Kubernetes Controller 间通讯方式 Kubernetes 三大核心组建之一的kube-controller-manager，是运行Controller组建进程的控制平面组件，包含了如下Controller集合\nKubernetes中不同的Controller间也会进行通讯，以Deployment Controller为例子：\n 用户通过 Kubectl 创建 Deployment，APIServer接收到请求后会对该请求进行权限、准入校验，鉴权通过后将 Deployment 的资源信息存储到 Etcd中。 Deployment Controller 基于List/Watch机制，收到Deployment资源的Add事件并处理，为该 Deployment 创建 Replicaset。 APIServer 接收到 Replicaset创建请求后，Replicaset的Add事件将被发布，随后ReplicaSet Controller接收到该事件，进行对应的处理逻辑：创建 Pod。  可以看到，Kubernetes Controller基于事件订阅-分发的工作方式，进行Controller间的通信、协调操作；也得其开放的工作机制，让我们可以自由的定制、开发自己的Custom Controller。\n"
            }
    
        ,
            {
                "id": 4,
                "href": "https://chinalhr.github.io/post/docker-runc/",
                "title": "Docker容器运行时引擎-runC分析",
                "section": "post",
                "date" : "2021.04.15",
                "body": " 容器技术的演进，标准的OCI Runtime实现-runC源码分析\n Docker、containerd、runC、Kubernetes 从Docker到containerd、runC Docker 是一个开源工具，它可以将你的应用打包成一个标准格式的镜像，并且以容器的方式运行。Docker 容器将一系列软件包装在一个完整的文件系统中（代码、运行时工具、系统工具、系统依赖\u0026hellip;)，保证了容器内应用程序运行环境的稳定性，不会被容器外的系统环境所影响。\n2013年Docker宣布开源，初始阶段的Docker容器化基于LXC实现，镜像分层文件系统基于AUFS，核心功能都是基于当时的现成技术组装实现，但由于其工具生态完善、公共镜像仓库、组建重用、多版本支持、以应用为中心的容器封装等特性，这也使得Docker火速地流行了起来。\n 可以参考Docker相关的FAQ：https://docs.docker.com/engine/faq\n 在2014年开源了基于Golang开发的libcontainer，越过LXC直接操作系统内核模块，不必依赖LXC来提供容器化隔离能力了，自身架构也不断发展拆分成了如下几个模块：\n2015年，Docker联合多家公司制定了开放容器交互标准（Open Container Initiative），即OCI；制定了关于容器相关的规范，包含运行时标准（runtime-spec ）、容器镜像标准（image-spec）和镜像分发标准（distribution-spec）。\n2016年，Docker为了符合OCI标准，将原本libcontainer模块独立出来，封装重构成runC项目，捐献给了Linux基金会管理，而后为了能够兼容所有符合标准的OCI Runtime实现，Docker重构了Docker Daemon子系统，将其中与运行时交互的部分抽象为containerd项目 捐献给了CNCF基金会管理，内部为每个容器运行时创建一个containerd-shim适配进程，与runC搭配工作。\nDocker Daemon、containerd与runC containerd提供了镜像管理（镜像、元信息等）、容器运行（调用最终运行时组件执行）的功能，向上为 Docker Daemon 提供了 gRPC 接口，使得 Docker Daemon 屏蔽下面的结构变化，确保原有接口向下兼容；向下运行容器由符合OCI规范的容器支持，如runC，使得引擎可以独立升级。\nKubernetes与Docker Kubernetes 是一个自动化部署，缩放，以及容器化管理应用程序的开源系统，前身是Google内部运行多年的集群管理系统Borg。\n2014年Google使用Golang完全重写后开源，诞生后备受云计算相关的业界巨头追捧，成为云原生时代的操作系统，让复杂软件在云计算下获得韧性（Resilience）、弹性（Elasticity）、可观测性（Observability）的最佳路径。\n2015年Kubernetes发布了第一个正式版本，Google宣布与Linux基金会共同筹建云原生基金会（Cloud Native Computing Foundation，CNCF），并且将Kubernetes托管到CNCF，成为其第一个项目。\n随着Kubernetes的火速流行，自身的架构也在渐进演化，演进路线如下：\nKubernetes 1.5之前与Docker是强绑定的关系，Kubernetes 管理容器的方式都是通过内部的DockerManager向Docker Engine发送指令（HTTP方式），通过Docker来操作镜像；调用链路如下所示\n Kubernetes Master → kubelet → DockerManager → Docker Engine → containerd → runC Kubernetes 1.5版本开始引入容器运行时接口（Container Runtime Interface，CRI），CRI为定义容器运行时应该如何接入到kubelet的规范标准，内部的DockerManager也被KubeGenericRuntimeManager替代，已提供更通用的实现；kubelet与KubeGenericRuntimeManager之间通过gRPC协议通信。由于Docker不能直接支持CRI，所以Kubernetes提供了DockerShim服务作为Docker与CRI的适配层，实现了DockerManager的功能，演化到这里Docker Engine对Kubernetes来说只是一项默认依赖；调用链路如下所示：\n Kubernetes Master → kubelet → KubeGenericRuntimeManager → DockerShim → Docker Engine → containerd → runC 2017年，由Google、RedHat、Intel、SUSE、IBM联合发起的CRI-O（Container Runtime Interface Orchestrator）项目发布了首个正式版本。CRI-O是完全遵循CRI规范进行实现的，而且可以支持所有符合OCI运行时标准的容器引擎，默认使用runC搭配工作也可换成其他OCI运行时。此时Kubernetes可以选择使用CRI-O、cri-containerd、DockerShim作为CRI实现，演化到这里Docker Engine对Kubernetes来说只是一个可选项；调用链路如下所示：\n Kubernetes Master → kubelet → KubeGenericRuntimeManager → CRI-O→ runC 2018年，由Docker捐献给CNCF的containerd发布了1.1版，与1.0的区别是完全支持CRI标准，意味着原本用作CRI适配器的cri-containerd不再需要了，Kubernetes也在1.10版本宣布开始支持containerd 1.1，演化到这里Kubernetes已经可以完全脱开Docker Engine；调用链路如下所示：\n Kubernetes Master → kubelet → KubeGenericRuntimeManager → containerd → runC 2020年，Kubernetes官方宣布从1.20版本起放弃对Docker的支持，即不在维护DockerShim这一Docker与CRI的适配层；往后CRI适配器可以选择CRI-O或者containerd。\n容器运行时-runC分析 runc是一个CLI工具，用于根据OCI规范生成和运行容器，这里会通过runC源码简单分析其运行原理与流程。runC可以说是Docker容器中最为核心的部分，容器的创建，运行，销毁等等操作最终都将通过调用runC完成。\nOCI标准 image spec\nOCI 容器镜像标准包含如下几个模块：\n config.json：包含容器必需的元信息 rootfs：容器root文件系统，保存在一个文件夹里  config.json包含的元素如下所示，具体可以参考runtime-spec相关文档\nociVersion：指定OCI容器的版本号 root：配置容器的root文件系统 process：配置容器进程信息 hostname：配置容器的主机名 mounts：配置额外的挂载点 hook：配置容器生命周期触发钩子 runtime spec\nOCI 容器运行时标准主要是指定容器的运行状态，和 runtime 需要提供的命令，如下状态流转图所示：\n基于busybox创建OCI runtime bundle相关文件\ndocker pull busybox mkdir mycontainer \u0026amp;\u0026amp; cd mycontainer mkdir rootfs # 创建rootfs，基于busybox镜像 docker export $(docker create busybox) | tar -C rootfs -xvf - # 根据OCI规范来生成配置文件 runc spec 文件目录如下所示\n└── mycontainer ├── config.json └── rootfs ├── bin ├── dev ├── etc ├── home ├── proc ├── root ├── sys ├── tmp ├── usr └── var runC相关命令 通过runc -h可以看到runc支持的命令如下所示：\ncheckpoint checkpoint正在运行的容器，配合restore使用 create 创建容器 delete 删除容器及容器所拥有的任何资源 events 显示容器事件，如OOM通知，cpu，内存和IO使用情况统计信息 exec 在容器内执行新的进程 init 初始化namespace并启动进程 kill 发送指定的kill信号（默认值：SIGTERM）到容器的init进程 list 列出容器 pause 暂停容器内的所有进程 ps 显示在容器内运行的进程 restore 从之前的checkpoint还原容器 resume 恢复容器内暂停的所有进程 run 创建并运行容器 spec 创建specification文件 start 在创建的容器中执行用户定义的进程 state 输出容器的状态 update 更新容器资源约束，如cgroup参数 实践操作：\n# 1. 修改config文件中的process元素的terminal与args \u0026#34;process\u0026#34;: { \u0026#34;terminal\u0026#34;: false, \u0026#34;args\u0026#34;: [ \u0026#34;/bin/sleep\u0026#34;, \u0026#34;3600\u0026#34; ] # 2. runc create创建容器（此时状态为created,容器内进程为init进程） sudo runc create mycontainer # 3. runc start启动容器（此时状态为running，容器内进程为我们指定的进程） sudo runc start mycontainer # 4. runc 暂停/恢复容器（暂停时状态为pause） sudo runc pause mycontainer sudo runc resume mycontainer # 5. runc kill关闭容器（此时状态为stopped） sudo runc kill mycontainer # 6. runc delete删除容器 sudo runc delete mycontainer # 7. runc run组合命令，组合了runc create创建，runc start启动以及在退出之后runc delete的删除命令 sudo runc run mycontainer runC-run runC源码在github runc项目可以获取到，项目结构如下所示，其中如create.go、delete.go等文件为runC命令的实现；libcontainer目录为核心包（参考上文runC的由来），可见runC本质上是对libcontainer的一层封装，在libcontainer上加了一层OCI的适配操作与hook；main.go是入口文件，使用 github.com/urfave/cli 库进行命令行解析与执行函数：\n$ tree -L 1 -F --dirsfirst . ├── contrib/ ├── docs/ ├── libcontainer/ ├── man/ ├── script/ ├── tests/ ├── types/ ├── vendor/ ├── checkpoint.go ├── create.go ├── delete.go ├── events.go ├── exec.go ├── init.go ├── kill.go ├── list.go ├── main.go ├── pause.go ├── ps.go ├── restore.go ├── run.go ├── spec.go ├── start.go ├── state.go ... 通过run.go我们可以看到runCommand相关的执行逻辑：\nAction: func(context *cli.Context) error { if err := checkArgs(context, 1, exactArgs); err != nil { return err } if err := revisePidFile(context); err != nil { return err } spec, err := setupSpec(context) if err != nil { return err } status, err := startContainer(context, spec, CT_ACT_RUN, nil) if err == nil { os.Exit(status) } return err } 这里主要涉及四步操作：\n 参数校验 若指定了pidFile，进行路径转换 读取 config.json 文件转换成 spec 结构对象 根据配置启动容器  第四步是操作调用了utils_linux.go下startContainer方法进行容器创建、运行，具体实现如下：\nfunc startContainer(context *cli.Context, spec *specs.Spec, action CtAct, criuOpts *libcontainer.CriuOpts) (int, error) { id := context.Args().First() ... container, err := createContainer(context, id, spec) if err != nil { return -1, err } ... r := \u0026amp;runner{ enableSubreaper: !context.Bool(\u0026#34;no-subreaper\u0026#34;), shouldDestroy: true, container: container, listenFDs: listenFDs, notifySocket: notifySocket, consoleSocket: context.String(\u0026#34;console-socket\u0026#34;), detach: context.Bool(\u0026#34;detach\u0026#34;), pidFile: context.String(\u0026#34;pid-file\u0026#34;), preserveFDs: context.Int(\u0026#34;preserve-fds\u0026#34;), action: action, criuOpts: criuOpts, init: true, logLevel: logLevel, } return r.run(spec.Process) }  通过调用createContainer创建出逻辑容器（包含保存了namespace、cgroups、mounts\u0026hellip;容器配置） 创建runner对象，通过调用runner的run方法运行容器进程，并进行对应的设置  Factory创建逻辑容器\nfunc createContainer(context *cli.Context, id string, spec *specs.Spec) (libcontainer.Container, error) { ... config, err := specconv.CreateLibcontainerConfig(\u0026amp;specconv.CreateOpts{ CgroupName: id, UseSystemdCgroup: context.GlobalBool(\u0026#34;systemd-cgroup\u0026#34;), NoPivotRoot: context.Bool(\u0026#34;no-pivot\u0026#34;), NoNewKeyring: context.Bool(\u0026#34;no-new-keyring\u0026#34;), Spec: spec, RootlessEUID: os.Geteuid() != 0, RootlessCgroups: rootlessCg, }) if err != nil { return nil, err } factory, err := loadFactory(context) if err != nil { return nil, err } return factory.Create(id, config) }  通过specconv.CreateLibcontainerConfig把spec转换成libcontainer内部config对象 通过factory 模式获取对应平台的factory实现（目前仅linux平台支持）,如libcontainer/factory_linux.go，并调用其Create方法构建返回libcontainer.Container 对象（包含容器命令的实现，如Run、Start、State\u0026hellip;） 注意libcontainer/factory_linux.goNew方法的InitPath为/proc/self/exe,InitArgs为init  func (l *LinuxFactory) Create(id string, config *configs.Config) (Container, error) { ... c := \u0026amp;linuxContainer{ id: id, root: containerRoot, config: config, initPath: l.InitPath, initArgs: l.InitArgs, criuPath: l.CriuPath, newuidmapPath: l.NewuidmapPath, newgidmapPath: l.NewgidmapPath, cgroupManager: l.NewCgroupsManager(config.Cgroups, nil), } if l.NewIntelRdtManager != nil { c.intelRdtManager = l.NewIntelRdtManager(config, id, \u0026#34;\u0026#34;) } c.state = \u0026amp;stoppedState{c: c} return c, nil } func New(root string, options ...func(*LinuxFactory) error) (Factory, error) { ... l := \u0026amp;LinuxFactory{ Root: root, InitPath: \u0026#34;/proc/self/exe\u0026#34;, InitArgs: []string{os.Args[0], \u0026#34;init\u0026#34;}, Validator: validate.New(), CriuPath: \u0026#34;criu\u0026#34;, } Cgroupfs(l) for _, opt := range options { if opt == nil { continue } if err := opt(l); err != nil { return nil, err } } return l, nil } Runner运行容器-container_linux.start\nrunner的run方法会根据OCI specs.Process生成 libcontainer.Process逻辑process对象（包含进程的命令、参数、环境变量、用户\u0026hellip;）；根据对应的command如run会调用container的Run方法，在linux平台下最终会调用container_linux的start方法与exec方法，start方法如下所示：\nfunc (c *linuxContainer) start(process *Process) (retErr error) { parent, err := c.newParentProcess(process) if err != nil { return newSystemErrorWithCause(err, \u0026#34;creating new parent process\u0026#34;) } // \tif err := parent.start(); err != nil { return newSystemErrorWithCause(err, \u0026#34;starting container process\u0026#34;) } ... //更新容器状态，PostStart Hook回调 \treturn nil } func (c *linuxContainer) newParentProcess(p *Process) (parentProcess, error) { parentInitPipe, childInitPipe, err := utils.NewSockPair(\u0026#34;init\u0026#34;) if err != nil { return nil, newSystemErrorWithCause(err, \u0026#34;creating new init pipe\u0026#34;) } messageSockPair := filePair{parentInitPipe, childInitPipe} parentLogPipe, childLogPipe, err := os.Pipe() if err != nil { return nil, fmt.Errorf(\u0026#34;Unable to create the log pipe: %s\u0026#34;, err) } logFilePair := filePair{parentLogPipe, childLogPipe} cmd := c.commandTemplate(p, childInitPipe, childLogPipe) if !p.Init { return c.newSetnsProcess(p, cmd, messageSockPair, logFilePair) } if err := c.includeExecFifo(cmd); err != nil { return nil, newSystemErrorWithCause(err, \u0026#34;including execfifo in cmd.Exec setup\u0026#34;) } return c.newInitProcess(p, cmd, messageSockPair, logFilePair) } newParentProcess方法执行如下操作：\n 创建parentPipe 和 childPipe 进程通信管道用于父进程与容器内父进程进行通信 cmd封装容器父进程运行模板/proc/self/exe - init 创建initProcess，即容器内父进程逻辑对象，包含（cmd、逻辑process对象、paerentPipe和childPipe等）  newParentProcess返回initProcess后container_linux的start方法会调用initProcess的start方法\nfunc (p *initProcess) start() (retErr error) { defer p.messageSockPair.parent.Close() //启动容器init进程 \terr := p.cmd.Start() p.process.ops = p p.messageSockPair.child.Close() p.logFilePair.child.Close() waitInit := initWaiter(p.messageSockPair.parent) ... //将容器pid加入到cgroup 中 \tif err := p.manager.Apply(p.pid()); err != nil { return newSystemErrorWithCause(err, \u0026#34;applying cgroup configuration for process\u0026#34;) } if p.intelRdtManager != nil { if err := p.intelRdtManager.Apply(p.pid()); err != nil { return newSystemErrorWithCause(err, \u0026#34;applying Intel RDT configuration for process\u0026#34;) } } //向容器init进程发送bootstrapData，即初始化数据 \tif _, err := io.Copy(p.messageSockPair.parent, p.bootstrapData); err != nil { return newSystemErrorWithCause(err, \u0026#34;copying bootstrap data to pipe\u0026#34;) } //等待容器init进程初始化 \terr = \u0026lt;-waitInit if err != nil { return err } ... //创建网络 interface \tif err := p.createNetworkInterfaces(); err != nil { return newSystemErrorWithCause(err, \u0026#34;creating network interfaces\u0026#34;) } //更新Spec状态 \tif err := p.updateSpecState(); err != nil { return newSystemErrorWithCause(err, \u0026#34;updating the spec state\u0026#34;) } // 给容器init进程发送进程配置信息 \tif err := p.sendConfig(); err != nil { return newSystemErrorWithCause(err, \u0026#34;sending config to init process\u0026#34;) } //和容器init进程进行同步,待其初始化完成之后，执行Hook回调操作，设置cgroup，最后，关闭init管道，容器创建完成 \tierr := parseSync(p.messageSockPair.parent, func(sync *syncT) error { ... } ... } initProcess的start操作主要涉及如下几个步骤：\n 执行cmd命令，通过/proc/self/exe - init启动容器init进程执行init command操作 向容器init进程发送bootstrapData初始化数据 创建网络 interface 和容器init进程进行同步,待其初始化完成之后，执行Hook回调操作，设置cgroup，最后，关闭init管道，容器创建完成  Runner运行容器-container_linux.exec\n具体实现如下\nfunc (c *linuxContainer) exec() error { path := filepath.Join(c.root, execFifoFilename) pid := c.initProcess.pid() blockingFifoOpenCh := awaitFifoOpen(path) for { select { case result := \u0026lt;-blockingFifoOpenCh: return handleFifoResult(result) case \u0026lt;-time.After(time.Millisecond * 100): stat, err := system.Stat(pid) if err != nil || stat.State == system.Zombie { if err := handleFifoResult(fifoOpen(path, false)); err != nil { return errors.New(\u0026#34;container process is already dead\u0026#34;) } return nil } } } } 通过打开路径exec.fifo的管道，调用readFromExecFifo从管道中将容器init进程从写入的信息读出。一旦管道中的数据被读出，容器init进程将不再被阻塞，完成之后的Exec系统调用，这时候容器init进程切换为command指定的进程。\nrunC-init 上文分析runc run流程，在最后通过/proc/self/exe - init启动容器init进程执行init command操作，这里直接追溯到init操作的核心方法standard_init_linux.go下的Init方法，如下所示：\nfunc (l *linuxStandardInit) Init() error { //配置容器网络与路由 \tif err := setupNetwork(l.config); err != nil { return err } if err := setupRoute(l.config.Config); err != nil { return err } //设置rootfs，挂载文件系统，chroot \tif err := prepareRootfs(l.pipe, l.config); err != nil { return err } //配置console, hostname, apparmor, sysctl，seccomp，user namespace... \t... //同步父进程 init进程准备执行exec \tif err := syncParentReady(l.pipe); err != nil { return errors.Wrap(err, \u0026#34;sync ready\u0026#34;) } ... //已完成初始化，关闭管道 \tl.pipe.Close() ... //向exec.fifo管道写入数据，阻塞到exec执行，读取管道中的数据 \tfd, err := unix.Open(\u0026#34;/proc/self/fd/\u0026#34;+strconv.Itoa(l.fifoFd), unix.O_WRONLY|unix.O_CLOEXEC, 0) if err != nil { return newSystemErrorWithCause(err, \u0026#34;open exec fifo\u0026#34;) } if _, err := unix.Write(fd, []byte(\u0026#34;0\u0026#34;)); err != nil { return newSystemErrorWithCause(err, \u0026#34;write 0 exec fifo\u0026#34;) } //调用Exec系统命令，执行用户进程 \tif err := unix.Exec(name, l.config.Args[0:], os.Environ()); err != nil { return newSystemErrorWithCause(err, \u0026#34;exec user process\u0026#34;) } return nil } 这里涉及如下几步操作：\n 配置容器网络与路由 设置rootfs，挂载文件系统，chroot 配置console, hostname, apparmor, sysctl，seccomp，user namespace 同步父进程，让父进程执行hook等操作 调用Exec系统命令，执行用户进程，此时容器的init进程会替换成用户进程  最后，在Init执行之前，会被/runc/libcontainer/nsenter劫持，在Go的runtime之前执行对应的C代码，从init管道中读取容器的配置，设置namespace，调用setns系统调用，将容器init进程加入到对应的namespace中。\nrunC-run-init-运行流程 runC-run运行流程\nrunC-init运行流程\n"
            }
    
        ,
            {
                "id": 5,
                "href": "https://chinalhr.github.io/post/distributed-systems-consensus-algorithm-paxos/",
                "title": "分布式系统的一致性与共识算法-Paxos",
                "section": "post",
                "date" : "2021.03.11",
                "body": " 本文讨论共识算法：Paxos\n 关于Paxos Paxos算法是Leslie Lamport于1990年提出的一种基于消息传递共识算法，能保证多副本数据强一致性与分区容错性；现已是当今分布式系统最重要的理论，为后续的如Raft、ZAB等算法、ZooKeeper、Etcd等分布式协调框架奠定了基础。\nGoogle Chubby的作者给了Paxos极高的评价：There is only one consensus protocol, and that\u0026rsquo;s “Paxos” — all other approaches are just broken versions of Paxos（世界上只有一种共识协议，就是Paxos，其他所有共识算法都是Paxos的退化版本）\n问题-假设 问题：基于前文对分布式环境下一致性与共识算法的基础理论，在分布式系统中进行节点通信大部分采用基于消息传递通信模型，不可避免的会发生如进程可能会慢、被杀死或者重启等问题，会对分布式系统中各个节点对某一个值达成一致性产生问题；而Paxos就是为了解决这个问题而生的。\n场景：在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点都执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。\n假设：Lamport虚构了一个名为Paxos的希腊城邦，这个城邦按照民主制度制定法律，却又不存在一个中心化的专职立法机构，立法靠着“兼职议会”（Part-Time Parliament）来完成，无法保证所有城邦居民都能够及时地了解新的法律提案、也无法保证居民会及时为提案投票。Paxos算法的目标就是让城邦能够在每一位居民都不承诺一定会及时参与的情况下，依然可以按照少数服从多数的原则，最终达成一致意见。注意：Paxos算法并不考虑拜占庭将军问题，即假设信息可能丢失也可能延迟，但不会被错误传递。\nPaxos算法运行在允许宕机故障的异步系统中，不要求可靠的消息传递，可容忍消息丢失、延迟、乱序以及重复，保证了2F+1的容错能力，即2F+1个节点的系统最多允许F个节点同时出现故障。\n算法推演 首先回看上文在解决分布式环境下复制带来的副本一致性问题时，我们提到可以通过两类基本复制算法解决：Replication methods that prevent divergence (single copy systems) 与Replication methods that risk divergence (multi-master systems) 。\n以Replication methods that prevent divergence为例，可以通过如Master/Slave（主从复制）、2-phase commit（两阶段提交）、Quorum机制(多数派读写) 方式实现，但都或多或少存在着问题；本文讨论的Paxos可以看作是对Quorum机制（多数派读写）的进一步升级。\n0x01.Paxos算法中对应的角色  Proposer：提出提案 (Proposal)；可以理解为客户端，Proposal信息包括提案编号 (Proposal ID) 和提议的值 (Value)。 Acceptor：参与决策，可以理解为存储节点，回应Proposers的提案。收到Proposal后可以接受提案，若Proposal获得多数派Acceptors的接受，则称该Proposal被批准。 Learners：用于学习被批准的提案  Paxos算法中的角色允许身兼数职，也有了如下的基本约束：\n 决策（value）只有在被 proposers 提出后才能被批准（未经批准的称为提案）； 在一次 Paxos 算法的执行实例中，只批准（chosen）一个 value； Learners 只能获得被批准（chosen）的 value。  作者Lamport主要通过不断加强上述3个约束（主要是第二个）获得了 Paxos 算法。\n0x02. 系统模型 一个系统中，存在多个Proposer节点提出提案，多个Acceptor节点负责决策-接受提案。为了满足只批准一个 value 的约束，要求经Quorum（多数派）接受的 value 成为正式的决议。即一个提案被选定需要被半数以上的Acceptor接受。\nQuorum机制下，假设只有一个Proposer提出了一个value，该value也会被决策，要保证约束2，就会产生P1约束P1：一个Acceptor必须接受第一次收到的提案\nP1 是不完备的，不同的Proposer提出不同的value的话，如果遵循P1，就会出现无法形成多数派的情况；因为存在多个提案，这里就需要给每个提案加上一个提案编号以表示顺序，即提案=编号+Value；只要提案的 value 是一样的，批准多个提案不违背约束2，我们就可以保证只有一个值被选中，可以得到如下约束P2：如果某个value为v的提案被批准（chosen），那么之后批准（chosen）的提案必须具有 value v\n因为一个提案只有被Acceptor接受才可能被选定，所以我们可以把P2约束改写成对Acceptor接受的提案的约束P2a：一旦一个具有 value v 的提案被批准（chosen），那么之后任何 Acceptor 再次接受（accept）的提案必须具有 value v\n因为通信是异步的，系统是不可靠的，P2a和P1可能会存在冲突，例如一个 value 被批准后，一个Proposer 和一个 Acceptor 从休眠中苏醒，前者提出一个具有新的 value 的提案；这种情况下根据 P1，Acceptor应当接受，根据 P2a，则不应当接受（如下图所示）， P2a 和 P1 有矛盾。于是需要换个思路，转而对 proposer 的行为进行约束得到P2b：一旦一个具有 value v 的提案被批准（chosen），那么以后任何 Proposer 提出的提案必须具有 value v。\n由于 acceptor 能接受的提案都必须由 proposer 提出，所以 P2b 蕴涵了 P2a，是一个更强的约束。但是根据 P2b 难以提出实现手段，需要进一步加强 P2b；假设一个编号为 m 的 Value v 已经获得批准，存在一个 Acceptors 的多数派 C，他们都接受（accept）了v，考虑到任何多数派都和 C 具有至少一个公共成员，可以找到一个蕴涵 P2b 的约束 P2c：如果一个编号为 n 的提案具有 value v，该提案被提出，那么存在一个多数派，要么他们中所有人都没有接受（accept）编号小于 n 的任何提案，要么他们已经接受（accept）的所有编号小于 n 的提案中编号最大的那个提案具有 value v。\n要满足P2c的约束，会涉及两个流程：\n prepare阶段：Proposer提出一个提案前，要和足以形成多数派的Acceptors进行通信，获得他们进行的最近一次接受（accept）的提案，根据回收的信息决定这次提案的value，形成提案开始投票。 批准阶段：当获得多数acceptors接受（accept）后，提案获得批准（chosen），由Acceptor将这个消息告知learner（这个过程逐渐细化，就形成了Paxos算法）  并发情况下：如果一个没有chosen过任何proposer提案的Acceptor在prepare过程中回答了一个proposer针对提案n的问题，但是在开始对n进行投票前，又接受（accept）了编号小于n的另一个提案（例如n-1），如果n-1和n具有不同的value，这个投票就会违背P2c。因此需要对P1进行加强，在prepare过程中，acceptor进行的回答后不会再接受（accept）编号小于n的提案，P1a：当且仅当Acceptor没有回应过编号大于n的prepare请求时，Acceptor接受（accept）编号为n的提案。\n0x03.prepare阶段：为什么需要获取多数派Acceptor最近接受的提案 如果两个Proposer进程并发进行读写操作, 在多数派读写的实现中,会产生一个Proposer覆盖另一个Proposer的问题. 从而产生了数据更新点丢失的情况\n已上图为例子：\n问题：Proposer2步骤④进行多数派写的时候，因为并发问题覆盖了Proposer1的多数派写操作，导致Proposer1写入的值失效。\n如何解决：Proposer2更新的时候不能直接更新i2版本，而是应该检测到i2的存在, 进而将自己的结果保存在下一个版本i3中,再进行多数派写。问题可推广：系统对于i的某个版本，只能有一次写入—\u0026gt; 一个值（变量的一个版本）被确定了之后，不允许进行修改\n解决方案：每次Proposer写之前进行一次多数派读，以便确认是否存在其他Proposer在写；如果存在，则放弃写入；这种操作称为写前读取操作。\n0x04.并发进行写前读取操作（确定一个值）导致数据不一致问题 问题：可能出现两个Proposer同时进行写前读取操作，获取到的结果都是没有其他Proposer在写入;这时候同时进行写操作，还是会造成数据不一致的情况。\n如何解决：Acceptor节点需要记录最后一个做过写前读取操作的Proposer；进行限制，只允许最后一个完成写前读取的Proposer可以进行后续写入,拒绝之前做过写前读取的Proposer写入的权限。\n已上便是Paxos的核心原理，通过2次多数派读写, 实现了强一致性的共识算法。\n0x05.提案的提出与批准  prepare阶段  Proposer选择一个提案编号n并将prepare请求发送给Acceptors中的一个多数派； Acceptor收到一个编号为N的prepare消息后，且N大于该Acceptor已经响应过的所有Prepare请求的编号，那么它就会将它已经接受过的编号最大的提案作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。   批准阶段  如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，进入批准阶段。Proposer会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。（V为prepare阶段响应中编号最大的提案的value，如果响应中不包含任何提案，那么V就由Proposer决定） 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，就接受该提案。    Basic Paxos Basic Paxos为最基本的Paxos实现，上文所述即为Basic Paxos，通过两轮RPC确定某一个提案（Value）\n要实现Basic Paxos，算法中的各个角色需要拥有如下功能：\nGlobal Round：表示一轮Paxos,包含了prepare阶段与批准阶段 rnd：全局递增，全局唯一的编号，每一个Round对应一个rnd，即提案编号 Acceptor last_rnd：Acceptor记住的最后一次发起prepare的Proposer对应的提案编号, 以此来批准哪个Proposer的提案 v：Acceptor记住最后被批准的值 vrnd：一组rnd和v，记录了v在哪个rnd被写入 算法实例（prepare阶段、批准阶段） 如下所示为一个不存在冲突的基本流程：\nprepare阶段\n Acceptor  Proposer发起的提案的rnd小于Acceptor的last_rnd（并发情况下，获取存在网络延迟），Acceptor会拒绝请求 Acceptor会把Proposer请求的rnd保存到本地为last_rnd，后续批准阶段Acceptor只接受带这个rnd为last_rnd的请求 之后Acceptor会返回last_rnd与之前已接受的v、vrnd   Propose  Proposer 收到多数(quorum)Acceptor的应答, 就可以执行后续的批准阶段，此时如果没收到多数(quorum)Acceptor的应答,Proposer就hang住了(paxos只能运行少于半数的节点失效的情况) 收到多数(quorum)Acceptor的应答后，如果所有Acceptor的v都为null、vrnd为0，表示所有的Acceptor没有批准过任何提案，Proposer可以选择自己要写入的v进行写入 收到多数(quorum)Acceptor的应答后，如果Acceptor的v不为null、vrnd不为0，Proposer选择要写入的v为Acceptor应答的v    批准阶段\n Acceptor  在并发提案的情况下，可能已经有其他Proposer又完成了一个rnd更大的prepare, 所以这时不一定能成功运行完prepare阶段；Acceptor会对比请求中的rnd和本地记录的last_rnd,如果请求的rnd=last_rnd，则允许请求；如果请求的rnd≠last_rnd，则拒绝请求 接受请求后，Acceptor会将v写入本地，更新vrnd   Propose  如果Propose在批准阶段被拒绝请求，表示有其他Proposer在进行、完成一个rnd更大的paxos；这时候Propose需要重新发起一轮paxos，修复可能存在已经中断的其他Proposer的运行，如下图所示    算法实例（learner学习阶段） Learner需要学习最终被选定的Value，一般可以通过以下方式进行通知\n Acceptor接收提案后，直接发送给所有Learner（通信次数较多，但Learner可以快速获取到被批准提案） Acceptor接收提案后，发给主Learner，主Learner再发送给其他Learner（通信次数较少，但容易出现单点故障） Acceptor接收提案后，发给一个Learner集合，再进行广播（通信次数较少，可靠性高，但网络通信复杂度也高）  算法活性 如果有两个Propose A 和 B，A通过Prepare阶段发送rnd 1，B又通过Prepare阶段发送rnd 2，这个时候 A 因为rnd2 无法通过又执行Prepare阶段发送rnd 3… 依次反复陷入了死循环即活锁，使程序无法取得进展。为了保证进展性，需要选择一个唯一的提议者进行提议，可以通过一些随机性技术实现。\nMulti Paxos Basic Paxos缺陷 Basic Paxos只能对单个值形成决议，并且一次Round的形成需要进行两轮RPC（prepare阶段、决策阶段），网络开销大，效率低，对工业化不友好；Base Paxos任何一个proposer节点都是平等的、可以与其他节点并发地提出提案因而带来的更大的实现复杂度。\n也可以理解为Paxos是解决对某一个问题达成一致的一个协议，但是实际生产中大部分的应用场景是对一堆连续的问题达成一致，所以基本上Basic Paxos几乎只被用作理论研究，实际应用都是基于更工业化的Multi Paxos或者Fast Paxos。\nMulti Paxos的优化 Multi Paxos对Basic Paxos的核心优化点是增加了Leader节点，只有Leader才能提出提案；再进一步简化节点角色，节点只有主（Leader）和从（Follower）的区别，Follwer节点接收到提案请求后，会转发给Leader节点进行发起提案。\n因为只有Leader节点可以发起提案，可以视为对提案的批准处于无并发的全局有序环境中，可以支持批量为多次提案运行prepare阶段，此时对某个提案达成一致只需要进行一轮次的RPC请求，即批准（accept）阶段。\n0x01.如何进行Leader选举 Leader选举的过程，可以理解为分布式系统各个节点对“申请Leader”这个提案达成一致。节点启动时，默认处于 Follower 的状态，各个节点会通过心跳的方式定时轮询，确定集群中的Leader节点是否存在；如果不存在Leader节点的情况节点会在心跳超时后基于Basic Paxos的prepare、批准两个阶段向所有其他节点广播竞选Leader的请求，提案被批准后则成为Leader节点。\nLeader带有任期属性（一个单调递增的编号），目的是在Leader节点陷入网络分区、宕机后重新恢复，但另外一部分节点仍存在多数派，且已经完成了重新选主的情况，此时必须以任期编号大的主节点为准。\n0x02.如何进行数据复制 基本状态机模型，如下图所示：\nMulti Paxos在数据复制上采用的是confirm机制，在Basic Paxos协议中，对于决议（value）的读取也是需要执行一轮多数派读取过程的，在实际工程中做数据恢复时（如Leader切换后），对每条日志都执行一轮多数派读代价过大，引入confirm机制的目的在于解决这个问题。\nconfirm机制\nLeader持久化一条日志的时候，客户端向Leader节点发起一个操作请求（如某个值的add command ），进行如下几步操作：\n Leader节点将command写入自己的变更日志，然后将command信息在下一次心跳包中广播给所有的Follower节点 Follower节点接收到信息，将command写入自己的日志，然后给主节点发送确认的消息 Leader节点获得多数派Follower节点的确认后，Leader节点提交自己的confirm、响应客户端并且给Follower节点广播该日志可以confirm的消息，Follower节点收到消息后confirm自己的变更  在回放日志时，判断如果一条日志有对应的confirm日志，则可以直接读取本地内容，不需要再执行一轮多数派读。\n网络传输与日志空洞\nMulti Paxos中的每一个命令都有一个递增的编号即logID，如果顺序的发布提案，效率会非常低下，所以Multi Paxos采用类似TCP滑动窗口的方案，实现基于logID的滑动窗口机制，可以每次发送N个提案对N个提案进行表决，以增加带宽。\nTCP协议中如果TCP包在传输中丢失，最坏的情况是会RST这条链接，然后交给应用层逻辑来解决；对于Multi Paxos如果某些提案因为网络或者其他原因没有被表决，那么就会存在日志空洞(即不连续的日志)如下图所示，Multi Paxos 是允许日志空洞这种情况存在的。\n异常情况-Leader切换\n如果网络出现了分区，部分节点失联，但只要仍能正常工作的节点的数量能够满足多数派（过半数）的要求，系统就仍可以正常运行。\n如下图所示的情况，Node B是任期1的Leader节点，由于网络故障出现了Node A、B和NodeC、D、E两个分区，这时候NodeC、D、E分区的节点通过心跳获知分区内不存在主节点，会进行一次Leader选举，节点数量满足多数派要求，NodeC 当选Leader；此时系统存在NodeC、B两个不同任期的Leader节点。\n客户端请求：如果请求到Node A、B分区，不构成多数派的批准，无法进行confirm，如果请求到NodeC、D、E，构成多数派的批准，可以进行confirm，系统可以继续提供服务。\n故障恢复：网络问题恢复后，分区会解除，这时候集群内存在两个Leader会以任期编号更大的为主；Node A、B分区的节点会回滚所有未confirm的日志，并进行数据恢复，从Leader节点也就是NodeC发送的心跳包中获得它们失联期间发生的所有变更，并进行confirm操作。\nLeader切换后新任Leader对日志重确认\n 新Leader竞选成功后，拥有对应confirm日志的原始日志，可以直接回放日志；而没有对应confirm日志的原始日志，则需要执行一轮paxos进行重确认操作。 新Leader在上一任Leader的任期内可能存在日志空洞，这些日志空洞也需要进行重确认来补全。 重确认操作的结束位置可以根据中所有Node的最大logID来作为重新确认操作的结束位置。  类Multi Paxos算法 Multi Paxos作为Basic Paxos的改进版本，仅在Paxos的论文中最后Lamport提供了改进的思路，在工程上实现一般都基于原算法的基础上进行一系列的改进，就有了如Raft、ZAB等类Multi Paxos算法；实现思路都是基于Multi Paxos，但是具体实现上都有微小差别，如：Leader竞选的条件、日志是否连续（是否允许存在日志空洞）、Follower节点获取日志confirm操作的方式。\n以Raft为例：\n Raft仅允许日志最多的节点当选为Leader，而Multi Paxos允许任意节点当选Leader Raft不允许出现日志空洞，便于做Leader切换后的日志重确认，而Multi Paxos允许，实现过程复杂了些  参考 https://zh.wikipedia.org/wiki/Paxos算法\nhttps://blog.openacid.com/algo/paxos\nhttps://icyfenix.cn/distribution/consensus/paxos.html\nhttps://github.com/dappFinance/paxos_raft_protocol\n"
            }
    
        ,
            {
                "id": 6,
                "href": "https://chinalhr.github.io/post/distributed-systems-consistency-consensus-algorithm/",
                "title": "分布式系统的一致性与共识算法-基础理论",
                "section": "post",
                "date" : "2021.01.25",
                "body": " 记录分布式系统的一些基础理论\n 分布式系统下的一致性问题 定义：一致性为在分布式系统领域中对于多个服务节点，给定一系列操作，在约定协议的保障下，使得它们对处理结果达成某种程度的协同。\n分布式系统中的节点通信存在两种模型：共享内存（Shared memory）和消息传递（Messages passing）。基于消息传递通信模型的分布式系统，不可避免的会发生以下错误：进程可能会慢、被杀死或者重启，消息可能会延迟、丢失、重复；发生上面任意一种异常都会对分布式系统中各个节点对某一个值达成一致性产生问题。\n一致性的要求：\n 可终止性（Termination）：一致的结果在有限时间内能完成（可以保障提供服务（Liveness）） 约同性（Agreement）：不同节点最终完成决策的结果是相同的（意味着算法要么不给出结果，任何给出的结果必定是达成了共识的，即安全性（Safety）） 合法性（Validity）：决策的结果必须是某个节点提出的提案（即达成的结果必须是节点执行操作的结果）  解决一致性问题的核心在于对不同空间发生的事件进行全局唯一排序。\n一致性模型：\n 强一致性模型  顺序一致性：所有操作都以某种顺序原子执行，该顺序与各个节点上看到的顺序一致，并且在所有节点上都相等；可以基于Lamport时间戳（逻辑时钟）进行实现。 线性一致性：所有操作都按照操作的全局实时顺序一致的顺序自动执行；在顺序一致性前提下加强了进程间的操作排序，形成唯一的全局顺序；依赖于全局的时钟或锁，有很强的原子性保证，但是比较难实现。   弱一致性模型  最终一致性：在未来的某个时间点进行冲突检测和修正，如DNS 客户端为中心型一致性：通过在client端库中建立额外的缓存来实现，如亚马逊Dynamo    拜占庭将军问题 拜占庭将军问题是 Leslie Lamport 在《The Byzantine Generals Problem》论文中提出的分布式对等网络通信容错问题，为分布式领域中最复杂、最严格的容错模型；拜占庭将军问题讨论的是允许存在少数节点作恶（消息可能被伪造）场景下的如何达成共识问题。\n拜占庭是古代东罗马帝国的首都，由于地域宽广，守卫边境的多个将军（类比系统中的多个节点）需要通过信使来传递消息，达成某些一致决定。但由于将军中可能存在叛徒（类比系统中节点出错），这些叛徒将向不同的将军发送不同的消息，试图干扰共识的达成；拜占庭问题即讨论在此情况下，如何让忠诚的将军们能达成行动的一致。\n在该模型下，系统不会对集群中的节点做任何的限制，它们可以向其他节点发送随机数据、错误数据，也可以选择不响应其他节点的请求，这些无法预测的行为使得容错这一问题变得更加复杂。\n早期解决方案\n在“拜占庭将军”的问题可以在“军官与士官的问题”里解决，以降低将军问题的发生。而所谓的“军官与士官的问题”，就是探讨军官与他的士官是否能忠实实行命令。\n解决方案1：即使出现了伪造或错误的消息。只要有问题的将军的数量不到三分之一，仍可以达到“拜占庭容错”。比如有军官A，士官B与士官C。当A要求B进攻，却要求C撤退时。就算B与C交换所收到的命令，B与C仍不能确定是否A有问题，因为B或C可能将窜改了的消息传给对方。以函数来表示，将军的总数为n，n里面背叛者的数量为t，则只要n \u0026gt; 3t就可以容错。\n解决方案2：通过数字签名实现拜占庭容错，并以此来找到有问题的将军。\n实用拜占庭容错算法\nPractical Byzantine Fault Tolerance，PBFT（实用拜占庭容错算法），由 Castro 和 Liskov 于论文《Practical Byzantine Fault Tolerance and Proactive Recovery》中提出，基于BFT做了改进，采用密码学相关技术（RSA 签名算法、消息验证编码和摘要）确保消息传递过程无法被篡改和破坏；该算法能提供高性能的运算，使得系统可以每秒处理成千的请求\n过程如下：\n1. 通过轮换或随机算法选出某个节点为主节点，此后只要主节点不切换，则称为一个视图（View）。 2. 在某个视图中，客户端将请求 \u0026lt;REQUEST,operation,timestamp,client\u0026gt; 发送给主节点，主节点负责广播请求到所有其它副本节点。 3. 所有节点处理完成请求，将处理结果 \u0026lt;REPLY,view,timestamp,client,id_node,response\u0026gt; 返回给客户端。客户端检查是否收到了至少 f+1 个来自不同节点的相同结果，作为最终结果。 共识算法 共识（Consensus）与一致性（Consistency） 一致性：含义比共识宽泛，在不同场景（基于事务的数据库、分布式系统等）下意义不同。在分布式系统场景下，一致性指的是多个副本对外呈现的状态。如之前提到的顺序一致性、线性一致性，描述了多节点对数据状态的共同维护能力。\n共识：特指在分布式系统中多个节点之间对某个事情达成一致看法的过程。需注意达成某种共识并不意味着就保障了一致性。\n共识算法解决的问题 共识算法解决的是分布式系统对某个提案（Proposal），大部分节点达成一致意见的过程。提案泛指多个事件发生的顺序、某个键对应的值\u0026hellip;对于分布式系而言，各个节点通常都是相同的确定性状态机模型（又称为状态机复制问题，State-Machine Replication），从相同初始状态开始接收相同顺序的指令，则可以保证相同的结果状态。\n这里共识算法需要解决两个基本问题：\n 如何提出一个待共识的提案（令牌传递、随机选取\u0026hellip;） 如何让多个节点对提案达成共识（投票、规则验证\u0026hellip;）  现实网络环境中存在各种各样的问题，在分布式环境下，共识算法还需要解决如通信问题（网络中断、分区）、节点故障、消息伪造\u0026hellip;\n共识算法分类 根据是否允许拜占庭错误（伪造信息恶意响应）的情况，共识算法分为 Crash Fault Tolerance (CFT) 和 Byzantine Fault Tolerance（BFT）两类。\nCrash Fault Tolerance (CFT) 算法：Paxos、Raft、ZAB\u0026hellip;\nByzantine Fault Tolerance(BFT) 算法：PBFT为代表的确定性系列算法、PoW为代表的概率算法\u0026hellip;\nFLP不可能定理、CAP理论、ACID原则与BASE原则 同步/异步系统模型 同步系统模型：指系统中的各个节点的时钟误差存在上限；并且消息传递必须在一定时间内完成，否则认为失败；同时各个节点完成处理消息的时间是一定的。因此同步系统中可以很容易地判断消息是否丢失。\n异步系统模型：系统中各个节点可能存在较大的时钟差异；同时消息传输时间是任意长的；各节点对消息进行处理的时间也可能是任意长的。这就造成无法判断某个消息迟迟没有被响应是哪里出了问题（节点故障还是传输故障）。现实生活中的系统往往都是异步系统。\nFLP不可能原理 定义：由 Fischer，Lynch 和 Patterson 三位科学家发表的《Impossibility of Distributed Consensus with One Faulty Process》论文中提出，在网络可靠，但允许节点失效（即便只有一个）的最小化异步模型系统中，不存在一个可以解决一致性问题的确定性共识算法。\n描述：FLP不可能原理假定节点只能因崩溃而失败； 网络可靠，并且异步系统模型的典型时序假设成立：例如，消息延迟没有限制的情况下，假设有A、B、C三个节点进行投票，A投票0，B投票1，而C收到了A与B的投票却没办法响应，A与B就没办法在有限的时间内获知最终结果；如果进行重新投票，类似的情况重复发生，则永远无法达到共识。\nFLP 不可能原理的意义在于，告诉我们不要浪费时间去为异步分布式系统设计在任意场景上都能够实现共识的算法，异步系统完全没有办法保证能在有限时间内达成一致。\nCAP理论 CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer\u0026rsquo;s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点，只能满足三项中的两项：\n 一致性（Consistency） （等同于所有节点访问同一份最新的数据副本） 可用性（Availability）（每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据） 分区容错性（Partition tolerance）（以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择）  证明：\n假设有两个通信中的节点出现了网络分区的情况，如果允许其中一个节点更新状态，则需要舍弃一致性（C）；如果为了保证数据一致性，将分区的节点设置为不可用，就需要舍弃可用性（A）；如果两个节点可以互相通信，才能既保证一致性又保证可用性，会丧失分区容错性（P）。\n三类系统模型\n CA（一致性+可用性）：包括完全严格的仲裁协议，例如2PC（两阶段提交）。 CP（一致性+分区容错性）： 包括多数仲裁协议，其中少数分区不可用，例如Paxos。 AP（可用性+分区容错性）： 包括使用冲突解决方案的协议，例如DynamoDB。  CA\\CP区别：CA和CP系统设计均提供相同的一致性模型：高度一致性。 唯一的区别是CA系统不能容忍任何节点故障。 CP系统可以容忍 f在给定 2f+1在非拜占庭式故障模型中。\n场景\n CA：弱化了分区容错性，早期分布式关系数据库系统中使用的许多系统设计如两阶段提交，都没有考虑分区容错性。 分区容错性是现代系统的重要属性，因为如果系统在多个地理环境上分布，网络分区出现的概览就会加大。 CP：弱化了可用性，一些对结果一致性很敏感的应用会选择基于此模型设计，当系统出现故障时会拒绝服务；Paxos、Raft 等共识算法，以及HBase、MongoDB等基于此模型设计。 AP：弱化了一致性，一些对结果一致性不敏感的应用会选择基于此模型设计，可以允许在新版本上线后过一段时间才最终更新成功，期间不保证一致性；分布式同步协议如 Gossip，以及DynamoDB、 CouchDB、Cassandra 数据库等基于此模型设计。  ACID原则与BASE原则 ACID原则\nACID 即 Atomicity（原子性）、Consistency（一致性）、Isolation（隔离性）、Durability（持久性）四种特性的缩写，一般出现在分布式数据库等基于事务过程的系统中；ACID 原则描述了分布式数据库需要满足的一致性需求，同时允许付出可用性的代价。\n Atomicity：每次事务是原子的，事务包含的所有操作要么全部成功，要么全部不执行。一旦有操作失败，则需要回退状态到执行事务之前； Consistency：数据库的状态在事务执行前后的状态是一致的和完整的，无中间状态。即只能处于成功事务提交后的状态； Isolation：各种事务可以并发执行，但彼此之间互相不影响。按照标准 SQL 规范，从弱到强可以分为未授权读取、授权读取、可重复读取和串行化四种隔离等级； Durability：状态的改变是持久的，不会失效。一旦某个事务提交，则它造成的状态变更就是永久性的。  BASE原则\nBASE即 Basic Availability（基本可用），Soft-state（弱状态），Eventual Consistency（最终一致性），为 eBay 技术专家 Dan Pritchett 提出的与ACID相对的一个原则，主要面向大型高可用分布式系统，主张牺牲掉对强一致性的追求，而实现最终一致性，来换取一定的可用性。\n Basic Availability：系统在出现不可预知的故障时候，允许损失部分可用性，保证核心服务可用。 Soft-state：允许系统在不同节点的数据副本之间进行数据同步的过程中存在延时（允许系统中的数据存在中间状态，不会影响系统的整体可用性）。 Eventual Consistency：系统中所有的数据副本，在进过一段时间的同步后，最终能够达到一个一致的状态。  核心问题-复制 复制与分区  分区：将数据集划分为更小的不同的独立集合; 这是用来减少数据集增长的影响，提高性能 复制：在多台计算机上复制相同的数据; 允许更多的服务器参与计算，主要是为了通过为数据新副本提供额外的计算能力和带宽，从而提高性能；因为数据的独立副本必须在多台计算机上保持同步，这意味着确保复制遵循内存一致性模型  为什么核心问题是复制：分布式存储相关的系统都必须用某种冗余的方式在廉价硬件的基础上搭建高可靠的存储，而冗余的基础就是复制（多副本策略）, 一份数据存多份. 多副本保证了可靠性, 而副本之间的一致, 就需要各种分布式共识算法来保证.\n复制是一个组通信问题。需要考虑哪种通信方式可以为我们提供我们想要的性能和可用性特性？面对网络分区以及节点同时发生故障，我们如何确保容错性，持久性以及避免分歧。\n基本复制方式  同步复制：强持久化保证，系统响应慢，对网络延迟敏感 异步复制：弱持久化保证，性能高，对网络延迟更加宽容  基本复制算法 基本复制算法大致可以分为两类：Replication methods that prevent divergence (single copy systems) 防止差异的复制方式（单拷贝系统）与Replication methods that risk divergence (multi-master systems) 有差异风险的复制方式（多主系统）\nReplication methods that prevent divergence (single copy systems) 防止差异的复制方式（单拷贝系统），对外表现得像一个单独的系统；当部分故障发生时，系统确保只有一个系统副本处于活动状态；系统需要确保副本始终保持一致，基于某一种共识算法去实现，一般有如下几种方式：\nMaster/Slave（主从复制）\n所有更新都在主服务器上执行，操作日志(或者更改)通过网络传送到备份副本；涉及两种相关的变体异步主/备份、同步主/备份、半同步主/备复制。\n1. 同步复制 直到数据真的安全的复制到全部的机器上之后, master才告知客户端数据已经完成同步 问题：强一致性持久化保证，但是系统响应慢，对网络延迟的变化非常敏感；系统的可用性随着副本数量指数降低 2. 异步复制 问题：性能高，但是为弱一致性持久化保证，数据存在丢失风险，会造成数据不一致的情况。 3. 半同步复制 要求master在应答客户端之前必须把数据复制到足够多的机器上, 而非全部机器. 这样副本数够多可以提供比较高的可靠性; 1台机器宕机也不会让整个系统停止写入;但系统中还是会存在数据不一致的情况。 2-phase commit（两阶段提交）\n[ Coordinator ] -\u0026gt; OK to commit? [ Peers ] \u0026lt;- Yes / No [ Coordinator ] -\u0026gt; Commit / Rollback [ Peers ] \u0026lt;- ACK 阶段一：投票阶段，协调人向所有参与者发送更新信息。每个参与者处理更新，并投票决定是提交还是放弃。当投票决定提交时，参与者将更新存储到一个临时区域（write-ahead log）。 阶段二：协调程序决定结果并通知每个参与者。如果所有参与者投票提交，那么更新将从临时区域获得并永久化。 问题：强一致性持久化保证，但是系统响应慢，对网络延迟的变化非常敏感；系统的可用性随着副本数量指数降低 Quorum机制（多数派）\nQuorum 机制，是一种分布式系统中常用的，用来保证数据冗余和最终一致性的投票算法，其主要数学思想来源于鸽巢原理；在分布式系统中，Quorum常用于副本的读写控制。\n假设每份数据有V个副本，每个副本对应一票，读、写操作首先要请求副本以获取其票数，定义： read quorum R（最小读票数）：读操作获取的票数必须大于该值才允许读； write quorum W（最小写票数）：写操作获取的票数必须大于该值才允许写； V、R、W必须满足： R + W \u0026gt; V：保证对于每份数据，不会 同时读和写（当一个写操作请求过来的时候，它必须要获得W个写票。而剩下的 数量是V-W是不够R的，因此不能再有读请求过来了） W \u0026gt; V / 2：保证对于每份数据，不会同时出现 两个写，即写操作是串行的 其他： 没有规定 R \u0026gt; V / 2，quorum 机制允许 多个读同时发生，即允许 并发读； 考虑write -\u0026gt; read序列，因为R + W \u0026gt; V，因此 W 和 V 之间至少有一个重叠（鸽巢原理），从而保证 write 之后， read 操作至少会获取一个最新副本； 在做复制冗余的时候，借助 Quorum 机制，5 个副本只需要完成 3 个写即可响应成功，提升了写操作的响应速度，又没有减弱可靠性；Quorum 机制本质上是把写负载转移到了读负载的一种设计权衡。\n问题：\n1. 对于一条数据的更新时, 会产生不一致的状态 问题：如第一次update，nodeA、nodeB写入a=x；第二次update，nodeB、nodeC写入a=y；如果读取a的客户端联系到nodeA和nodeB，会得到不一致的数据 解决：对每次的写入增加全局时间戳，以后写入的优先 2. 并发环境下，因为无法保证顺序执行，所以无法保证系统的正确性 问题： 结论：Quorum机制无法保证强一致性，即无法实现任何时刻任何用户或节点都可以读到最近一次成功提交的副本数据；后续Paxos对Quorum机制进行了改进，通过2次多数派读写, 实现了严谨的强一致共识算法。\nReplication methods that risk divergence (multi-master systems) Gossip算法\nGossip算法Palo Alto研究中心在论文《Epidemic Algorithms for Replicated Database Maintenance》中提出的一种用于分布式数据库在多节点间复制同步数据的算法；特点是要同步的信息如同流言一般传播，最终一致性。\n具体的工作过程如下：\n1. 如果有某一项信息需要在整个网络中所有节点中传播，那从信息源开始，选择一个固定的传播周期（如1秒），随机选择它 相连接的k个节点（称为Fan-Out）进行消息传播。 2. 每一个节点收到消息后，如果这个消息是它之前没有收到过的，将在下一个周期内，选择除了发送消息给它的那个节点外的 其他相邻k个节点发送相同的消息，理论上最终网络的所有节点都会拥有相同的消息。 上图从一致性、延迟、吞吐量、数据丢失和故障转移对比了各个类型共识算法实现。\n参考 https://zh.wikipedia.org/wiki/拜占庭将军问题\nhttps://www.iminho.me/wiki/docs/blockchain_guide/distribute_system-intro.md\nhttp://book.mixu.net/distsys/single-page.html\nhttps://zh.wikipedia.org/wiki/Quorum_(分布式系统)\nhttps://en.wikipedia.org/wiki/Gossip_protocol\n"
            }
    
        ,
            {
                "id": 7,
                "href": "https://chinalhr.github.io/post/jenkins-kubernetes-helm-ci-cd/",
                "title": "Jenkins-Kubernetes-Docker-Helm CI/CD实践",
                "section": "post",
                "date" : "2021.01.12",
                "body": " 基于Kubernetes、Docker、Helm、Jenkins的CI/CD实践，对于前文Jenkins-pipeline-Docker 实现CI/CD 的补充扩展\n Helm Helm是Kubernetes生态系统中的一个包管理工具；Helm使用的包格式称为 charts，chart就是一个描述Kubernetes相关资源的文件集合。\nKubernetes应用部署面临的一些问题 kubernetes提供了基于容器的应用集群管理，容器编排，路由网关、水平扩展、监控、备份、灾难恢复等运维功能。用户通过使用kubernetes API对象来描述应用程序规格，如Pod，Service，Volume，Namespace，ReplicaSet，Deployment，Job\u0026hellip;；而对这些对象的操作都需要写入到yaml文件通过kubectl进行部署；这时就会遇到如下的几个问题：\n 如何对kubernetes应用配置文件进行管理 如何把一套的相关配置文件作为一个应用进行管理 如何分发和重用kubernetes的应用配置  这时候就可以引进Helm之类的包管理工具解决如上问题\nHelm提供了什么功能 对于开发者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库；Helm提供了kubernetes上的软件部署，删除，升级，回滚应用的功能。\nHelm相关组件  Helm: Kubernetes应用打包工具。 Tiller: Helm服务端，部署于Kubernetes集群中，用于接收处理Helm的相关命令。 Chart: Helm的打包格式，描述Kubernetes相关资源的文件集合。 Repoistory: Helm的软件仓库。  相关内容可以参考helm官方文档：https://helm.sh/zh/docs\nKubernetes Helm Jenkins CI/CD流程 准备  一个安装了Tiller的kubernetes集群 部署服务器，安装配置了Helm、Docker、Kubectl、Jenkins、Git 代码托管服务，如Gitlab、Github Docker Registry  基本流程  开发人员提交代码到Git仓库 运维人员触发Jenkins deploy任务，拉取代码进行Docker镜像打包，推送到远程仓库 Jenkins使用 Helm 更新 Release到Kubernetes集群中  实践 项目 本次实践的项目是一个简单的Spring Boot 后端服务\n仓库地址为：https://github.com/ChinaLHR/shovel-kubernetes-helm-devops\n项目目录如下：\n├── Jenkinsfile （Jenkins Pipeline Script） ├── Makefile （打包构建相关markfile） ├── README.md ├── chart （chart文件夹） │ ├── Chart.yaml │ ├── templates │ │ ├── NOTES.txt │ │ ├── _helpers.tpl │ │ ├── configmap.yaml │ │ ├── deployment.yaml │ │ ├── service.yaml │ │ ├── serviceaccount.yaml │ │ └── tests │ └── values.yaml ├── package.Dockerfile ├── pom.xml ├── release.Dockerfile ├── script （运行脚本） │ └── bin │ └── deploy.sh └── src\t（项目源码） ├── main │ ├── java │ └── resources └── test └── java Jenkins Pipeline  Jenkins流水线项目配置  新增一个流水线项目，配置Pipeline from SCM，配置对应的参数BRANCH  代码拉取  这里使用Jenkins Checkout插件进行Git代码拉取\nstage(\u0026#39;Clone target repo\u0026#39;) { checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: branch]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;deploy\u0026#39;, url: \u0026#39;https://gitee.com/***.git\u0026#39;]]]) }  代码编译打包-Docker镜像构建  这里使用Docker多阶段构建，第一阶段使用maven:3-jdk-8基础镜像对项目进行打包，第二阶段将第一阶段生成的jar与项目的script部署脚本添加到镜像中进行镜像打包。\n最后编写makefile，使用make命令进行构建打包镜像、项目打包操作、构建项目镜像、推送项目镜像到远程Docker仓库的操作。\n具体可以参考项目对应package.Dockerfile、release.Dockerfile、Makefile、deploy.sh文件；相关pipeline script如下：\nstage(\u0026#39;Build package\u0026#39;) { sh \u0026#34;make build-package-image package-image=${packageImage}\u0026#34; sh \u0026#34;make package package-image=${packageImage}\u0026#34; } stage(\u0026#39;Build release\u0026#39;) { sh \u0026#34;make build-release-image release-image=${releaseImage}\u0026#34; } stage(\u0026#39;Push release image to registry\u0026#39;) { withDockerRegistry(credentialsId: \u0026#39;docker-user\u0026#39;, url: \u0026#39;https://registry.cn-hangzhou.aliyuncs.com\u0026#39;) { sh \u0026#34;docker push ${releaseImage}\u0026#34; } }  Helm Chart更新 Release到Kubernetes集群中  初始化Helm Chart：\nhelm create shovel mv shovel chart # 删除非必要文件 cd chart-copy/templates \u0026amp;\u0026amp; rm hpa.yaml \u0026amp;\u0026amp; rm ingress.yaml 修改Chart配置文件：\n# values.yaml文件 # 修改镜像image相关配置，去除ingress相关配置，增加自定义配置如下： env: app: port: 8088 name: shovel-server-app # templates/deployment.yaml # 修改containers下的configMap、ports、livenessProbe标签配置 containers: - name: {{ .Chart.Name }} securityContext: {{- toYaml .Values.securityContext | nindent 12 }} image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\u0026#34; imagePullPolicy: {{ .Values.image.pullPolicy }} envFrom: - configMapRef: name: {{ include \u0026#34;shovel.fullname\u0026#34; . }} ports: - name: http containerPort: 8088 protocol: TCP livenessProbe: httpGet: path: /ping port: http readinessProbe: httpGet: path: /ping port: http resources: {{- toYaml .Values.resources | nindent 12 }} # 添加templates/configmap.yaml设置应用的环境变量 apiVersion: v1 kind: ConfigMap metadata: name: {{ include \u0026#34;shovel.fullname\u0026#34; . }} labels: {{- include \u0026#34;shovel.labels\u0026#34; . | nindent 4 }} data: SERVER_PORT: \u0026#34;{{ .Values.env.app.port}}\u0026#34; APPLICATION_NAME: {{ .Values.env.app.name }}  使用Helm发布/更新服务  切换当前context到dev（这里的env可以通过jenkins parameters 进行传递），使用Helm发布/更新服务\nstage(\u0026#39;Deploy\u0026#39;) { sh \u0026#34;\u0026#34;\u0026#34; kubectl config use-context dev helm -n ${namespace} upgrade ${project} chart \\ -f chart/values.yaml \\ --set-string image.tag=${imageTag} \\ --wait --install \u0026#34;\u0026#34;\u0026#34; } "
            }
    
        ,
            {
                "id": 8,
                "href": "https://chinalhr.github.io/post/kubernetes-telepresence/",
                "title": "Kubernetes本地调试利器-Telepresence",
                "section": "post",
                "date" : "2020.12.30",
                "body": " Kubernetes环境下基于Telepresence开发调试实践\n 以Kubernetes为基础的微服务下开发本地调试面临的问题 微服务是随着领域驱动设计，持续交付，容器化技术，基础设施自动化，小型自治团队，大型集群系统的实践与流行，而总结出来的一种逐渐流行的模式；基于Kubernetes作为微服务应用载体的架构，kubernetes提供了服务注册、配置管理、负载均衡、故障隔离、业务恢复、自动扩容等能力。相比于传统的Spring Cloud微服务体系，将更多的基础服务下层到基础设施上，让应用层更关注业务开发，同时对多平台语言有了更好的支持。\n但是在以Kubernetes为基础的微服务下进行开发、调试；就会面临下面几个问题：\n 由于服务依赖关系复杂，服务众多，本地进行调试的话需要把关联的服务都搭建-启动起来 微服务依赖的基础设施也需要在本地环境进行搭建-启动，如消息队列、数据库、配置中心、调度中心\u0026hellip;  一些解决方案\n 通过Docker Compose在本地运行整个多服务应用程序；这种方式并未在Kubernetes内部运行服务，一些基础设施也不容易在本地使用。 通过类似Telepresence的工具，通过将Kubernetes环境中的数据（连接、环境变量、volumes）代理到本地进程以方便本地进行调试。  Telepresence简介 Telepresence是CNCF 基金会下的一个项目，使用Telepresence可让您在本地运行单个服务，同时将该服务连接到远程Kubernetes群集。 这使在微服务下开发人员可以：\n 本地单个服务依赖于群集中的其他服务，也可以进行本地开发与调试 可以在本地使用IDE调试服务 使本地开发机器像Kubernetes集群一样运行  实现原理\nTelepresence在Kubernetes集群中运行的Pod中部署了双向网络代理，将Kubernetes环境中的数据（连接、环境变量、volumes）代理到本地进程。本地进程的网络透明覆盖，因此DNS调用和TCP连接通过代理路由到远程Kubernetes集群。实现了：\n 本地服务具有对远程群集中其他服务的完全访问权限 本地服务可以访问Kubernetes的环境变量，secrets，ConfigMap 远程服务可以访问到本地服务  实现方式如下：\n 默认使用 --method vpn-tcp，默认情况下，使用程序创建类似VPN的隧道程序sshuttle，sshuttle通过SSH连接建立数据包的隧道，并将DNS查询转发到群集中的DNS代理。 使用 --method inject-tcp，使用在Linux / OSX上 机制来实现 LD_PRELOAD/ DYLD_INSERT_LIBRARIES ，其中可以将共享库注入到进程中并覆盖库调用；会覆盖DNS解析和TCP连接，并通过SOCKS代理将它们路由到群集。  安装\n mac上安装Telepresence  brew install kubectl # 配置相关的kube config... brew cask install osxfuse brew install datawire/blackbird/telepresence Telepresence使用 连接到远程Kubernetes集群 场景\n在context为dev的kubernetes集群内，服务A依赖于服务B，需要在本地对服务A进行调试。\n操作\n启动Telepresence，连接对应集群\ntelepresence --context dev 对应启动日志\nT: Starting proxy with method 'vpn-tcp', which has the following limitations: All processes are affected, only one telepresence can run per machine, and you can't use other VPNs. You may need to add cloud hosts and headless services with --also-proxy. For a full list of T: method limitations see https://telepresence.io/reference/methods.html T: Volumes are rooted at $TELEPRESENCE_ROOT. See https://telepresence.io/howto/volumes.html for details. T: Starting network proxy to cluster using new Pod telepresence-1611716361-290886-47290 T: No traffic is being forwarded from the remote Deployment to your local machine. You can use the --expose option to specify which ports you want to forward. T: Connected. Flushing DNS cache. T: Setup complete. Launching your command. 当运行 Telepresence 命令的时候，默认使用vpn-tcp模式；它创建了一个Deployment，对应的 Spec 是负责转发流量的代理容器；对应的pod为上面启动日志的telepresence-1611716361-290886-47290；同时Telepresence在本地创建了一个全局VPN；使得本地的所有程序都可以访问到集群中的服务。\n这时候就可以把本地环境的Service A启动起来进行调试，所有的环境变量可以基于上文的dev环境配置。\n远程Kubernetes集群内部服务与本地服务联调 场景\n在context为dev的kubernetes集群内，服务A依赖于服务B，需要在本地对服务B进行调试；这里调试的前提是我们要有正式的来着服务A的调用流量。\n操作\n使用Telepresence提供的参数--swap-deployment DEPLOYMENT_NAME[:CONTAINER], -s DEPLOYMENT_NAME[:CONTAINER] 将集群中的一个 Deployment 替换为本地的服务；使用 --expose PORT[:REMOTE_PORT]进行本地-集群Deployment绑定的端口转发。启动本地进程板顶对应expose的PORT进行调试。\n# 这里替换了 context为dev、namespace为test的 service-a 到本地服务；并对集群内service-b：8080的流量转发到本地10080端口 telepresence --context dev --namespace test --swap-deployment service-b --expose 10080:8088 这里Telepresence操作的过程如下：\n 在集群中创建一个代理Deployment复制service-b的所有Label 建立一个路由通道，将代理容器的所有流量转发到本地 10080 端口 将service-b的 replicas 设置为0，kubernetes Service 的 selector 就只能匹配到刚刚创建的代理容器上  一些参数  帮助文档  telepresence --help  新建一个deployment用于测试本地服务访问远程服务  # --new-deployment:创建一个名为 server-c 的 deployment telepresence --new-deployment server-c --run-shell --also-proxy 参考 https://www.telepresence.io/\n"
            }
    
        ,
            {
                "id": 9,
                "href": "https://chinalhr.github.io/post/frp-intranet-penetration/",
                "title": "基于FRP的内网穿透实践",
                "section": "post",
                "date" : "2020.11.10",
                "body": " FRP内网穿透实践\n 什么是内网穿透（NAT穿透） 内网穿透（NAT穿透）指计算机是局域网内时，外网与内网的计算机节点需要连接通信，有时就会出现不支持内网穿透，两台计算机要进行通信数据就需要经过互联网然后到达目标计算机。\n简单的说内网穿透就是实现不同局域网内的主机之间通过互联网进行通信的技术。\nNAT地址转换 NAT是一种在IP数据包通过路由器或防火墙时重写来源IP地址或目的IP地址的技术。这种技术被普遍使用在有多台主机但只通过一个公有IP地址访问互联网的私有网络中。NAT通常部署在一个组织的网络出口位置，通过将内部网络IP地址替换为出口的IP地址提供公网可达性和上层协议的连接能力。\n一般来说一个本地网络使用一个专有网络的指定子网（比如192.168.x.x或10.x.x.x）和连在这个网络上的一个路由器。这个路由器占有这个网络地址空间的一个专有地址（比如192.168.0.1），同时它还通过一个或多个因特网服务提供商提供的公有的IP地址（过载NAT）连接到因特网上。当信息由本地网络向因特网传递时，源地址从专有地址转换为公用地址。由路由器跟踪每个连接上的基本数据，主要是目的地址和端口。当有回复返回路由器时，它通过输出阶段记录的连接跟踪数据来决定该转发给内部网的哪个主机；\n基本NAT和端口号转换\n 基本网络地址转换（Basic NAT）：也称静态NAT，仅支持地址转换，不支持端口映射。维护一个公网的地址池，将内部网络的私有IP地址转换为公有IP地址，IP地址对是一一对应的。   网络地址端口转换（NAPT）：支持端口的映射，采用端口多路复用的方式，并允许多台主机共享一个公网IP地址。分两种类型，源地址转换与目的地址转换。源地址转换发起连接的计算机的IP地址将会被重写，使得内网主机发出的数据包能够到达外网主机。目的地址转换被连接计算机的IP地址将被重写，使得外网主机发出的数据包能够到达内网主机。一般会一起使用以支持双向通信。NAPT维护一个带有IP以及端口号的NAT表。  DDNS DDNS（动态域名服务）是将用户的动态IP地址映射到一个固定的域名解析服务上，用户每次连接网络的时候客户端程序就会通过信息传递把该主机的动态IP地址传送给位于服务商主机上的服务器程序，服务器程序负责提供DNS服务并实现动态域名解析。\nDDNS捕获用户每次变化的IP地址，然后将其与域名相对应，这样其他上网用户就可以通过域名来进行交流。\n内网穿透的障碍  位于局域网内的主机有两个IP 地址，一个是内部网络私有 IP，一个是经过NAT转换后的外网 IP 地址，用于与外网程序进行通信。 局域网内除非主机主动向外部发送连接请求，当路由设备接收到外部数据包时，如果查询不到对应的记录，这些数据包将会被丢弃。  内网穿透的常见方案  DDNS解析：花生壳 反向代理：frp（开源）、ngrok（1.x开源，基本不可用，2.x闭源）  FRP内网穿透原理 frp分为服务端frps与客户端frpc，frps运行在有公网 IP 的服务器上，frpc运行在局域网内的设备上，服务端默认会先开放 7000 端口，然后与客户端相连，进行网络桥接。\n同时frpc可以开启某个端口与服务端的某个端口做映射，如22（ssh）、5901（vnc）、3389（rdp）；这样我们在终端访问frps部署的服务端的端口时，会自动转发到安装了frpc客户端去。\nFRP实践 服务器部署frps  下载frp程序  wget https://github.com/fatedier/frp/releases/download/v0.34.3/frp_0.34.3_linux_amd64.tar.gz tar -zxvf frp_0.34.3_linux_amd64.tar.gz 程序相关文件  frpc # frp client frpc_full.ini # 完整frp client 配置文件 frpc.ini # frp client 配置文件 frps # frp server frps_full.ini # 完整frp server 配置文件 frps.ini # frp server 配置文件 配置frps.ini  基本配置如下所示，具体详细配置参考官方文档\nbind_port = 80077 # frps监听端口 kcp_bind_port = 80077\t# frps kcp监听端口 dashboard_port = 80075\t# dashboard 监听端口 token = ** # 鉴权使用的 token 值，和frpc配置一致 dashboard_user = admin\t# dashboard用户名 dashboard_pwd = ***\t# dashboard密码 启动frps  这里使用简单的shell进行启动操作\n#! /bin/bash  FRP_DIR=/home/frp d_echo(){ echo [auto-robot] `date +%F\\ %T` $1 } frp_start(){ d_echo \u0026#34;start up frp server\u0026#34; nohup ${FRP_DIR}/frps -c ${FRP_DIR}/frps.ini \u0026amp; d_echo \u0026#34;start up frp server done\u0026#34; } frp_close(){ d_echo \u0026#34;close frp server\u0026#34; FRP_PID=`ps -ef|grep \u0026#34;${FRP_DIR}/frps\u0026#34;|grep -v grep|awk \u0026#39;{print $2}\u0026#39;` if [[ ${FRP_PID} ]]; then d_echo \u0026#34;try kill ${FRP_PID}\u0026#34; kill -9 \u0026#34;${FRP_PID}\u0026#34; d_echo \u0026#34;close frp server successful\u0026#34; else d_echo \u0026#34;close frp server failure\u0026#34; fi } frp_restart(){ frp_close sleep 5 frp_start } case $1 in start) frp_start ;; close) frp_close ;; restart) frp_restart ;; esac 内网设备安装frpc  下载frp程序 内网设备配置frpc.ini  [common] server_addr = 服务器公网地址 server_port = 80077 token = ** # 配置rdp端口映射，本地3389映射到服务器的3389 [rdp] type = tcp local_ip = 127.0.0.1 local_port = 3389 remote_port = 3389 启动frpc程序  frpc -c frpc.ini 参考 https://gofrp.org/docs/\n"
            }
    
        ,
            {
                "id": 10,
                "href": "https://chinalhr.github.io/post/permission-system-design/",
                "title": "权限系统设计-概念与思路",
                "section": "post",
                "date" : "2020.09.16",
                "body": " 权限系统设计相关概念与思路\n 前言： 权限系统的设计几乎是每个系统都必需的模块，目的是对不同的人访问资源进行权限的控制，避免因权限控制缺失或操作不当引发的风险问题，如操作错误，隐私数据泄露等问题。从万物皆文件的Linux文件权限到各种CRM、CMS、OA管理系统，都可以看到权限系统的身影。\n权限管控分类 如果从控制力的角度来进行划分的话，权限管控可以分为功能级权限管控（功能权限）和数据级权限管控（数据权限）。\n 功能权限  指用户登录系统后能看到哪些模块，操作哪些按钮。常见的CRM系统，不同业务的人员，登录系统后，看到的功能模块也不尽相同；而同属于一个业务线，因为职位等级不同，拥有的操作功能也可能不同（例如组长、负责人才拥有删除权限）。\n 数据权限  指用户在某个模块里能看到哪些范围的数据，如A业务线的销售人员只能看到自己的客户数据，但是A的业务线的销售总监可以查看整个区域销售人员的客户数据。\n认证与授权 权限系统进行权限控制，主要包含了两部分，认证与授权。\n认证（Authentication） 身份验证（Authentication）又称“认证”、“鉴权”，是指通过一定的手段，完成对用户身份的确认。身份验证的目的是确认当前所声称为某种身份的用户，确实是所声称的用户。一般使用的技术手段有：HTTP Basic Authentication、HMAC（AK/SK）、OAuth2、JWT等，可以参考文章：https://chinalhr.github.io/post/api-auth-program。\n授权（Authorization） 授权（Authorization）一般是指对信息安全或计算机安全相关的资源定义与授予访问权限，尤指访问控制。在权限系统中体现为授予认证用户的功能权限与数据权限。\n功能权限模型 主流的功能权限模型包括ACL（Access Control List）访问控制列表、RBAC（Role-based access control）基于角色的访问控制模型、ABAC(Attribute-based access control)基于属性的访问控制。\nACL 访问控制表（Access Control List，ACL），又称存取控制串列，是使用以访问控制矩阵为基础的访问控制表，每一个（文件系统内的）对象对应一个串列主体。访问控制表由访问控制条目（access control entries，ACE）组成。访问控制表描述用户或系统进程对每个对象的访问控制权限。\nACL是一种面向资源的访问控制模型，每一项资源，都配有一个列表，这个列表记录的就是哪些用户可以对这项资源执行CRUD中的那些操作。当系统试图访问这项资源时，会首先检查这个列表中是否有关于当前用户的访问权限，从而确定当前用户可否执行相应的操作。\nACL核心在于用户可以直接和权限挂钩，设计简单但缺点也是很明显的，由于需要维护大量的访问权限列表，所以在性能上有明显的不足，因而便有了对ACL设计的改进如RBAC、ABAC。\n访问控制矩阵如下所示：\n    资产 1 资产 2 文件 设备     角色 1 read, write, execute, own execute read write   角色 2 read read, write, execute, own      RBAC RBAC 以角色为基础的访问控制,认为权限授权的过程可以抽象地概括为：Who是否可以对What进行How的访问操作，并对这个逻辑表达式进行判断是否为True的求解过程，也即是将权限问题转换为What、How的问题，Who、What、How构成了访问权限三元组。\n包含了三个基础组成部分，分别是：用户、角色和权限。\nRBAC通过定义角色的权限，并对用户授予某个角色从而来控制用户的权限，实现了用户和权限的逻辑分离（区别于ACL模型），方便权限的管理。\nRBAC0-RBAC核心思想模型 RBAC0是RBAC模型的核心，主要包含以下\n User：用户，每个用户都有唯一的UID识别，并被授予对应的角色 Role：角色，不同角色具有不同的权限 Permission：权限，包含所有访问功能的权限 Session：会话，存储特定用户会话的基本权限信息  RBAC1-基于角色的分层模型 RBAC1是对RBAC进行了扩展，对RBAC的角色进行了分层处理，引入了角色的继承概念，有了继承的关系就有了上下级的包含关系。\nRBAC2-RBAC约束模型。 RBAC2是对RBAC进行了扩展，主要引入了SSD（静态职责分离）和DSD（动态职责分离）。\nSSD主要应用在用户和角色之间（授权阶段），主要约束\n 互斥角色，同一个用户不能授于互斥关系的角色 基数约束，一个用户拥有的角色是有限的，一个角色拥有的许可是有限的 先决条件约束，用户想得到高级权限，必须先拥有低级权限  DSD是会话和角色之间的约束，主要动态决定怎么样计划角色，如：一个用户拥有5个角色，只能激活2个。\nRBAC3 RBAC3就是整合了RBAC1+RBAC2模式，这里不再阐述。\nABAC 基于属性的访问控制（ABAC），也称为基于策略的访问控制(IAM)，定义了访问控制范式，其中通过使用将属性组合在一起的策略，将访问权限授予用户。这些策略可以使用任何类型的属性（用户属性，资源属性，对象，环境属性等）。该模型支持布尔逻辑，其中规则包含有关谁在发出请求，资源和操作的“ IF，THEN”语句。例如：如果请求者是管理者，则允许对敏感数据的读/写访问。\n对比RABC模型，ABAC的主要区别在于表示复杂布尔规则集的策略的概念可以评估许多不同的属性，更加灵活可控。\n基于属性的访问控制维度\n 外部授权管理 动态授权管理 基于策略的访问控制 细粒度的授权  ABAC组件\nAttribute：属性，用于表示 subject、object 或者 environment conditions 的特点，attribute 使用 key-value 的形式来存储这些信息。\nSubject：指代使用系统的人或者其他使用者（non-person entity，NPE），比如说客户端程序，访问 API 的 client 或者移动设备等等。一个 subject 可以有多个的 attributes，例如用户属性等。\nObject：指代 ACM 需要管理的资源，如文件，某项记录，某台机器或者某个网站， object 也可以有多项属性，比如x组的测试、生产实例。\nOperation：subject需要做的操作，比如查看某条记录，使用某个功能，登录某台服务器。往往包括我们常说的读、写、修改、拷贝等等，一般 operation 是会表达在 request 中的，比如 HTTP method。\nPolicy：通过 subject、object 的 attribute 与 environment conditions 一起来判断 subject 的请求是否能够允许的关系表示，一般是一系列的 boolean 逻辑判断的组合。\nEnvironment Conditions：表示目前进行的访问请求发生时的操作或情境的上下文。Environment conditions 常常用来描述环境特征，是独立于 subject 与 object 的，常用来描述系统的情况：比如时间，当前的安全等级，生产环境还是测试环境等等。\n一个典型的 ABAC 场景描述如下图，当 subject 需要去读取某一条记录时，我们的访问控制机制在请求发起后遍开始运作，该机制需要计算，来自 policy 中记录的规则，subject 的 attribute，object 的 attribute 以及 environment conditions，而最后会产生一个是否允许读取的结果，也就是上图的aloow or deny。\n阿里云的RAM访问控制运用的就是ABAC模型：\n{ \u0026#34;Version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;oss:List*\u0026#34;, \u0026#34;oss:Get*\u0026#34;], \u0026#34;Resource\u0026#34;: [\u0026#34;acs:oss:*:*:samplebucket\u0026#34;, \u0026#34;acs:oss:*:*:samplebucket/*\u0026#34;], \u0026#34;Condition\u0026#34;: { \u0026#34;IpAddress\u0026#34;: { \u0026#34;acs:SourceIp\u0026#34;: \u0026#34;42.160.1.0\u0026#34; } } }] } 组织架构与功能权限 常见的组织架构如下所示是一个树形的结构，最顶部节点为根部门，根部门对应多个子部门，子部门也可以对应多个子部门。\n每个组织部门下都会有多个岗位,比如研发部有后端开发、前端开发、技术经理、技术总监\u0026hellip;职位,虽然都在同一部门,但是每个职位的权限是不同的,职位高的拥有更多的权限。\n组织架构结合功能权限：可以把岗位与角色进行关联，用户加入某个岗位后，就会自动获得该岗位的全部角色,减少授权操作，同时用户在调岗时，角色即可批量调整。\n基于组织架构模型与RBAC0的模型图如下所示：\n数据权限 数据权限解决的是用户能看到多少数据量和什么数据的问题，例如A和B两个用户都能看到订单模块，但A能看到320条数据，B只能看到100条数据，且A能看到的320条数据中包含着B能看到的100条数据，这些都是由数据权限决定的。\n数据权限一般和组织架构相关，常见的数据权限类型有如下几种：\n  只能查看本人数据\n  只能查看本部门数据\n  只能查看本部门及其子部门数据\n  只能查看自定义部门数据\n  只能查看自定义部门及其子部门数据\n  可以查看所有部门数据\n  组织架构一般为树状的架构，而常见的数据权限一般和部门产生过滤关系，且不同部门的不同岗位有着不同的数据权限，如销售部门的销售组长岗位可以看到本部门的所有数据，而销售部门的销售员岗位只能看到自己本人的数据；因此可以基于上文对组织架构与功能权限的模型，添加岗位和数据权限的关联关系，如下图所示：\n因为数据权限涉及到每张表里面的每一行数据，如何设计数据权限的查询模型，合理地利用数据库索引的特性，提高查询的效率显得至关重要；\n按照正常的思路，数据入库的时候我们会存储对应的用户ID（creator_id）与对应部门的ID（department_id）。数据权限的获取就包括了获取用户绑定的数据权限类型（枚举值）、用户可以查看哪些部门的数据（遍历部门、子部门）或者本人的数据（user_id）;权限数据的筛选我们可以通过筛选部门的ID与用户ID进行实现。\n主要优化点有两点：1. 数据权限的获取（权限部门数据获取涉及多次IO操作） 2. 权限数据的筛选（避免通过in查询筛选数据导致大数据量下存在性能问题，合理利用索引特性）；这里作者提供一种数据权限查询优化的设计思路：\n为每个部门指定一个data_key，并通过一定规则进行data_key的配置，使其与自己的上级部门、下级部门产生关联，如上图所示：根部门的data_key为0，直属部门的data_key为0 001、0 002\u0026hellip;以此类推，根据此模型，结合mysql的前置查询索引的特性，可以通过获取部门的data_key即可查询本部门数据或者本部门及其子部门数据，并且可以合理的利用索引的特性，提高性能。\n示例：\n-- 1. 用户在所在部门data_key 0001001，数据权限：本部门 select * from bill where data_key = \u0026#39;0001001\u0026#39;; -- 2. 用户在所在部门data_key 0001，数据权限：本部门及其子部门 select * from bill where data_key like \u0026#39;0001%\u0026#39;; -- 3. 用户id 372，数据权限：本人 select * from bill where creator_id = 372; 功能权限与数据权限系统设计实现思路 依据上述对功能权限，数据权限模型的描述，我们可以简单设计一个拥有组织架构、基于RBAC-0的功能权限，基于组织架构的数据权限系统。系统使用的技术栈为 Kotlin、Spring Boot\u0026hellip;\n表结构设计 功能权限系统设计  认证  这里我们可以基于JWT机制实现认证，用户登录后颁发JWT Token充当用户的登录凭证，JWT Token Payload存储了用户的ID;当JWT Token过期时让用户重新登录。这里可以基于Refresh Token、Redis实现Token的刷新机制，具体细节不在本文进行讨论。\n 授权  以一个Spring Boot项目为例子，我们可以通过实现HandlerInterceptor拦截所有的api接口，在preHandle的地方解析token获取用户的id，再查询出用户所拥有的的所有接口权限、数据权限、origanization_key\u0026hellip;，授权于本次请求（session）通过ThreadLocal存放于请求的上下文中。\n/** * 授权上下文对象 * */ class GrantPermissionContext( var interfacePermissionKey: List\u0026lt;String\u0026gt;, var dataPermissionType: DataPermissionType, var adminUserId: Long, var organizationKey: String, var enabledStatus: EnabledStatus ) { companion object { private val GRANT_PERMISSION_CONTEXT = ThreadLocal\u0026lt;GrantPermissionContextV2\u0026gt;() fun get(): GrantPermissionContext? { return GRANT_PERMISSION_CONTEXT.get() } fun set(interfacePermissionKey: List\u0026lt;String\u0026gt;, dataPermissionType: DataPermissionType, adminUserId: Long, enabledStatus: EnabledStatus, organizationKey: String ) { val grantPermissionContext = GrantPermissionContextV2( interfacePermissionKey = interfacePermissionKey, dataPermissionType = dataPermissionType, adminUserId = adminUserId, enabledStatus = enabledStatus, organizationKey = organizationKey ) GRANT_PERMISSION_CONTEXT.set(grantPermissionContext) } fun clean() { GRANT_PERMISSION_CONTEXT.remove() } } } /** * 授权拦截器 * */ @Component class GrantPermissionInterceptor : HandlerInterceptor { override fun preHandle(request: HttpServletRequest, response: HttpServletResponse, handler: Any): Boolean { //...解析JWT Token，获取授权信息设置到GrantPermissionContext中  GrantPermissionContext.set( interfacePermissionKey = interfacePermissionKey, dataPermissionType = dataPermissionType, adminUserId = userId, enabledStatus = enabledStatus， organizationKey = organizationKey ) return true } override fun afterCompletion(request: HttpServletRequest, response: HttpServletResponse, handler: Any, ex: Exception?) { //清除GrantPermissionContext  GrantPermissionContext.clean() } }  鉴权  可以通过注解 + Aspect切面，对设置了对应注解的接口进行拦截鉴权\n@Target(AnnotationTarget.FUNCTION) @Retention(AnnotationRetention.RUNTIME) annotation class RequiresPermissions(val permissionKey: String) @Aspect @Component @Order(value = 1) class InterfacePermissionAspect { @Around(\u0026#34;@annotation(requiresPermissions)\u0026#34;) fun doInterfacePermissionBefore(joinPoint: ProceedingJoinPoint, requiresPermissions: RequiresPermissions): Any? { val grantPermissionContext = GrantPermissionContext.get() val interfacePermissionKey = grantPermissionContext?.interfacePermissionKey ?: throw NotInterfacePermissionException() val permissionKey = requiresPermissions.permissionKey if (!interfacePermissionKey.contains(permissionKey) || grantPermissionContext.enabledStatus == EnabledStatus.DISABLE) { throw NotInterfacePermissionException() } return joinPoint.proceed() } } 数据权限设计  数据筛选  可以利用MyBatis的Interceptor机制、或者 Spring Data JPA 的Filter机制，在SQL执行之前通过GrantPermissionContext获取对应的数据权限类型与organizationKey，对SQL进行统一的判断筛选，最后再进行sql的执行。\n"
            }
    
        ,
            {
                "id": 11,
                "href": "https://chinalhr.github.io/post/kubeadm-install-kubernetes/",
                "title": "使用kubeadm部署Kubernetes集群实践",
                "section": "post",
                "date" : "2020.06.08",
                "body": " 使用kubeadm部署Kubernetes 1.18.0 集群实践记录\n kubeadm Kubeadm是一种工具，旨在为创建Kubernetes集群提供最佳实践的“快速路径”，它以用户友好的方式执行必要的操作，以使可以最低限度的可行，安全的启动并运行群集。只需将kubeadm,kubelet，kubectl安装到服务器，其他核心组件以容器化方式快速部署。\nkubeadm地址：https://github.com/kubernetes/kubeadm\n参考文档地址：https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/\n 常见的cmdlet  kubeadm init 启动一个 Kubernetes 主节点 kubeadm join 启动一个 Kubernetes 工作节点并且将其加入到集群 kubeadm upgrade 更新一个 Kubernetes 集群到新版本 kubeadm config 如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 kubeadm upgrade 命令 kubeadm token 使用 kubeadm join 来管理令牌 kubeadm reset 还原之前使用 kubeadm init 或者 kubeadm join 对节点产生的改变 kubeadm version 打印出 kubeadm 版本 kubeadm alpha 预览一组可用的新功能以便从社区搜集反馈  成熟度     Area Maturity Level     Command line UX GA   Implementation GA   Config file API beta   CoreDNS GA   kubeadm alpha subcommands alpha   High availability alpha   DynamicKubeletConfig alpha   Self-hosting alpha     Master节点     Protocol Direction Port Range Purpose Used By     TCP Inbound 6443* Kubernetes API server All   TCP Inbound 2379-2380 etcd server client API kube-apiserver, etcd   TCP Inbound 10250 Kubelet API Self, Control plane   TCP Inbound 10251 kube-scheduler Self   TCP Inbound 10252 kube-controller-manager Self     node节点     Protocol Direction Port Range Purpose Used By     TCP Inbound 10250 Kubelet API Self, Control plane   TCP Inbound 30000-32767 NodePort Services† All    前置准备 系统准备  开放端口：  开放Kubernetes各个组件所需要的端口，可以参考上文所展示的端口范围进行设置\n# 查看已开放的端口(默认不开放任何端口) firewall-cmd --list-ports # 开启80端口 firewall-cmd --zone=public(作用域) --add-port=10250/tcp(端口和访问类型) --permanent(永久生效) firewall-cmd --zone=public --add-port=10250/tcp --permanent # 重启防火墙 firewall-cmd --reload # 停止防火墙 systemctl stop firewalld.service # 禁止防火墙开机启动 systemctl disable firewalld.service  禁用SELINUX：  setenforce 0 vi /etc/selinux/config # 设置SELINUX=disabled  bridge设置  touch /etc/sysctl.d/k8s.conf # 添加如下内容 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 # 执行命令 modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf  关闭系统swap  # 关闭swap分区 swapoff -a # 修改配置文件 - /etc/fstab 注释掉如下行 /mnt/swap swap swap defaults 0 0 # 调整 swappiness 参数 vim /etc/sysctl.conf # 永久生效 # 修改 vm.swappiness 的修改为 0 kube-proxy开启ipvs设置 为kube-proxy开启ipvs的前提需要加载以下的内核模块：\nip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack_ipv4 touch /etc/sysconfig/modules/ipvs.modules # 添加如下脚本，保证在节点重启后能自动加载所需模块 #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 # 启用并查看是否已经正确加载所需的内核模块 chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 安装ipset软件包与ipvsadm管理工具\nyum -y install ipset yum -y install ipvsadm 安装并设置docker Kubernetes从1.6开始使用CRI(Container Runtime Interface)容器运行时接口。默认的容器运行时仍然是Docker，使用的是kubelet中内置dockershim CRI实现。\n 安装docker  # 更新yum包 sudo yum update # 安装需要的软件包 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 # 设置yum源 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # 可以查看所有仓库中所有docker版本，并选择特定版本安装 yum list docker-ce --showduplicates | sort -r # 安装docker sudo yum install docker-ce sudo yum install \u0026lt;FQPN docker-ce.x86_64 3:19.03.5-3.el7\u0026gt; # 启动并加入开机启动 sudo systemctl start docker sudo systemctl enable docker # 镜像加速 vim /etc/docker/daemon.json ## 加入镜像地址 { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://registry.aliyuncs.com\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34; ] } ## 重启服务 sudo systemctl daemon-reload sudo systemctl restart docker  修改docker cgroup driver为systemd  对于使用systemd作为init system的Linux的发行版，使用systemd作为docker的cgroup driver可以确保服务器节点在资源紧张的情况更加稳定\n# 创建或修改/etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } # 重启docker sudo systemctl daemon-reload sudo systemctl restart docker 安装kubeadm,kubectl和kubelet  Centos安装  # 添加 kubernetes.repo vim /etc/yum.repos.d/kubernetes.repo # 写入如下信息后保存 [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg # 使用yum makecache 生成缓存 yum makecache fast # 查看kubeadm版本 yum list kubelet kubeadm kubectl --showduplicates|sort -r # 安装kubeadm yum install -y kubelet kubeadm kubectl # 安装指定版本kubeadm yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 kubeadm：用于初始化 Kubernetes 集群 kubectl：Kubernetes 的命令行工具，主要作用是部署和管理应用，查看各种资源，创建，删除和更新组件 kubelet：主要负责启动 Pod 和容器 kubeadm部署kubernetes集群 kubeadm init 配置kubernetes master节点  设置开机启动kubelet服务  systemctl enable kubelet.service  导出配置文件并修改  # 导出配置文件 kubeadm config print init-defaults --kubeconfig ClusterConfiguration \u0026gt; /data/kubeadm/config/kubeadm.yml # 修改配置文件 vim kubeadm.yml # 修改内容如下 apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: # 修改为主节点 IP advertiseAddress: 192.168.1.102 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: localhost.localdomain taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: extraArgs: horizontal-pod-autoscaler-use-rest-clients: \u0026#34;true\u0026#34; horizontal-pod-autoscaler-sync-period: \u0026#34;10s\u0026#34; node-monitor-grace-period: \u0026#34;10s\u0026#34; dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd # 修改registry为阿里云 imageRepository: registry.aliyuncs.com/google_containers kind: ClusterConfiguration # 修改Kubernetes版本号 kubernetesVersion: v1.18.0 networking: dnsDomain: cluster.local # 配置Calico 的默认网段 podSubnet: \u0026#34;172.16.0.0/16\u0026#34; serviceSubnet: 10.96.0.0/12 scheduler: {} --- # 开启 IPVS 模式 apiVersion: kubeadm.k8s.io/v1beta2 kind: KubeProxyConfiguration featureGates: supportipvsproxymodedm.ymlvim kubeadm.yml: true mode: ipvs  查看并拉取镜像  # 查看镜像 kubeadm config images list --config /data/kubeadm/config/kubeadm.yml # 拉取镜像 kubeadm config images pull --config /data/kubeadm/config/kubeadm.yml  配置kubernetes master节点  kubeadm init --config=/data/kubeadm/config/kubeadm.yml --upload-certs | tee /data/kubeadm/log/kubeadm-init.log --upload-certs 参数：可以在后续执行加入节点时自动分发证书文件\ntee kubeadm-init.log参数： 用以输出日志\n Kubeadm init 执行过程  执行init操作的时候可以看到日志如下：\nW0601 11:33:16.858211 1719 strict.go:47] unknown configuration schema.GroupVersionKind{Group:\u0026#34;kubeadm.k8s.io\u0026#34;, Version:\u0026#34;v1beta2\u0026#34;, Kind:\u0026#34;KubeProxyConfiguration\u0026#34;} for scheme definitions in \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31\u0026#34; and \u0026#34;k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs/scheme.go:28\u0026#34; W0601 11:33:16.858535 1719 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io] [config] WARNING: Ignored YAML document with GroupVersionKind kubeadm.k8s.io/v1beta2, Kind=KubeProxyConfiguration [init] Using Kubernetes version: v1.18.0 [preflight] Running pre-flight checks [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [localhost.localdomain kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.1.102] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.1.102 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost.localdomain localhost] and IPs [192.168.1.102 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; W0601 11:33:33.405533 1719 manifests.go:225] the default kube-apiserver authorization-mode is \u0026#34;Node,RBAC\u0026#34;; using \u0026#34;Node,RBAC\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; W0601 11:33:33.411476 1719 manifests.go:225] the default kube-apiserver authorization-mode is \u0026#34;Node,RBAC\u0026#34;; using \u0026#34;Node,RBAC\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 31.511863 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.18\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Storing the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [upload-certs] Using certificate key: ca23402e2e70c5613b2ee10507b6065a548bb715f992c335e6498f25d30c0f96 [mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the label \u0026#34;node-role.kubernetes.io/master=\u0026#39;\u0026#39;\u0026#34; [mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: abcdef.0123456789abcdef [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.1.102:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:2d14d0998d3d2921771e6c6a81477b5124d87f920b7c4caeec8ebefe3c94fe5b 执行过程关键内容：\n[kubelet-start] 生成kubelet的配置文件”/var/lib/kubelet/config.yaml” [certificates] 生成相关的各种证书 [kubeconfig] 生成 KubeConfig 文件，存放在 /etc/kubernetes 目录中，组件之间通信需要使用对应文件 [control-plane] 使用 /etc/kubernetes/manifest 目录下的 YAML 文件，安装 Master 组件 [etcd] 使用 /etc/kubernetes/manifest/etcd.yaml 安装 Etcd 服务 [kubelet] 使用 configMap 配置 kubelet [patchnode] 更新 CNI 信息到 Node 上，通过注释的方式记录 [mark-control-plane] 为当前节点打标签，打了角色 Master，和不可调度标签，默认就不会使用 Master 节点来运行 Pod [bootstrap-token] 生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到 [addons] 安装附加组件 CoreDNS 和 kube-proxy  配置Kubectl  mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config # 非 ROOT 用户执行 chown $(id -u):$(id -g) $HOME/.kube/config 验证\nkubectl get node # 结果 NAME STATUS ROLES AGE VERSION kubernetes-master NotReady master 92m v1.18.0 kubeadm join 配置kubernetes slave节点 将 slave 节点加入到集群中，只需要在 slave 服务器上安装 kubeadm，kubectl，kubelet 三个工具，然后使用 kubeadm join 命令加入\n 配置kubernetes slave节点  kubeadm join 192.168.1.120:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:4133848ddc81242c50c95b684be0fa049e63362b1af542a49d9c31a65c2b138b # 结果 [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [kubelet-start] Downloading configuration for the kubelet from the \u0026#34;kubelet-config-1.18\u0026#34; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster.  验证  kubectl get nodes # 结果 NAME STATUS ROLES AGE VERSION kubernetes-slave1 NotReady \u0026lt;none\u0026gt; 5m14s v1.18.0 kubernetes-master NotReady master 92m v1.18.0 配置网络插件  关于容器网络  容器网络是容器选择连接到其他容器、主机和外部网络的机制。容器的 runtime 提供了各种网络模式，以Docker为例子，Docker 默认情况下可以为容器配置以下网络：\nnone： 将容器添加到一个容器专门的网络堆栈中，没有对外连接。 host： 将容器添加到主机的网络堆栈中，没有隔离。 default bridge： 默认网络模式。每个容器可以通过 IP 地址相互连接。 自定义网桥： 用户定义的网桥，具有更多的灵活性、隔离性和其他便利功能。  CNI  CNI（Container Network Interface）是CNCF旗下的一个项目，由一组用于配置Linux容器的网络接口的规范和库组成，同时还包含了一些插件。CNI仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。 CNI 的初衷是创建一个框架，用于在配置或销毁容器时动态配置适当的网络配置和资源。插件负责为接口配置和管理 IP 地址，并且通常提供与 IP 管理、每个容器的 IP 分配、以及多主机连接相关的功能。容器运行时会调用网络插件，从而在容器启动时分配 IP 地址并配置网络，并在删除容器时再次调用它以清理这些资源。 运行时或协调器决定了容器应该加入哪个网络以及它需要调用哪个插件。然后，插件会将接口添加到容器网络命名空间中，作为一个 veth 对的一侧。接着，它会在主机上进行更改，包括将 veth 的其他部分连接到网桥。再之后，它会通过调用单独的 IPAM（IP地址管理）插件来分配 IP 地址并设置路由。 在 Kubernetes 中，kubelet 可以在适当的时间调用它找到的插件，为通过 kubelet 启动的 pod进行自动的网络配置。 Kubernetes 中可选的 CNI 插件如下： - Flannel - Calico - Canal - Weave  安装calico  kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.apps/calico-node created serviceaccount/calico-node created deployment.apps/calico-kube-controllers created serviceaccount/calico-kube-controllers created  验证  watch kubectl get pods --all-namespaces kube-proxy开启ipvs  修改配置文件  修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: \u0026quot;ipvs\u0026quot;\nkubectl edit cm kube-proxy -n kube-system  重启各个节点上的kube-proxy pod  kubectl get pod -n kube-system | grep kube-proxy | awk \u0026#39;{system(\u0026#34;kubectl delete pod \u0026#34;$1\u0026#34; -n kube-system\u0026#34;)}\u0026#39;  验证  kubectl logs kube-proxy- -n kube-system kubectl 部署Nginx  检测组件运行状态  kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\u0026#34;health\u0026#34;:\u0026#34;true\u0026#34;}  检测master与slave节点状态  kubectl cluster-info kubectl get nodes  YAML配置文件  apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 这里的yaml配置文件，对应到 Kubernetes 中，就是一个 API Object（API 对象）。将配置文件提交给Kubernetes后， Kubernetes 就会负责创建出这些对象所定义的容器或者其他类型的 API 资源。\nKind 字段：指定了这个 API 对象的类型（Type）为 Deployment。\nPod 模版（spec.template）：定义一个 Pod 模版（spec.template）, Pod 包含一个容器，容器的镜像（spec.containers.image）是 nginx:1.7.9，容器监听端口（containerPort）是 80\nMetadata 字段：设置标识，Labels 字段主要用于 Kubernetes 过滤对象\n一个 Kubernetes 的 API 对象的定义，可以分为 Metadata 和 Spec 两个部分。前者存放的是这个对象的元数据；而后者存放属于这个对象独有的定义，用来描述它所要表达的功能。\n这里使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中叫作“控制器”模式（controller pattern）。Deployment 是 Pod 的控制器的角色。\n 相关指令  # 运行 kubectl create -f nginx-deployment.yaml # 更新 kubectl replace -f nginx-deployment.yaml kubectl apply -f nginx-deployment.yaml kubectl edit -f nginx-deployment.yaml # 删除 kubectl delete -f nginx_deployment.yml # 进入pod kubectl exec -it nginx-deployment-cc7df4f8f-nlcn8 # 查看对应的pod状态 kubectl get pods -l app=nginx # 查看Pod 的详细信息 kubectl describe pod nginx-deployment-cc7df4f8f-nlcn8  映射服务  # 映射Nginx服务80端口 kubectl expose deployment nginx-deployment --port=80 --type=LoadBalancer # 查看已发布服务 kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 7h55m nginx-deployment LoadBalancer 10.103.3.26 \u0026lt;pending\u0026gt; 80:30626/TCP 18s # 可以通过访问http://ip:30626/ 访问nginx页面 Kubernetes常用组件部署 Helm  关于Helm与chart  Helm是一个 Kubernetes 应用的包管理工具，用来管理 char——预先配置好的安装包资源，类似于 Ubuntu 的 APT 和 CentOS 中的 YUM。\nHelm chart 是用来封装 Kubernetes 原生应用程序的 YAML 文件，用于在部署应用的时候自定义应用程序的一些 metadata，便于应用程序的分发。\n Chart相关概念  一个 Chart 是一个 Helm 包，它包含在 Kubernetes 集群内部运行应用程序，工具或服务所需的所有资源定义。可以把它想像为一个自制软件，一个 Apt dpkg 或一个 Yum RPM 文件的 Kubernetes 环境里面的等价物。\n一个 Repository 是 Charts 收集和共享的地方，类似于包管理中心。\n一个 Release 是处于 Kubernetes 集群中运行的 Chart 的一个实例。一个 chart 通常可以多次安装到同一个群集中。每次安装时，都会创建一个新 release 。\n Helm相关概念  Helm 将 charts 安装到 Kubernetes 中，每个安装创建一个新 release 。要找到新的 chart，可以搜索 Helm charts 存储库 repositories。\n  Helm与chart作用\n  应用程序封装\n  版本管理\n  依赖检查\n  便于应用程序分发\n    Helm安装\n  Helm由客户端命helm令行工具和服务端tiller组成，下载helm命令行工具到master节点node1的/usr/local/bin下进行安装\nwget http://storage.googleapis.com/kubernetes-helm/helm-v2.15.1-linux-amd64.tar.gz tar -zxvf helm-v2.15.1-linux-amd64.tar.gz cd linux-amd64/ cp helm /usr/local/bin/ Helm 的服务器端部分 Tiller 通常运行在 Kubernetes 集群内部。因为Kubernetes APIServer开启了RBAC访问控制，所以需要创建tiller使用的service account: tiller并分配合适的角色给它。 通过查看helm文档中的Role-based Access Control。 简单的直接分配cluster-admin这个集群内置的ClusterRole给它，rbac-config.yaml文件如下所示：\napiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: tiller spec: roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system # 运行 kubectl create -f rbac-config.yaml # 结果 serviceaccount/tiller created clusterrolebinding.rbac.authorization.k8s.io/tiller created 使用helm部署tiller\n# tiller镜像地址修改为可用的，可以通过docker search tiller查看可用镜像，charts repo地址修改为国内源 helm init --upgrade -i sapcc/tiller:v2.15.1 --service-account=tiller --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 查看运行状况\n# tiller默认被部署在k8s集群中的kube-system的helm下 kubectl get pods -n kube-system -l app=helm NAME READY STATUS RESTARTS AGE tiller-deploy-6bdbf9884d-pstx4 1/1 Running 0 5m43s  Helm常用命令  # 查看版本 helm version # 查看当前安装的charts helm list # 查询 charts helm search nginx # 下载远程安装包到本地 helm fetch rancher-stable/rancher # 查看package详细信息 helm inspect chart #安装charts helm install --name nginx --namespaces prod bitnami/nginx # 查看charts状态 helm status nginx # 删除charts helm delete --purge nginx # 增加repo helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts helm repo add --username admin --password password myps https://harbor.pt1.cn/chartrepo/charts # 更新repo仓库资源 helm repo update # 创建charts helm create helm_charts # 测试charts语法 helm lint # 打包charts cd helm_charts \u0026amp;\u0026amp; helm package ./ # 查看生成的yaml文件 helm template helm_charts-0.1.1.tgz # 更新image helm upgrade --set image.tag=‘v201908‘ test update myharbor/study-api-en-oral # 回滚relase helm rollback nginx 使用Helm部署Ingress-nginx  部署Ingress-nginx，可以方便地将集群中的服务暴露到集群外部，从集群外部访问，使用helm部署操作如下  # 查询相关的Charts helm search stable/nginx-ingress # 下载远程安装包到本地 helm fetch stable/nginx-ingress tar -xvf nginx-ingress-1.40.2.tgz  修改values.yaml配置如下  # 1. 修改repository 地址 name: controller image: repository: siriuszg/nginx-ingress-controller tag: \u0026#34;0.33.0\u0026#34; pullPolicy: IfNotPresent runAsUser: 101 allowPrivilegeEscalation: true # ...... # Required for use with CNI based kubernetes installations (such as ones set up by kubeadm), # since CNI and hostport don\u0026#39;t mix yet. Can be deprecated once https://github.com/kubernetes/kubernetes/issues/23920 # is merged # 2. 设置nginx ingress controller使用宿主机网络，设置hostNetwork为true hostNetwork: true dnsConfig: {} dnsPolicy: ClusterFirst reportNodeInternalIp: false ## Use host ports 80 and 443 # 3. 使用主机端口打开 daemonset: useHostPort: true hostPorts: http: 80 https: 443 # ...... # 4. 因为使用的是hostnetwork的方式，因此不创建service，这里设置enabled为false service: enabled: false annotations: {} labels: {} # ......  安装Chart  helm install --name nginx-ingress -f nginx-values.yaml . --namespace kube-system  验证  # 查看nginx-ingress-controller 部署到的ip，访问http://192.168.1.121返回default backend，则部署完成 kubectl get pod -n kube-system -o wide 使用Helm部署kubernetes-dashboard  创建/安装tls secret  openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout ./tls.key -out ./tls.crt -subj \u0026#34;/CN=192.168.1.121\u0026#34; kubectl -n kube-system create secret tls dashboard-tls-secret --key ./tls.key --cert ./tls.crt # 查看 kubectl get secret -n kube-system |grep dashboard  helm下载kubernetes-dashboard  helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ helm search kubernetes-dashboard/kubernetes-dashboard helm fetch kubernetes-dashboard/kubernetes-dashboard tar -xvf kubernetes-dashboard-2.2.0.tgz cp values.yaml dashboard-chart-2.yaml  自定义chart文件  image: repository: kubernetesui/dashboard tag: v2.0.3 pullPolicy: IfNotPresent pullSecrets: [] replicaCount: 1 ingress: enabled: true annotations: kubernetes.io/ingress.class: nginx kubernetes.io/tls-acme: \u0026#39;true\u0026#39; nginx.ingress.kubernetes.io/backend-protocol: \u0026#34;HTTPS\u0026#34; hosts: - lhr.dashboard.com tls: - secretName: dashboard-tls-secret hosts: - lhr.dashboard.com paths: - / metricsScraper: enabled: true image: repository: kubernetesui/metrics-scraper tag: v1.0.4 resources: {} containerSecurityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 rbac: create: true clusterRoleMetrics: true serviceAccount: create: true name: dashboard-admin livenessProbe: initialDelaySeconds: 30 timeoutSeconds: 30 podDisruptionBudget: enabled: false minAvailable: maxUnavailable: containerSecurityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 networkPolicy: enabled: false  chart安装  # 安装 helm install --name kubernetes-dashboard-2 -f dashboard-chart.yaml . --namespace kube-system # 更新配置 helm upgrade kubernetes-dashboard-2 -f dashboard-chart.yaml . --namespace kube-system  token获取  # 新增token获取脚本 vim dashboard-token.sh # 如下所示 #!/bin/sh TOKENS=$(kubectl describe serviceaccount dashboard-admin -n kube-system | grep \u0026#34;Tokens:\u0026#34; | awk \u0026#39;{ print $2}\u0026#39;) kubectl describe secret $TOKENS -n kube-system | grep \u0026#34;token:\u0026#34; | awk \u0026#39;{ print $2}\u0026#39; # 设置别名 vim ~/.bashrc alias k8s-dashboard-token=\u0026#34;sh /data/chart/dashboard/kubernetes-dashboard/dashboard-token.sh\u0026#34; source ~/.bashrc 使用Helm部署metrics-server metrics-server是kubernetes的监控组件，可以配合kubernetes-dashboard使用\n helm下载metrics-server  helm search stable/metrics-server helm fetch stable/metrics-server tar -xvf metrics-server-2.11.1.tgz  自定义chart文件  rbac: create: true pspEnabled: true serviceAccount: create: true name: metircs-admin apiService: create: true hostNetwork: enabled: false image: repository: mirrorgooglecontainers/metrics-server-amd64 tag: v0.3.6 pullPolicy: IfNotPresent replicas: 1 args: - --logtostderr - --kubelet-insecure-tls=true - --kubelet-preferred-address-types=InternalIP livenessProbe: httpGet: path: /healthz port: https scheme: HTTPS initialDelaySeconds: 20 readinessProbe: httpGet: path: /healthz port: https scheme: HTTPS initialDelaySeconds: 20 securityContext: allowPrivilegeEscalation: false capabilities: drop: [\u0026#34;all\u0026#34;] readOnlyRootFilesystem: true runAsGroup: 10001 runAsNonRoot: true runAsUser: 10001 service: annotations: {} labels: {} port: 443 type: ClusterIP podDisruptionBudget: enabled: false minAvailable: maxUnavailable:  Chart安装  helm install --name metrics-server -f metrics-chart.yaml . --namespace kube-system helm upgrade metrics-server -f metrics-chart.yaml . --namespace kube-system  查看相关指标  # kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% kubernetes-master 396m 19% 1189Mi 68% kubernetes-slave 194m 19% 746Mi 42%  效果图  FAQ  Kubernetes的slave节点上执行kubectl命令出现错误：The connection to the server localhost:8080 was refused - did you specify the right host or port?  出现这个问题的原因是kubectl命令需要使用kubernetes-admin来运行，需要将主节点中的/etc/kubernetes/admin.conf文件拷贝到从节点相同目录下，然后配置环境变量\n# 拷贝内容 vim /etc/kubernetes/admin.conf # 配置环境变量 echo \u0026#34;export KUBECONFIG=/etc/kubernetes/admin.conf\u0026#34; \u0026gt;\u0026gt; ~/.bash_profile source ~/.bash_profile  helm list命令出现 Error: Get https://10.96.0.1:443/api/v1/namespaces/kube-system/configmaps?labelSelector=OWNER%!D(MISSING)TILLER: dial tcp 10.96.0.1:443: i/o timeout  出现这个问题的原有是网络组件（比如Calico)的IP_POOL和宿主机所在的局域网IP段冲突了。这里我们使用的是Calico，可以通过calicoctl修改Calico插件的IP池。\ncalicoctl允许您从命令行创建、读取、更新和删除Calico对象。可以参考相关的文档：https://docs.projectcalico.org/introduction/\n1-安装calicoctl到kubernetes的master节点上\ncd /usr/local/bin curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.8.2/calicoctl chmod +x calicoctl 2-安装 calicoctl 为 Kubernetes pod,设置alias\nkubectl apply -f https://docs.projectcalico.org/manifests/calicoctl.yaml alias calicoctl=\u0026#34;kubectl exec -i -n kube-system calicoctl -- /calicoctl\u0026#34; 3-变更IP池\n# 查看ip池 calicoctl get ippool -o wide NAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTOR default-ipv4-ippool 192.168.0.0/16 true Always Never false all() # 添加新的ip池 calicoctl create -f -\u0026lt;\u0026lt;EOF apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: b-ipv4-pool spec: cidr: 172.16.0.0/16 ipipMode: Always natOutgoing: true EOF; # 再次查看ip池 calicoctl get ippool -o wide NAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTOR b-ipv4-pool 172.16.0.0/16 true Always Never false all() default-ipv4-ippool 192.168.0.0/16 true Always Never false all() # 禁用旧的IP池 calicoctl get ippool -o yaml \u0026gt; /data/calico/pools.yaml vim /data/calico/pools.yaml apiVersion: projectcalico.org/v3 items: - apiVersion: projectcalico.org/v3 kind: IPPool metadata: creationTimestamp: \u0026#34;2020-07-01T14:32:18Z\u0026#34; name: b-ipv4-pool resourceVersion: \u0026#34;299739\u0026#34; uid: f908d360-3477-4309-a207-f79ac5750b14 spec: blockSize: 26 cidr: 172.16.0.0/16 ipipMode: Always natOutgoing: true nodeSelector: all() vxlanMode: Never - apiVersion: projectcalico.org/v3 kind: IPPool metadata: creationTimestamp: \u0026#34;2020-06-26T17:14:39Z\u0026#34; name: default-ipv4-ippool resourceVersion: \u0026#34;3819\u0026#34; uid: 84b1ab4d-7236-4ff4-8e83-597a51856c21 spec: blockSize: 26 cidr: 192.168.0.0/16 ipipMode: Always natOutgoing: true disabled: true # 设置disabled 为true nodeSelector: all() vxlanMode: Never kind: IPPoolList metadata: resourceVersion: \u0026#34;299994\u0026#34; # 执行操作，变更应用 calicoctl apply -f - \u0026lt; pools.yaml calicoctl get ippool -o wide NAME CIDR NAT IPIPMODE VXLANMODE DISABLED SELECTOR b-ipv4-pool 172.16.0.0/16 true Always Never false all() default-ipv4-ippool 192.168.0.0/16 true Always Never true all() # 重启tiller pod/所有pod kubectl -n kube-system delete pods tiller-deploy-6bdbf9884d-qz978 kubectl -n [命名空间] delete pods --all # 删除旧的ip池 calicoctl delete pool default-ipv4-ippool  解决k8s.gcr.io被墙问题  由于官方镜像地址被墙，所以我们需要首先获取所需镜像以及它们的版本。然后从国内镜像站获取。\n# 获取镜像列表 kubeadm config images list 从阿里云获取镜像，通过docker tag命令来修改镜像的标签\nimages=( kube-apiserver:v1.12.1 kube-controller-manager:v1.12.1 kube-scheduler:v1.12.1 kube-proxy:v1.12.1 pause:3.1 etcd:3.2.24 coredns:1.2.2 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName done  Chart镜像版本过低问题  $ helm search kubernetes-dashboard NAME CHART VERSION\tAPP VERSION\tDESCRIPTION stable/kubernetes-dashboard\t0.6.0 1.8.3 General-purpose web UI for Kubernetes clusters $ helm repo add stable http://mirror.azure.cn/kubernetes/charts/ \u0026#34;stable\u0026#34; has been added to your repositories $ helm search kubernetes-dashboard NAME CHART VERSION\tAPP VERSION\tDESCRIPTION stable/kubernetes-dashboard\t1.11.1 1.10.1 DEPRECATED! - General-purpose web UI for Kubernetes clusters  kubeadm token过期问题  # 生成一条永久有效的token kubeadm token create --ttl 0 # 获取ca证书sha256编码hash值 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; # node节点加入 kubeadm join 192.168.1.120:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:4133848ddc81242c50c95b684be0fa049e63362b1af542a49d9c31a65c2b138b "
            }
    
        ,
            {
                "id": 12,
                "href": "https://chinalhr.github.io/post/devops-sonar-practice/",
                "title": "DevOps流程中使用SonarQube推动代码质量优化",
                "section": "post",
                "date" : "2020.04.12",
                "body": " 在持续集成中利用SonarQube推动代码质量的持续优化的实践\n DevOps DevOps（Development和Operations的组合词）是一种重视“开发人员（Dev）”和“运维人员（Ops）”之间沟通合作的文化、运动或惯例。通过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。\nDevOps生命周期： 版本控制、持续集成、持续部署和持续监控贯穿于软件开发的整个生命周期。\n持续集成： 将所有软件工程师对于软件的工作副本持续集成到共享主线（mainline）的一种举措，例如将git的特性分支合并到主分支。持续集成的宗旨是避免集成问题， 组织在运用持续性集成(CI)一般会建置CI服务器来维护持续性套用质量控制的程序，一般包括自动化单元测试，自动化静态代码扫描，自动化代码风格检测\u0026hellip;.，意在提升软件质量以及减少交付的时间。\n参考链接：https://zh.wikipedia.org/wiki/DevOps\n消除技术债务危机 软件的质量通常分为两种，内部质量通常指代码和设计的质量。内部质量可以通过应用设计和编程达到最佳实践，也可以通过持续一致的开发和交付流程来提高。外部质量是通过查看和使用软件（例如验收测试）来度量的。从长远的角度看，内部质量不佳最终会影响外部质量，应用程序会持续不断地冒出新的bug，产生技术债务，而且开发时间会由于技术债务的增加而变长，由于初始鲁莽的设计决策，在未来的开发中我们需要付出更多努力，消耗更多的时间，这就是技术债务危机。\n技术债务危机产生一般分为两类：\n 无意识的 - 由于经验的缺乏导致初级开发者编写了质量低劣的代码 有意识的 - 团队根据当前而非未来进行设计选型，这种方式能很快解决当前的问题，但却很拙劣  现象主要围绕着下面几点：\n 重复 糟糕的复杂性分布 意大利面式设计风格 缺少单元测试 缺乏代码标准 潜在的bug 注释不足或过多  技术债务的处理：\n在企业的DevOps建设过程中，我们可以基于持续集成的代码构建和自动化分析, 通过很多不同的维度评价代码设计的内部质量 。 可以通过Sonar质量度量平台，实现代码设计质量的持续监控，包括对代码复杂度、重复度、代码风格、单元测试、测试覆盖率的监控等。促进技术债务的及时处理，从而提高软件系统的总体开发运维效益。\nSonarQube 介绍 SonarQube是一个开源的代码质量管理系统。\n特性：\n 支持超过25种编程语言。 提供重复代码、编码标准、单元测试、代码覆盖率、代码复杂度、潜在Bug、注释和软件设计报告。 提供了指标历史记录、计划图和微分查看。 提供了完全自动化的分析：与Maven、Ant、Gradle和持续集成工具（例如Jenkins）。 可以与Eclipse，IDEA等开发环境集成。 可以与JIRA、Mantis、LDAP、Fortify等外部工具集集成。 支持扩展插件。  架构 SonarQube平台由4个组件组成：\n SonarQube Server：  SonarQube Web服务器：供开发者、管理人员浏览质量指标和进行SonarQube的配置 基于Elasticsearch的搜索 Compute Engine：负责处理代码分析报告并将其保存在SonarQube数据库中   SonarQube Database： 存储SonarQube的配置与质量报告，各种视图数据 SonarQube Plugins：SonarQube插件支持，包括开发语言，SCM，持续集成，安全认证等 SonarQube Scanner：在构建/持续集成服务器上运行一个或多个SonarScanner，以分析项目  工作流程 上图可以看出SonarQube各组件的工作流程：\n 开发者在IDE中编码，可以使用SonarLint执行本地代码分析 开发者向SCM（Git，SVN，TFVC等）提交代码 代码提交触发持续集成平台（如Jenkins等）自动构建，执行SonarQube Scanner进行分析 持续集成平台将分析报告发送到SonarQube Server进行处理 SonarQube Server处理好的分析报告生成对应可视化的视图并保存数据到数据库 开发者可以在SonarQube UI进行查看，评论，通过解决问题来管理和减少技术债 SonarQube支持导出报表，使用API提取数据，基于JMX的监控  基础概念  指标：指标的定义主要有复杂度，重复项，问题，可维护性，安全性，复杂度，测试覆盖率等。 代码分析规则：SonarQube中可以通过插件提供的规则对代码进行分析并生成问题。规则中定义了修复问题的成本（时间），解决问题的代价以及技术债可以通过这些问题进行计算。规则一般有三种类型：可靠性（Bug），可维护性（坏味道），安全性（漏洞）。 质量阈 ： 质量阈是一系列对项目指标进行度量的条件形成的阈值。  部署 具体部署方式可以参考官方文档：https://docs.sonarqube.org/latest/setup/overview/ \n简单便捷，小规模使用可以基于docker compose部署，如下所示：\n# 镜像拉取与基础目录创建 docker pull sonarqube:8.2-community docker pull postgres:12 mkdir -p /opt/sonarqube-data/{conf,data,logs,extensions,postgresql,postgresql-data} chmod -R 777 /opt/sonarqube-data # 启动 docker-compose -f sonar-docker-compose.yml up -d version: \u0026#34;3\u0026#34;services: sonarqube: image: sonarqube:8.2-community ports: - \u0026#34;9000:9000\u0026#34; networks: - sonarnet environment: - sonar.jdbc.url=jdbc:postgresql://db:5432/sonar?useUnicode=true\u0026amp;characterEncoding=utf8\u0026amp;rewriteBatchedStatements=true\u0026amp;useConfigs=maxPerformance\u0026amp;useSSL=false\u0026#34; - sonar.jdbc.username=sonar - sonar.jdbc.password=sonar restart: always volumes: - sonarqube_conf:/opt/sonarqube/conf - sonarqube_data:/opt/sonarqube/data - sonarqube_extensions:/opt/sonarqube/extensions db: image: postgres:12 networks: - sonarnet environment: - POSTGRES_USER=sonar - POSTGRES_PASSWORD=sonar restart: always volumes: - postgresql:/var/lib/postgresql - postgresql_data:/var/lib/postgresql/datanetworks: sonarnet: driver: bridgevolumes: sonarqube_conf: sonarqube_data: sonarqube_extensions: postgresql: postgresql_data:Jenkins集成SonarQube 利用Jenkins集成SonarQube可以实现在持续集成过程中的利用SonarQube Scanner进行静态代码分析。\n静态代码分析 前置准备   Jenkins安装插件：SonarQube Scanner for Jenkins\n  系统设置—\u0026gt;配置—\u0026gt;SonarQube servers中对SonarQube server进行配置\n  注意：server authentication token为SonarQube用户生成的token，类型为Secret text。\n 系统管理—\u0026gt;全局工具配置—\u0026gt;SonarQube Scanner配置   代码库的根目录创建sonar-project.properties  这里使用的项目以美团开源的Leaf为例子，项目目录如下：\n├── Leaf │ ├── leaf-core │ ├── leaf-server sonar-project.properties配置文件配置如下：\nsonar.projectKey=Leaf sonar.projectName=Leaf sonar.projectVersion=0.0.1 sonar.sources=src/main/java sonar.java.binaries=target/classes sonar.java.source=1.8 sonar.java.target=1.8 sonar.language=java sonar.sourceEncoding=UTF-8 sonar.modules=leaf-core,leaf-server 参考SonarQube文档：https://docs.sonarqube.org/latest/analysis/languages/java/\nPipeline配置 stage(\u0026#39;拉取代码，构建\u0026#39;) { ... } stage(\u0026#39;静态代码扫描\u0026#39;) { steps { script {scannerHome = tool \u0026#39;SonarQube Scanner\u0026#39;} withSonarQubeEnv(\u0026#39;SonarQube-Dev\u0026#39;) { sh \u0026#34;${scannerHome}/bin/sonar-scanner\u0026#34; } } } 构建成功，SonarQube Scanner扫描完成后可以在SonarQube服务器看见对应项目的报表如下：\n测试覆盖率控制 关于Jacoco jacoco是一个开源的覆盖率工具，可以嵌入到 Ant 、Maven 中使用，提供了Eclipse与IDEA插件，也可以使用 Java Agent 技术监控 Java 程序，第三方自动化工具如SonarQube，Jenkins也对jacoco提供了很好的支持。\n代码覆盖(Code coverage) 是软件测试中的一种度量，描述程序中源代码被测试的比例和程度。\nJacoco 包含了多种尺度的覆盖率计数器,包含指令级（Instructions,C0 coverage），分支（Branches,C1 coverage）、圈复杂度（Cyclomatic Complexity）、行（Lines）、方法（Non-abstract Methods）、类（Classes）。\n Instructions：Jacoco 计算的最小单位是字节码指令。指令覆盖率表明了在所有的指令中，哪些被执行过以及哪些没有被执行。这项指数完全独立于源码格式并且在任何情况下有效，不需要类文件的调试信息。 Branches：Jacoco 对所有的 if 和 switch 指令计算了分支覆盖率。这项指标会统计所有的分支数量，并同时支出哪些分支被执行，哪些分支没有被执行。这项指标也在任何情况都有效。异常处理不考虑在分支范围内。 Cyclomatic Complexity：Jacoco 为每个非抽象方法计算圈复杂度，并也会计算每个类、包、组的复杂度。根据 McCabe 1996 的定义，圈复杂度可以理解为覆盖所有的可能情况最少使用的测试用例数。这项参数也在任何情况下有效。 Lines：该项指数在有调试信息的情况下计算。 Methods：每一个非抽象方法都至少有一条指令。若一个方法至少被执行了一条指令，就认为它被执行过。因为 Jacoco 直接对字节码进行操作，所以有些方法没有在源码显示（比如某些构造方法和由编译器自动生成的方法）也会被计入在内。 Classes：每个类中只要有一个方法被执行，这个类就被认定为被执行。同 Methods一样，有些没有在源码声明的方法被执行，也认定该类被执行。  前置准备   Jenkins安装插件JaCoCo plugin。\n  maven配置jacoco插件\n  \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.jacoco\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jacoco-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.7.5.201505241946\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;pre-unit-test\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;prepare-agent\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;destFile\u0026gt; ${project.build.directory}/${project.artifactId}-jacoco.exec \u0026lt;/destFile\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;post-unit-test\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;report\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;dataFile\u0026gt; ${project.build.directory}/${project.artifactId}-jacoco.exec \u0026lt;/dataFile\u0026gt; \u0026lt;outputDirectory\u0026gt;${project.build.directory}/jacoco\u0026lt;/outputDirectory\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; 该配置会在当前目录下的target目录产生一个jacoco.exec文件，该文件就是覆盖率的文件，相关的报表输出文件为当前目录下的target/jacoco目录。由于绑定了maven test生命周期，可以执行如下命令进行覆盖率文件与报表生成： mvn clean test -Dmaven.test.failure.ignore=true\npipeline配置 pipeline脚本如下所示，其中jacoco对覆盖率指标进行阈值设置，可参考pipeline-syntax进行配置，由于我们使用了SonarQube作为质量管理系统，对于覆盖率指标的把控也可以使用SonarQube的质量阈进行控制。\nstage(\u0026#39;代码覆盖率分析\u0026#39;) { steps { withMaven(maven: \u0026#39;maven3.6.3\u0026#39;) { sh \u0026#39;mvn clean test -Dmaven.test.failure.ignore=true\u0026#39; } jacoco( buildOverBuild: false, //未达到要求修改pipeline的status为fail  changeBuildStatus: true, //各种尺度指标阈值  maximumBranchCoverage: \u0026#39;80\u0026#39;, maximumClassCoverage: \u0026#39;80\u0026#39;, maximumComplexityCoverage: \u0026#39;80\u0026#39;, maximumLineCoverage: \u0026#39;80\u0026#39;, maximumMethodCoverage: \u0026#39;80\u0026#39;, minimumBranchCoverage: \u0026#39;50\u0026#39;, minimumClassCoverage: \u0026#39;50\u0026#39;, minimumComplexityCoverage: \u0026#39;50\u0026#39;, minimumLineCoverage: \u0026#39;50\u0026#39;, minimumMethodCoverage: \u0026#39;50\u0026#39;, sourceInclusionPattern: \u0026#39;**/*.java\u0026#39; ) } } 运行后在Jenkins上的Coverage Report可以查看到对应的覆盖率报告：\n单元测试覆盖率统计数据上报SonarQube 需要在sonar-project.properties配置文件里配置jacoco报表，surefire报表路径，如下所示：\n# 配置junit单元测试源码与报表 sonar.tests=src/test sonar.java.test.binaries=target/test-classes sonar.junit.reportsPaths=target/surefire-reports # 配置jacoco相关 sonar.java.coveragePlugin=jacoco sonar.coverage.jacoco.xmlReportPaths=target/jacoco/jacoco.xml # 如果仅需要扫描某个模块，可以设置为xmodule.sonar.surefire.reportsPath=target/surefire-reports... 参考sonarQube文档：https://docs.sonarqube.org/latest/analysis/coverage/\n最终效果如下：\nSonarQube推动代码质量持续优化 上文说明了如何利用SonarQube结合Jenkins在pipeline中进行静态代码检测与测试覆盖率检测，在实际的开发中我们可以基于Git-Flow分支模型，通过Git的Webhook结合Jenkins，SonarQube质量阈，在CI中利用SonarQube推动代码质量的持续优化，具体流程如下所示。\n具体流程  工程师完成特性分支的开发，将代码推送到远程服务器上 Git 服务器 根据配置的Webhook 回调通知Jenkins服务器 Jenkins服务器SonarQube 质量分析Job执行（可以基于Git-flow的工作流，过滤特定的分支如test/develop提交时才进行执行） Jenkins 通过SonarQube Scanner 插件分析代码 Jenkins将分析产生的报告结果发送到SonarQube服务器上 SonarQube服务器对接收到的分析报告进行可视化，存储以及质量阈的校验 SonarQube服务器回调通知Jenkins质量阈状态 当质量阈状态为失败时，Jenkins将结果对工程师进行通知  具体操作  Generic Webhook Trigger配置  Jenkins安装 Generic Webhook Trigger插件，该插件可以接收任何HTTP请求，然后从JSON或XML中提取任何值，并使用这些值作为变量来触发作业。一般用来配合GitHub，GitLab，Bitbucket，Jira等的Webhook一起使用。\njob勾选构建触发器Generic Webhook Trigger选项，设置token，如图所示：\n以Gitlab，Gitee或者Github的代码仓库WebHooks配置Jenkins Generic Webhook Trigger的回调地址 （http://[JenkinsIP]/generic-webhook-trigger/invoke?token=gitee-leaf-sonar-token)。以码云为例子，可以选择如下几个触发事件，当事件触发时回调我们的Jenkins 地址，触发job执行。\n参考文档：https://plugins.jenkins.io/generic-webhook-trigger/\n Generic Webhook Trigger指定分支触发  一般情况下，我们都会基于分支模型进行开发，例如常见的有git-flow分支模型。基于git-flow分支模型，我们可以在feature分支合并到develop/test的时候（特性分支提测的时候），触发Jenkins Job执行，确保在上线正式/灰度环境之前代码已经通过了SonarQube 质量阈。\n以Gitee为例子，触发事件发送请求，请求的body里包含了本次提交的一些信息，其中 \u0026quot;ref\u0026quot;: \u0026quot;refs/heads/test\u0026quot;,表示本次提交的分支。有了这个信息，可以通过Generic Webhook Trigger的Post content parameters将请求体中的ref内容提取出来通过表达式Expression赋给$ref。\n在Optional filter中 Expression 填写正则，Text 填写我们刚刚设置的变量 $ref进行匹配，如下所示对test分支进行了匹配，实现指定分支触发job的目的。\n SonarQube配置Jenkins webhook  这里用于在Sonarqube完成扫描后，通知Jenkins扫描结果，参考文档：https://docs.sonarqube.org/latest/project-administration/webhooks/\nhttps://www.cnblogs.com/hackyo/p/10149171.html-/\n Jenkins配置Quality Gates - Sonarqube  Jenkins安装Sonar Quality Gates 插件，通过此插件可以让Jenkins等待SonarQube分析完成并返回质量阈状态 。当返回为Fail时，我们可以aborted 对应的job并进行通知操作。在 Jenkins的Manage Jenkins -\u0026gt; Configure System -\u0026gt; Quality Gates - Sonarqube，对其进行设置。\n 配置质量阈  在SonarQube质量阈界面对指标进行配置，并分配到项目上。\n pipeline脚本如下  pipeline { agent any stages { stage(\u0026#39;拉取代码\u0026#39;) { steps { git branch: \u0026#39;test\u0026#39;, credentialsId: \u0026#39;5c5a2fae-5468-472a-a4d5-0934dd311fd5\u0026#39;, url: \u0026#39;git@gitee.com:**/***.git\u0026#39; } } stage(\u0026#39;构建\u0026#39;) { steps { withMaven(maven: \u0026#39;maven3.6.3\u0026#39;) { sh \u0026#39;mvn clean package -Dmaven.test.skip=true\u0026#39; } } } stage(\u0026#39;代码覆盖率分析\u0026#39;) { steps { withMaven(maven: \u0026#39;maven3.6.3\u0026#39;) { sh \u0026#39;mvn clean test -Dmaven.test.failure.ignore=true\u0026#39; } } } stage(\u0026#39;静态代码扫描\u0026#39;) { steps { script {scannerHome = tool \u0026#39;SonarQube Scanner\u0026#39;} withSonarQubeEnv(\u0026#39;SonarQube-Dev\u0026#39;) { sh \u0026#34;${scannerHome}/bin/sonar-scanner\u0026#34; } } } stage(\u0026#34;质量阈校验\u0026#34;) { steps { timeout(time: 1, unit: \u0026#39;HOURS\u0026#39;) { script{ // Wait for SonarQube analysis to be completed and return quality gate status  def qg = waitForQualityGate(); if (qg.status != \u0026#39;OK\u0026#39;) { error \u0026#34;Pipeline aborted due to quality gate failure: ${qg.status}\u0026#34; //进行消息通知，可以通过SonarQube Web API获取具体失败信息  } } } } } } } "
            }
    
        ,
            {
                "id": 13,
                "href": "https://chinalhr.github.io/post/java-asyncprogram/",
                "title": "Java技术域中的异步编程",
                "section": "post",
                "date" : "2020.04.03",
                "body": " Java技术域异步编程总结\n 异步编程优势 异步编程是可以让程序并行运行的一种手段，其可以让程序中的一个工作单元与主应用程序线程分开独立运行，并且等工作单元运行结束后通知主应用程序线程它的运行结果或者失败原因。使用它有许多好处，例如改进的应用程序性能和减少用户等待时间等。异步编程充分利用单核CPU的性能，意在单个CPU上执行几个松耦合的任务。\n同步阻塞对比异步非阻塞\n当线程发生一次rpc调用或者http调用，又或者其他的一些耗时的IO调用，发起之后，如果是同步阻塞，这个线程就会被阻塞挂起，直到结果返回，如果IO调用很频繁的话CPU使用率其实会很低。通过异步非阻塞调用，当发生IO调用时我只需要把回调函数写入这次IO调用，我这个时候线程可以继续处理新的请求，当IO调用结束结束时，会调用回调函数，充分利用CPU资源。\n异步编程场景：\n异步化并不是万能，异步化程序并不能缩短整个链路调用时间长的问题，而是旨在最大化提升qps，但也需要针对场景异步优化。\nIO密集型：例如网络调用，文件传输，文件读取\u0026hellip;，这个时候线程一般会挂起阻塞，异步编程可以针对这个场景进行优化。\nCPU密集型：例如一些数据的聚合运算，对象的序列化，排序查找等CPU耗时任务，异步化并不能解决这个问题，需要进行一些算法的优化或者利用一些并行处理框架进行优化，充分利用多核CPU。\nJava异步编程技术实现方式 线程池 具体使用方式如下代码所示：\nprivate final static ThreadPoolExecutor POOL_EXECUTOR = new ThreadPoolExecutor(AVALIABLE_PROCESSORS, AVALIABLE_PROCESSORS * 2, 1, TimeUnit.MINUTES, new LinkedBlockingQueue\u0026lt;\u0026gt;(5), new ThreadPoolExecutor.CallerRunsPolicy()); /** * 投递任务，不获取返回值 */ @Test public void threadPoolAsyncRunnable() { StopWatch watch = new StopWatch(); watch.start(\u0026#34;ThreadPoolAsyncTask\u0026#34;); POOL_EXECUTOR.execute(() -\u0026gt; doSomeThing(\u0026#34;task1\u0026#34;)); doSomeThing(\u0026#34;task2\u0026#34;); watch.stop(); log.info(\u0026#34;total time :{} millisecond\u0026#34;, watch.getTotalTimeMillis()); } /** * 投递任务，同步获取 */ @Test public void threadPoolAsyncSubmit() throws ExecutionException, InterruptedException { StopWatch watch = new StopWatch(); watch.start(\u0026#34;ThreadPoolAsyncTask\u0026#34;); Future\u0026lt;String\u0026gt; resultFuture = POOL_EXECUTOR.submit(() -\u0026gt; doSomeThing(\u0026#34;task1\u0026#34;)); doSomeThing(\u0026#34;task2\u0026#34;); //同步等待结果  String result = resultFuture.get(); watch.stop(); log.info(\u0026#34;total time :{} millisecond\u0026#34;, watch.getTotalTimeMillis()); } 核心理念 使用线程来进行异步执行，避免阻塞当前线程，利用线程池来实现线程的复用。\n线程池构造  ctl：是Integer的原子变量，同时记录线程池状态和线程池中线程个数  - 线程池状态 RUNNING：接收新任务并且处理阻塞队列里的任务 SHUTDOWN：拒绝新任务但是处理阻塞队列里的任务 STOP：拒绝新任务并且抛弃阻塞队列里的任务，同时中断正在处理的任务 TIDYING：所有任务都执行完（包含阻塞队列里面任务），当前线程池活动线程为0，将要调用terminated方法 TERMINATED：终止状态。terminated方法调用完成以后的状态 - 状态转换 显式调用shutdown()方法或者隐式调用了finalize()：RUNNING→SHUTDOWN 显式调用shutdownNow()方法：RUNNING或者SHUTDOWN→STOP 当线程池和任务队列都为空时：SHUTDOWN→TIDYING 当线程池为空时：STOP→TIDYING 当terminated() hook方法执行完成时：TIDYING→TERMINATED  corePoolSize：线程池核心线程个数 workQueue：用于保存等待执行的任务的阻塞队列 maximunPoolSize：线程池最大线程数量 threadFactory：创建线程的工厂类 defaultHandler：饱和策略，当队列满了并且线程个数达到maximunPoolSize后采取的策略，比如AbortPolicy（抛出异常）、CallerRunsPolicy（使用调用者所在线程来运行任务）、DiscardOldestPolicy（调用poll丢弃一个任务，执行当前任务）、DiscardPolicy（默默丢弃，不抛出异常） keeyAliveTime：存活时间。如果当前线程池中的线程数量比核心线程数量要多，并且是闲置状态的话，这些闲置的线程能存活的最大时间。  线程池投递任务原理简述  execute  大致流程如下：\npublic void execute(Runnable command) { ... //1. 获取当前线程池的状态+线程个数变量的组合值  int c = ctl.get(); //2. 当前线程池线程个数是否小于corePoolSize，小于则开启新线程运行  if (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } //3. 如果线程池处于RUNNING状态，则添加任务到阻塞队列  if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { //3.1 二次检查  int recheck = ctl.get(); //3.2 如果当前线程池状态不是RUNNING则从队列删除任务，并执行拒绝策略  if (! isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); //3.3 如果当前线程池线程为空，则添加一个线程  else if (workerCountOf(recheck) == 0) addWorker(null, false); } //4. 如果队列满了，则新增线程执行，如果当前线程池的线程个数大于maximumPoolSize执行拒绝策略  else if (! addWorker(command, false)) reject(command); }  submit  submit会将Runnable包装成RunnableFuture后，调用execute投递到线程池执行\n worker  上述addWorker方法执行后，用户线程会马上返回，任务稍后再由Worker线程执行。Worker本身实现了Runnable方法，具体如下：\npublic void run() { runWorker(this); //委托给runWorker方法 } worker的runWorker方法会获取线程进行任务执行，并且会执行调用前后的钩子方法。\n线程池关闭 总结 虽然线程池方式提供了线程复用可以获取任务返回值，但是获取返回值时还是需要阻塞调用线程的。\nJDK的Future JUC包中Future可以用于异步计算，Future中提供了一系列方法用来检查计算结果是否已经完成，同步等待任务执行完成，获取计算结果。\nFutureTask FutureTask实现了Future接口，接受的任务可以是Callable类型，也可以是Runnable类型，一般被提交到线程池中进行异步执行，最后调用get系列方法阻塞获取。具体使用方式如下代码所示：\n//异步执行task1  FutureTask\u0026lt;String\u0026gt; task = new FutureTask\u0026lt;\u0026gt;(() -\u0026gt; doSomeThing(\u0026#34;task1\u0026#34;)); POOL_EXECUTOR.execute(task); //同步执行task2  String result2 = doSomeThing(\u0026#34;task2\u0026#34;); //同步等待task1  String result1 = task.get(); FutureTask构造  state：用来记录任务的状态  private static final int NEW = 0;\t//新建 private static final int COMPLETING = 1;\t//完成中... private static final int NORMAL = 2;\t//正常 private static final int EXCEPTIONAL = 3;\t//异常 private static final int CANCELLED = 4;\t//取消 private static final int INTERRUPTING = 5;\t//中断中... private static final int INTERRUPTED = 6;\t//中断 - 状态转换 NEW→COMPLETING→NORMAL：正常终止流程转换 NEW→COMPLETING→EXCEPTIONAL：执行过程中发生异常流程转换 NEW→CANCELLED：任务还没开始就被取消 NEW→INTERRUPTING→INTERRUPTED：任务被中断  outcome：任务运行的结果 runner：记录运行该任务的线程 waiters：链表，记录等待任务结果的线程  FutureTask执行与获取  run  当创建一个FutureTask时，其任务状态初始化为NEW，提交到线程或者线程池后，会有一个线程来执行该FutureTask任务，通过调用FutureTask的run()执行。run()方法判断并且设置state状态，调用call执行任务，当任务执行完毕后会把结果或者异常信息设置到outcome变量，最后遍历激活waiters链表中所有由于等待获取结果而被阻塞的线程（LockSupport.unpark()），并从waiters链表中移除它们。\n get  在其他线程调用FutureTask的get()方法来等待获取结果，get()方法会判断任务状态是否小于等于COMPLETING，是则阻塞线程循环等待任务完成，加入waiters链表，调用LockSupport.park()挂起线程，如果任务状态为COMPLETING（正在执行），调用Thread.yield()进行线程让步。当线程被激活时，会去获取outcome变量拿到结果。\n总结 FutureTask虽然提供了用来检查任务是否执行完成、等待任务执行结果、获取任务执行结果的方法，但是它并不能清楚地表达多个FutureTask之间的关系。而且从Future获取结果需要调用get()方法，该方法会在任务执行完毕前阻塞调用线程。\nJDK的CompletableFuture CompletableFuture是一个可以通过编程方式显式地设置计算结果和状态以便让任务结束的Future，并且其可以作为一个CompletionStage（计算阶段），当它的计算完成时可以触发一个函数或者行为；当多个线程企图调用同一个CompletableFuture的complete、cancel方式时只有一个线程会成功。\nCompletableFuture所有异步的方法在没有显式指定Executor参数的情形下都是复ForkJoinPool的commonPool()线程池来执行。\nCompletableFuture基于栈收集任务，所以在同一个CompletableFuture对象上行为注册的顺序与行为执行的顺序是相反的。\n显式设置CompletableFuture结果 通过编程显式设置结果的future（complete），阻塞获取结果（get）。\nCompletableFuture\u0026lt;String\u0026gt; future = new CompletableFuture\u0026lt;\u0026gt;(); POOL_EXECUTOR.execute(()-\u0026gt;{ String result = doSomeThing(\u0026#34;task1\u0026#34;); //显式设置  future.complete(result); }); //阻塞获取 log.info(\u0026#34;result:{}\u0026#34;,future.get()); 异步计算与结果转换 方法命名规律：以run为例子，不以Async结尾的方法由原来的线程计算，以Async结尾的方法由默认的线程池ForkJoinPool.commonPool()或者指定的线程池executor运行。\nrunAsync：无返回值的异步计算\nstatic CompletableFuture\u0026lt;Void\u0026gt; runAsync(Runnable runnable,Executor executor) supplyAsync：带有返回值的异步计算，可以通过get方法获取返回值\nstatic \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; supplyAsync(Supplier\u0026lt;U\u0026gt; supplier,Executor executor) thenRunAsync：执行完成任务后，激活其他任务（其他任务拿不到之前任务的返回值）\nCompletableFuture\u0026lt;Void\u0026gt; thenRunAsync(Runnable action,Executor executor) thenAcceptAsync：执行完成任务后，激活其他任务（其他任务可以拿到之前任务的返回值）\nCompletableFuture\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action,Executor executor) \u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenApplyAsync(Function\u0026lt;? super T,? extends U\u0026gt; fn, Executor executor) //example  CompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; doSomeThing(\u0026#34;task from thenRun1\u0026#34;),POOL_EXECUTOR); CompletableFuture\u0026lt;Void\u0026gt; thenRunFuture = future.thenRunAsync(() -\u0026gt; doSomeThing(\u0026#34;task from thenRun2\u0026#34;),POOL_EXECUTOR); whenCompleteAsync：设置回调函数，通过回调的方式，不会阻塞调用线程\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; doSomeThing(\u0026#34;task from supplyAsync\u0026#34;), POOL_EXECUTOR); future.whenCompleteAsync((s, throwable) -\u0026gt; { if (Objects.nonNull(s)){ log.info(\u0026#34;result:{}\u0026#34;,s); } },POOL_EXECUTOR); 多CompletableFuture组合运算 thenCompose：当一个CompletableFuture执行完毕后，执行另外一个CompletableFuture\n\u0026lt;U\u0026gt; CompletableFuture\u0026lt;U\u0026gt; thenComposeAsync( Function\u0026lt;? super T, ? extends CompletionStage\u0026lt;U\u0026gt;\u0026gt; fn) thenCombine：当两个并发运行的CompletableFuture任务都完成后，使用两者的结果作为参数再执行一个异步任务\n\u0026lt;U,V\u0026gt; CompletableFuture\u0026lt;V\u0026gt; thenCombineAsync( CompletionStage\u0026lt;? extends U\u0026gt; other, BiFunction\u0026lt;? super T,? super U,? extends V\u0026gt; fn) //example  CompletableFuture.supplyAsync(() -\u0026gt; doSomeThing(\u0026#34;task from supplyAsync1\u0026#34;), POOL_EXECUTOR) .thenCombineAsync(CompletableFuture .supplyAsync(() -\u0026gt; doSomeThing(\u0026#34;task from supplyAsync2\u0026#34;), POOL_EXECUTOR) , (result1, result2) -\u0026gt; result1 + result2) .whenCompleteAsync((s, throwable) -\u0026gt; { if (Objects.nonNull(s)) { log.info(\u0026#34;result:{}\u0026#34;, s); } }, POOL_EXECUTOR); allOf：等待多个并发运行的CompletableFuture任务执行完毕\nstatic CompletableFuture\u0026lt;Void\u0026gt; allOf(CompletableFuture\u0026lt;?\u0026gt;... cfs) //example ArrayList\u0026lt;CompletableFuture\u0026gt; futures = new ArrayList\u0026lt;\u0026gt;(); futures.add(CompletableFuture.supplyAsync(() -\u0026gt; doSomeThing(\u0026#34;task from supplyAsync1\u0026#34;), POOL_EXECUTOR)); futures.add(CompletableFuture.supplyAsync(() -\u0026gt; doSomeThing(\u0026#34;task from supplyAsync2\u0026#34;), POOL_EXECUTOR)); futures.add(CompletableFuture.supplyAsync(() -\u0026gt; doSomeThing(\u0026#34;task from supplyAsync3\u0026#34;), POOL_EXECUTOR)); CompletableFuture.allOf(futures.toArray(new CompletableFuture[futures.size()])).get(); anyOf：等多个并发运行的CompletableFuture任务任意一个执行完毕\nstatic CompletableFuture\u0026lt;Object\u0026gt; anyOf(CompletableFuture\u0026lt;?\u0026gt;... cfs) 异常处理 当出现异常时，可以调用future.completeExceptionally(e) 把异常信息设置到future内部，get获取时会把异常带出来，或者future.exceptionally设置出现异常时返回的默认值。\nStream与CompletableFuture 可以使用map将一些数据传进CompletableFuture进行异步调用，并转换成CompletableFuture。\nList\u0026lt;String\u0026gt; requests = Arrays.asList(\u0026#34;request1\u0026#34;, \u0026#34;request2\u0026#34;, \u0026#34;request3\u0026#34;); //转换成CompletableFuture，异步调用  List\u0026lt;CompletableFuture\u0026lt;String\u0026gt;\u0026gt; futures = requests.stream() .map(request -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; getRpcResult(request), POOL_EXECUTOR)) .collect(Collectors.toList()); //同步阻塞等待调用完毕，收集结果  List\u0026lt;String\u0026gt; rpcResponse = futures.stream() .map(CompletableFuture::join) .collect(Collectors.toList()); log.info(\u0026#34;rpcResponse:{}\u0026#34;, rpcResponse); RxJava与Reactor 响应式编程（Reactive Programming）是一种涉及数据流和变化传播的异步编程范式，可以通过所采用的编程语言轻松地表达静态（例如阵列）或动态（例如事件发射器）数据流。RxJava与Reactor都是对响应式编程理念实现，基于Reactive Streams标准，遵循了同一个规范，可以很轻易地从一方切换到另一方 。\n对比Java的异步编程模型\nJava提供了两种异步编程模型\nCallBacks：异步方法没有返回值，绑定回调函数在结果可用时调用。\nFutures：异步方法立即返回Future，异步线程计算任务，并当结果计算出来后设置到Future。\nCallBacks与Futures局限性：\n多个Callback组合在一起后，容易形成回调地狱（Callback Hell）；多个Future难以编排，尽管是改进后的CompletableFuture，也存在不支持延迟计算和高级错误处理的缺陷。\n响应式编程库基于声明式编程，比较Callbacks更通俗易懂，同时自带很多操作符，提供高级特性，简化处理。\n参考对比：Reactor 3 参考文档\n响应式编程库通过如下几点弥补Java异步编程模型的不足\n 可编排性（Composability） 以及 可读性（Readability） 使用丰富的 操作符 来处理形如 流 的数据 在 订阅（subscribe） 之前什么都不会发生 背压（backpressure） 具体来说即消费者能够反向告知生产者生产内容的速度的能力 高层次 （同时也是有高价值的）的抽象，从而达到 并发无关 的效果  基于RxJava实现异步编程  observeOn（切换调度线程执行订阅者函数）  注意，observeOn切换到了其他线程异步执行，但是事件还是按发送顺序同步执行。\nFlowable.fromArray(requests.toArray(new String[0])) //切换到IO调度线程执行订阅者函数  .observeOn(Schedulers.io()) //异步调用，按发布顺序顺序执行  .map(this::getRpcResponse) .subscribe(response-\u0026gt;log.info(\u0026#34;get Rpc Response:{}\u0026#34;,response));  subscribeOn（切换到调度线程执行发布者函数）  Flowable.fromCallable(this::generateRequests) //切换到IO调度线程执行发布者函数  .subscribeOn(Schedulers.io()) //切换到Signle调度线程执行订阅者函数  .observeOn(Schedulers.single()) .subscribe(System.out::print);  flatMap与subscribeOn进行并发调用  针对上述observeOn例子顺序执行的情况，要做到并发执行，我们可以通过flatMap操作符配合subscribeOn进行操作。如下所示，使用flatMap将Request参数转换为Flowable，利用subscribeOn切换到调度线程执行，做到了并发调用。\nFlowable.fromArray(generateRequests()) .flatMap(requests -\u0026gt; //flatMap将Request转换为Flowable对象  Flowable.just(requests) //切换到调度线程执行发布者函数  .subscribeOn(Schedulers.io()) //执行rpc调用转换成Response  .map(this::getRpcResponse)) //阻塞等待所有的rpc调用并发执行完毕  .blockingSubscribe(System.out::print); 基于Reactor实现异步编程 Reactor中的流操作符与RxJava基本相同，下面简单重写了上述RxJava的例子\nFlux.fromArray(generateRequests()) .flatMap(requests -\u0026gt; //flatMap将Request转换为Flowable对象  Flux.just(requests) //切换到调度线程执行发布者函数  .subscribeOn(Schedulers.elastic()) //执行rpc调用转换成Response  .map(this::getRpcResponse)) .subscribe(response -\u0026gt; { log.info(\u0026#34;get rpc response :{}\u0026#34;, response); }); Flux.fromArray(generateRequests()) //切换到IO调度线程执行订阅者函数  .publishOn(Schedulers.elastic()) .map(this::getRpcResponse) .subscribe(response -\u0026gt; log.info(\u0026#34;get Rpc Response:{}\u0026#34;, response)); Spring的TaskExecutor Spring 2.0版本开始提供了一种新的处理执行器（executors）的抽象，TaskExecutor。\npublic interface TaskExecutor { void execute(Runnable task); } Spring内置TaskExecutor实现\n SimpleAsyncTaskExecutor（每个请求会新创建一个对应的线程来执行） SyncTaskExecutor（同步使用调用线程来执行任务） ConcurrentTaskExecutor（对JDK5中的java.util.concurrent.Executor的一个包装，通过setConcurrentExecutor设置一个JUC中的线程池到其内部来做适配） SimpleThreadPoolTaskExecutor（Quartz的SimpleThreadPool的子类，会监听Spring的生命周期回调） ThreadPoolTaskExecutor（比较常用，用于配置java.util.concurrent.ThreadPoolExecutor并将其包装在TaskExecutor中。如果需要一些高级的接口，例如ScheduledThreadPoolExecutor，可以使用Concurrent TaskExecutor） TimerTaskExecutor（对所有提交的任务都在Timer内的单独线程中执行）  在SpringBoot中使用TaskExecutor进行异步处理  配置Executor参数  @Slf4j @Configuration public class AsyncTaskConfig implements AsyncConfigurer { @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor threadPoolTaskExecutor = new ThreadPoolTaskExecutor(); threadPoolTaskExecutor.setThreadNamePrefix(\u0026#34;AsyncThread-\u0026#34;); threadPoolTaskExecutor.setCorePoolSize(100); threadPoolTaskExecutor.setMaxPoolSize(100); threadPoolTaskExecutor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { // 打印线程池异常信息...  super.rejectedExecution(r, e); } }); //关闭执行器时不等待正在执行的任务执行完毕就中断执行任务的线程  threadPoolTaskExecutor.setWaitForTasksToCompleteOnShutdown(true); threadPoolTaskExecutor.afterPropertiesSet(); return threadPoolTaskExecutor; } //拒绝策略  @Override public AsyncUncaughtExceptionHandler getAsyncUncaughtExceptionHandler() { return new SimpleAsyncUncaughtExceptionHandler(); } } 在SpringBootApplication类上添加@EnableAsync注解。 在方法上添加@Async，异步执行该方法，该方法的实际执行将发生在Spring的TaskExecutor异步处理器线程中。基于@Async注解的异步处理是支持返回值的，但是返回值类型必须是Future或者其子类类型的，如JDK的Future类型，Spring框架的ListenableFuture类型，或者JDK8中的 CompletableFuture类型，又或者Spring中的AsyncResult类型等。  @Async注解执行原理 @EnableAsync开启后会把ProxyAsyncConfiguration的实例注入Spring容器。默认情况下，Spring框架是使用Cglib对标注@Async注解的方法进行代理的，具体拦截器是AnnotationAsyncExecutionInterceptor作为切面逻辑，当我们调用含有@Async注解的Bean的方法时候，实际调用的是被代理后的Bean，AnnotationAsyncExecutionInterceptor的invoke方法如下：\npublic Object invoke(final MethodInvocation invocation) throws Throwable { //1．被代理的目标对象  Class\u0026lt;? \u0026gt; targetClass = (invocation.getThis() ! = null ? AopUtils. getTargetClass(invocation.getThis()) : null); //2. 获取被代理的方法  Method specificMethod = ClassUtils.getMostSpecificMethod(invocation. getMethod(), targetClass); final Method userDeclaredMethod = BridgeMethodResolver.findBridgedMethod( specificMethod); //3. 判断使用哪个执行器执行被代理的方法  AsyncTaskExecutor executor = determineAsyncExecutor(userDeclaredMethod); if (executor == null) { throw new IllegalStateException( \u0026#34;No executor specified and no default executor set on AsyncExecutionInterceptor either\u0026#34;); } //4. 使用Callable包装要执行的方法  Callable\u0026lt;Object\u0026gt; task = () -\u0026gt; { try { Object result = invocation.proceed(); if (result instanceof Future) { return ((Future\u0026lt;? \u0026gt;) result).get(); } } catch (ExecutionException ex) { handleError(ex.getCause(), userDeclaredMethod, invocation. getArguments()); } catch (Throwable ex) { handleError(ex, userDeclaredMethod, invocation.getArguments()); } return null; }; //5. 提交包装的Callable任务到指定执行器执行  return doSubmit(task, executor, invocation.getMethod().getReturnType()); } doSubmit会判断方法的返回值类型进行包装，然后提交到线程池中运行。\nprotected Object doSubmit(Callable\u0026lt;Object\u0026gt; task, AsyncTaskExecutor executor, Class\u0026lt;? \u0026gt; returnType) { //5.1判断方法返回值是否为CompletableFuture类型或者是其子类  if (CompletableFuture.class.isAssignableFrom(returnType)) { return CompletableFuture.supplyAsync(() -\u0026gt; { try { return task.call(); } catch (Throwable ex) { throw new CompletionException(ex); } }, executor); } //5.2判断返回值类型是否为ListenableFuture类型或者是其子类  else if (ListenableFuture.class.isAssignableFrom(returnType)) { return ((AsyncListenableTaskExecutor) executor). submitListenable(task); } //5.3判断返回值类型是否为ListenableFuture类型或者是其子类  else if (Future.class.isAssignableFrom(returnType)) { return executor.submit(task); } //5.4其他情况下没有返回值  else { executor.submit(task); return null; } } Servlet异步编程 Servlet3.0规范前，Servlet容器的线程模型如下：\n每个请求对应一个线程这种1 : 1的模式进行处理的同步线程模型，线程数是有限的，当线程池资源耗尽后就不能接收处理新的请求了，限制了服务器的并发请求数。\nServlet3.0 提供的异步处理 Servlet 3.0规范中引入了异步处理请求的能力，相对3.0之前的同步线程模型，Servlet内开启异步处理后会立刻释放Servlet容器线程，具体对请求进行处理与响应的是业务线程池中的线程。\n官方例子：\n@WebServlet(urlPatterns={\u0026#34;/asyncservlet\u0026#34;}, asyncSupported=true) public class AsyncServlet extends HttpServlet { /* ... Same variables and init method as in SyncServlet ... */ @Override public void doGet(HttpServletRequest request, HttpServletResponse response) { response.setContentType(\u0026#34;text/html;charset=UTF-8\u0026#34;); final AsyncContext acontext = request.startAsync(); acontext.start(new Runnable() { public void run() { String param = acontext.getRequest().getParameter(\u0026#34;param\u0026#34;); String result = resource.process(param); HttpServletResponse response = acontext.getResponse(); /* ... print to the response ... */ acontext.complete(); } }); } } Servlet3.1 提供的非阻塞IO处理 Servlet 3.0规范让Servlet的执行变为了异步，但是其IO还是阻塞式的（从ServletInputStream中读取请求体时是阻塞的）。\n在Servlet3.1规范中提供了非阻塞IO处理方式，（当内核支持）Servlet3.1允许我们在ServletInputStream上通过函数setReadListener注册一个监听器，该监听器在发现内核有数据时才会进行回调处理函数。\n示例：\nfinal AsyncContext asyncContext = req.startAsync(); //设置数据就绪监听器  final ServletInputStream inputStream = req.getInputStream(); inputStream.setReadListener(new ReadListener() { @Override public void onError(Throwable throwable) { //异常处理  } @Override public void onDataAvailable() throws IOException { //数据就绪时回调，获取数据流  final ServletInputStream inputStream = asyncContext. getRequest().getInputStream(); } @Override public void onAllDataRead() throws IOException { //请求体的数据全部被读取完毕后，进行业务处理  } } Netty Netty是一个异步、基于事件驱动的网络应用程序框架，其对Java NIO进行了封装，简化了TCP或者UDP服务器的网络编程开发。\nNetty框架将网络编程逻辑与业务逻辑处理分离开来，其内部会自动处理好网络与异步处理逻辑，使用者只需要关注逻辑处理。Netty的异步非阻塞能力与CompletableFuture结合可以让我们轻松实现网络请求的异步调用。\n很多现代化，高性能的Web框架，RPC框架底层都是用来Netty来实现，例如WebFlux，Vert.x，Dubbo，RocketMq\u0026hellip;。\n线程模型 以Netty Server端为例子，NettyServer启动时会创建两个NioEventLoop Group线程池组，其中boss组用来接收客户端发来的连接，worker组则负责对完成TCP三次握手的连接进行处理；图中每个NioEventLoopGroup里面包含了多个Nio EventLoop，每个NioEventLoop中包含了一个NIO Selector、一个队列、一个线程；其中线程用来做轮询注册到Selector上的Channel的读写事件和对投递到队列里面的事件进行处理。\n当客户端发来一个连接请求时，boss线程池组中注册了监听套接字的NioEventLoop中的Selector会读取TCP三次握手的请求，然后创建对应的连接套接字通道NioSocketChannel，接着把其注册到worker线程池组的某一个NioEventLoop中管理的一个NIO Selector上，该连接套接字通道NioSocketChannel上的所有读写事件都由该NioEventLoop管理。\n 非阻塞write  NioSocketChannel的write系列方法向连接里面写入数据时是非阻塞的，具体实现是执行write方法时判断是否是IO线程调用，不是则把写入请求封装为WriteTask并投递到与其对应的NioEventLoop中的队列里面。\nprivate void write(Object msg, boolean flush, ChannelPromise promise) { ... //1．如果调用线程是IO线程,直接执行  EventExecutor executor = next.executor(); if (executor.inEventLoop()) { if (flush) { next.invokeWriteAndFlush(m, promise); } else { next.invokeWrite(m, promise); } } else {//2．如果调用线程不是IO线程，封装为WriteTask投递到对应的NioEventLoop中的队列，NioEventLoop的线程轮询队列处理  AbstractWriteTask task; if (flush) { task = WriteAndFlushTask.newInstance(next, m, promise); } else { task = WriteTask.newInstance(next, m, promise); } safeExecute(executor, task, promise, m); } }  非阻塞read  NioSocketChannel中读取数据时，等NioEventLoop中的IO轮询线程发现Selector上有数据就绪时，通过事件通知方式来通知我们业务数据已经就绪，是非阻塞的。\n"
            }
    
        ,
            {
                "id": 14,
                "href": "https://chinalhr.github.io/post/uid-generator-scheme/",
                "title": "分布式ID解决方案分析",
                "section": "post",
                "date" : "2020.03.29",
                "body": " 分布式ID解决方案分析，百度uid-generator，美团Leaf原理分析\n 为什么需要分布式ID 在一些复杂的分布式系统中，往往需要对大量的数据和消息进行唯一标识。主要考虑两点：\n 为了后续扩展，数据库分库分表后需要有一个唯一ID来标识一条数据或消息，自增ID并不能满足分库分表的需求 如一些金融、支付、电商、物流等产品的系统中都需要有唯一ID做标识，如订单号、用户标识、优惠券号。因此需要一个能够生成全局唯一ID的分布式ID生成系统  分布式ID特性 基本特性  全局唯一性：不能出现重复的ID号 趋势递增，由于多数RDBMS使用B-tree的数据结构来存储索引数据，在主键的选择上面我们应该尽量使用有序的主键保证写入性能。 单调递增：保证下一个ID一定大于上一个ID，可以满足排序、IM增量消息、事务版本号等特殊需求。 信息安全：如果ID是连续的，很容易被恶意用户进行数据扒取与推算数据，所以要求ID尽量不规则。 可逆性：分布式id里面最好包含时间戳/业务标识且可逆分析，利于排查问题。  上述3和4特性是互斥的，但是可以进行取舍，例如基于SnowFlake算法的ID生成是单调递增的，但是根据获取的场景不是连续的。\n设计要求  高性能：高可用低延时，ID要快速生成响应，避免成为业务瓶颈 高可用：一般很多关键的核心服务对分布式ID生成服务有强依赖，需要保证高可用避免影响系统正常运行。 易于接入：设计和实现上要尽可能的简单，做到拿来即用的设计。  分析几种分布式ID生成方案 简单方案[UUID/数据库自增ID/Redis incr] UUID  什么是UUID  UUID是由一组32位数的16进制数字所构成，UUID理论上的总数为16^32=2^128，若每纳秒(ns)产生1兆个UUID，要花100亿年才会将所有UUID用完。\n UUID格式  UUID 的 16 个 8 位字节表示为 32 个十六进制（基数16）数字，显示在由连字符分隔 \u0026lsquo;-\u0026rsquo; 的五个组中，\u0026ldquo;8-4-4-4-12\u0026rdquo; 总共 36 个字符（32 个字母数字字符和 4 个连字符）\nxxxxxxxx-xxxx-Mxxx-Nxxx-xxxxxxxxxxxx 例如：123e4567-e89b-12d3-a456-426655440000\n其中四位数字 M表示 UUID 版本，数字 N的一至三个最高有效位表示 UUID 变体，布局如下：\n   Name Length (bytes) Length (hex digits) Contents     time_low 4 8 整数：低位 32 bits 时间   time_mid 2 4 整数：中间位 16 bits 时间   time_hi_and_version 2 4 最高有效位中的 4 bits“版本”，后面是高 12 bits 的时间   clock_seq_hi_and_res clock_seq_low 2 4 最高有效位为 1-3 bits“变体”，后跟13-15 bits 时钟序列   node 6 12 48 bits 节点 ID     版本  版本1 UUID 是根据时间和节点 ID（通常是MAC地址）生成 版本2 UUID是根据标识符（通常是组或用户ID）、时间和节点ID生成 版本3 和 版本5 确定性UUID 通过散列 (hashing) 名字空间 (namespace) 标识符和名称生成 版本4 UUID 使用随机性或伪随机性生成   碰撞  对于版本一，二使用MAC地址生成的UUID，版本三四五产生碰撞的可能想更大，但通常可能是忽略。在 103 万亿 个 版本4 UUID中找到重复概率是十亿分之一。\n 方案分析  使用UUID做分布式ID是可行的，有点是生成方式简单，快速，但是并不推荐使用，因为使用UUID生成的是无意义的字符串，而且非趋势递增，用作主键存储性能差。\n 参考：https://zh.wikipedia.org/wiki/通用唯一识别码  基于数据库自增ID 基于关系型数据库如MySql的主键自增（AUTO_INCREMENT）特性。建立一张专门的ID生成的表，需要获取ID就在表中插入一条记录获取自增的ID，也可以批量插入获取。\n 方案分析  使用数据库自增ID的方式实现简单，但是生成的是递增连续的ID的信息不安全，而且不适合作为订单号等业务的主键；并且对数据库有强依赖关系，难以扩展有宕机风险。\n基于Redis的incr命令 基于Redis的incr命令实现ID的原子自增，先使用set 命令设置初始值，每次获取ID的时候使用incr命令获取自增ID。\n 方案分析  与基于数据库自增ID存在相同的问题，利用Redis内存数据库与单线程模型的特性有更高的并发性，但需要注意数据持久化的问题。基于RDB的定时快照方式重启后可能会出现ID重复问题，基于AOF的命令记录方式又存在重启恢复时间长的问题。\n基于数据库集群模式 基于数据库集群的方式是对基于自增ID方式的高可用优化。对数据库做多主模式集群。\n实现方式  设置自增ID起始值与步长  如果是双主模式集群，那么需要设置自增ID的步长为2，可以通过如下命令设置自增ID的起始值与步长。\nset @@auto_increment_offset = 1; -- 起始值 set @@auto_increment_increment = 2; -- 步长 第一台MySql的起始值为1，步长为2的话第二台的起始值为2，以此类推。\n 后续扩容问题  如果后续需要增加MySql节点，需要修改一、二两台MySQL实例的ID起始值和步长，并把第三台机器的ID起始生成位置设定在比现有最大自增ID大。\n方案分析 虽然解决了基于数据库自增ID方式的高可用性问题，但是在出现性能瓶颈后需要做繁琐的扩容处理，操作不当可能出现ID重复的风险。而且也存在着数据库自增ID方式的其他问题。\n基于数据库号段模式 基于号段的方式旨在通过批量的获取自增ID的方式，降低对数据库的压力，每次从数据库取出一个号段范围。\n实现方式  表结构设计  基于号段的方式获取ID在表设计上有多种选择，但核心在于需要记录当前最大的可用ID。\nCREATE TABLE id_generator ( id int(10) NOT NULL, max_id bigint(20) NOT NULL COMMENT \u0026#39;当前最大的可用id\u0026#39;, step int(20) NOT NULL COMMENT \u0026#39;号段的长度\u0026#39;, biz_id\tint(20) NOT NULL COMMENT \u0026#39;业务id\u0026#39;, version int(20) NOT NULL COMMENT \u0026#39;版本号\u0026#39;, PRIMARY KEY (`id`) )  ID批量获取  某个业务需要批量获取ID可用通过update语句，设置biz_id所在的max_id=max_id+step，获取到号段(max_id,max_id+step]，例如max_id=1000,step=2000,那么获取到的号段为(1000,3000]，获取后当前最大max_id=3000。\nupdate id_generator set max_id = max_id+step, version = version + 1 where version = version and biz_id = *** 方案分析 通过批量获取ID的思路降低了对数据库的压力与强依赖关系，依据这个思路可以作为一些特定业务如用户短ID生成的方案。\n基于雪花算法的ID生成 雪花算法（Snowflake）是twitter内部使用的分布式ID生成算法，旨在依据一定的算法逻辑，生成一个Long类型的ID（64bit）。组成结构大致如下：\n正数位（1bit）+时间戳（41bit）+机器ID（5bit）+ 数据中心（5bit）+ 自增ID（12bit）\n具体实现方式可参考博主之前的一篇文章：https://chinalhr.github.io/post/uidgenerate_snowflake\n方案分析 基于雪花算法的ID生成方式简单，趋势递增且带有可逆性，用作主键存储性能高，也支持后续数据库分库分表，算法实现方式灵活性高可自定义组合优化，高性能每秒能够产生26万个ID，并且不依赖于数据库，以服务的方式部署，稳定性更高。\n但是因为强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态，而且雪花算法通过提前规划好机器标识来实现，但在分布式生产环境中是不够友好的，一般需要自动启停，增减机器，因此很多互联网大厂都对改算法进行了一定的改造。\n分布式ID开源项目分析-百度uid-generator项目 UidGenerator是由百度技术部开发的，通过Java实现, 基于Snowflake算法的唯一ID生成器。UidGenerator以组件形式工作在应用项目中,支持自定义workerId位数和初始化策略, 从而适用于docker等虚拟化环境下实例自动重启、漂移等场景。\nUidGenerator项目地址：https://github.com/baidu/uid-generator\n数据结构 UidGenerator生成的ID默认结构如下：\n **sign(1bit) **：固定1bit符号标识，即生成的UID为正数。 **delta seconds (28 bits) **：当前时间，相对于时间基点\u0026quot;2016-05-20\u0026quot;的增量值，单位：秒，最多可支持约8.7年 worker id (22 bits)： 机器id，最多可支持约420w次机器启动。内置实现为在启动时由数据库分配，默认分配策略为用后即弃，后续可提供复用策略。 sequence (13 bits)： 每秒下的并发序列，13 bits可支持每秒8192个并发。  项目结构 项目目录：\n ├── BitsAllocator.java\t- Bit分配器(C) ├── UidGenerator.java\t- UID生成器接口(I) ├── buffer │ ├── BufferPaddingExecutor.java\t- 填充RingBuffer的执行器(C) │ ├── BufferedUidProvider.java\t- RingBuffer中UID的生产者(C) │ ├── RejectedPutBufferHandler.java\t- 拒绝Put到RingBuffer的处理器(C) │ ├── RejectedTakeBufferHandler.java\t- 拒绝从RingBuffer中Take的处理器(C) │ └── RingBuffer.java\t- RingBuffer数据结构实现类(C) ├── exception\t- 相关的自定义异常类 ├── impl │ ├── CachedUidGenerator.java\t- 基于RingBuffer存储的UID生成器实现类(C) │ └── DefaultUidGenerator.java\t- 默认UID生成器实现类(C) ├── utils - 工具类（日期，网络，线程池，枚举） └── worker\t├── DisposableWorkerIdAssigner.java\t- 一次性的WorkerId分配器(C) ├── WorkerIdAssigner.java\t- WorkerId分配器接口(I) ├── WorkerNodeType.java\t- 工作节点类型，分为容器与实体两种(E) ├── dao │ └── WorkerNodeDAO.java\t- WorkerNode数据库访问接口 └── entity └── WorkerNodeEntity.java\t- WorkerNode 实体类 项目依赖：\n主要依赖了springframework的核心包，mybatis，Apache commons工具类，logback\n表结构：\nCREATE TABLE WORKER_NODE( ID BIGINT NOT NULL AUTO_INCREMENT COMMENT \u0026#39;auto increment id\u0026#39;, HOST_NAME VARCHAR(64) NOT NULL COMMENT \u0026#39;host name\u0026#39;, PORT VARCHAR(64) NOT NULL COMMENT \u0026#39;port\u0026#39;, TYPE INT NOT NULL COMMENT \u0026#39;node type: ACTUAL or CONTAINER\u0026#39;, LAUNCH_DATE DATE NOT NULL COMMENT \u0026#39;launch date\u0026#39;, MODIFIED TIMESTAMP NOT NULL COMMENT \u0026#39;modified time\u0026#39;, CREATED TIMESTAMP NOT NULL COMMENT \u0026#39;created time\u0026#39;, PRIMARY KEY(ID) ) COMMENT=\u0026#39;DB WorkerID Assigner for UID Generator\u0026#39;,ENGINE = INNODB; 从上面的项目目录来看，UidGenerator提供了两种实现方式，分别是实时ID生成方式的DefaultUidGenerator与预生成ID生成方式的CachedUidGenerator。\nDefaultUidGenerator实现方式 UidGenerator接口如下\npublic interface UidGenerator { /** * 获取UID */ long getUID() throws UidGenerateException; /** * 解析格式化输出UID */ String parseUID(long uid); } DefaultUidGenerator实现了UidGenerator与InitializingBean，具体实现流程如下：\n Bean初始化，配置的timeBits，workerBits，seqBits位数与epochStr日期，因为最多可支持约8.7年，所以最好设置epochStr值为合适的日期。 afterPropertiesSet方法初始化BitsAllocator设置timeBits，workerBits，seqBits等位数信息，初始化workId（每次初始化往WORKER_NODE表插入一条记录，返回的主键ID就是workId，所以最多可支持约420w次机器启动）。 核心方法nextId获取UID，具体方式如下  protected synchronized long nextId() { //获取当前时间，单位秒，会校验是否超过最大的时间年限，超过则抛出异常  long currentSecond = getCurrentSecond(); // 如果出现时间回拨，就抛出异常  if (currentSecond \u0026lt; lastSecond) { long refusedSeconds = lastSecond - currentSecond; throw new UidGenerateException(\u0026#34;Clock moved backwards. Refusing for %d seconds\u0026#34;, refusedSeconds); } // 同一秒内序列号自增，  if (currentSecond == lastSecond) { sequence = (sequence + 1) \u0026amp; bitsAllocator.getMaxSequence(); //超过最大序列，等待下一秒生成uid  if (sequence == 0) { currentSecond = getNextSecond(lastSecond); } // 不同秒内序列号从0开始  } else { sequence = 0L; } lastSecond = currentSecond; // bits位移操作生成UID  return bitsAllocator.allocate(currentSecond - epochSeconds, workerId, sequence); } 总结：DefaultUidGenerator的实现与雪花算法实现对比改动较小，对生成ID结构的位数可自定义，对分布式环境下增减机器的WorkId分配更友好，但没有解决时间回拨问题，采用了简单的抛出异常的方式处理。\nRingBuffer RingBuffer本质上是一个数组，数组中每个项被称为slot。UidGenerator设计了两个RingBuffer，一个保存唯一ID，一个保存flag。RingBuffer的尺寸是2^n。\nRingBuffer Of UID RingBuffer Of UID是存储UID的RingBuffer 使用两个指针，Tail指针表示最新生成的UID，如果这个Tail指针追上了Cursor指针，此时RingBuffer已经满了，不允许再继续生成UID了。可以通过属性rejectedPutBufferHandler指定拒绝策略。\nCursor指针表示最后一个已经给消费的UID，如果Cursor指针追上了Tail指针，此时RingBuffer已经空了，不允许继续获取UID了。可以通过rejectedTakeBufferHandler指定拒绝策略。\nRingBuffer Of Flag RingBuffer Of Flag是存储Uid状态的RingBuffer，每个slot的值都是0或者1，0是CAN_PUT_FLAG（可以填充）的标志位，1是CAN_TAKE_FLAG（可以消费）的标识位。\n配置参数 \u0026lt;!-- 以下为可选配置, 如未指定将采用默认值 --\u0026gt; \u0026lt;!-- RingBuffer size扩容参数, 可提高UID生成的吞吐量. --\u0026gt; \u0026lt;!-- 默认:3， 原bufferSize=8192, 扩容后bufferSize= 8192 \u0026lt;\u0026lt; 3 = 65536 --\u0026gt; \u0026lt;property name=\u0026#34;boostPower\u0026#34; value=\u0026#34;3\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;!-- 指定何时向RingBuffer中填充UID, 取值为百分比(0, 100), 默认为50 --\u0026gt; \u0026lt;!-- 举例: bufferSize=1024, paddingFactor=50 -\u0026gt; threshold=1024 * 50 / 100 = 512. --\u0026gt; \u0026lt;!-- 当环上可用UID数量 \u0026lt; 512时, 将自动对RingBuffer进行填充补全 --\u0026gt; \u0026lt;property name=\u0026#34;paddingFactor\u0026#34; value=\u0026#34;50\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;!-- 另外一种RingBuffer填充时机, 在Schedule线程中, 周期性检查填充 --\u0026gt; \u0026lt;!-- 默认:不配置此项, 即不使用Schedule线程. 如需使用, 请指定Schedule线程时间间隔, 单位:秒 --\u0026gt; \u0026lt;property name=\u0026#34;scheduleInterval\u0026#34; value=\u0026#34;60\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;!-- 拒绝策略: 当环已满, 无法继续填充时 --\u0026gt; \u0026lt;!-- 默认无需指定, 将丢弃Put操作, 仅日志记录. 如有特殊需求, 请实现RejectedPutBufferHandler接口(支持Lambda表达式) --\u0026gt; \u0026lt;property name=\u0026#34;rejectedPutBufferHandler\u0026#34; ref=\u0026#34;XxxxYourPutRejectPolicy\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; \u0026lt;!-- 拒绝策略: 当环已空, 无法继续获取时 --\u0026gt; \u0026lt;!-- 默认无需指定, 将记录日志, 并抛出UidGenerateException异常. 如有特殊需求, 请实现RejectedTakeBufferHandler接口(支持Lambda表达式) --\u0026gt; \u0026lt;property name=\u0026#34;rejectedTakeBufferHandler\u0026#34; ref=\u0026#34;XxxxYourTakeRejectPolicy\u0026#34;\u0026gt;\u0026lt;/property\u0026gt; CacheLine补齐 RingBuffer的数据都是使用数组来存储的，在内存中是连续分配的，可最大程度利用CPU cache以提升性能。但同时会带来「伪共享」FalseSharing问题。tail和cursor变量如果直接用原生的AtomicLong类型，tail和cursor可能会缓存在同一个cacheLine中，多个线程读取该变量可能会引发CacheLine的RFO请求，反而影响性能,为了防止伪共享问题，填充了6个long类型的成员变量，加上long类型的value成员变量，刚好占满一个Cache Line（其中Java对象还有8byte的对象头），这种方式称之为CacheLine补齐。项目中使用了PaddedAtomicLong进行包装。\npublic class PaddedAtomicLong extends AtomicLong { private static final long serialVersionUID = -3415778863941386253L; /** Padded 6 long (48 bytes) */ public volatile long p1, p2, p3, p4, p5, p6 = 7L; /** * Constructors from {@link AtomicLong} */ public PaddedAtomicLong() { super(); } public PaddedAtomicLong(long initialValue) { super(initialValue); } /** * To prevent GC optimizations for cleaning unused padded references */ public long sumPaddingToPreventOptimization() { return p1 + p2 + p3 + p4 + p5 + p6; } }  RingBuffer填充时机   初始化预填充：初始化时预先填充满 即时填充：take消费，即时检查剩余可用slot量是否小于设定阈值，小于则进行填充 周期填充：通过Schedule线程，定时补全空闲slots。  CachedUidGenerator实现方式 CachedUidGenerator是基于RingBuffer去实现的。\nCachedUidGenerator继承自DefaultUidGenerator，实现了DisposableBean。\n初始化  调用父类DefaultUidGenerator的afterPropertiesSet方法初始化BitsAllocator与WorkId。 根据配置的boostPower与paddingFactor初始化RingBuffer，根据配置的scheduleInterval初始化bufferPaddingExecutor，初始化拒绝策略rejectedPutBufferHandler与rejectedTakeBufferHandler。 初始化填满RingBuffer中所有slot（初始化填充UID），此处调用bufferPaddingExecuto的paddingBuffer()方法。 开启Ringbuffer补丁线程（前提是配置了属性scheduleInterval）  RingBuffer.put（填充） CachedUidGenerator生成UID的方式主要是通过paddingBuffer方法实现的，生成流程如下图所示，主要关注点是在满足填充新的唯一ID条件时，通过时间值递增得到新的时间值（lastSecond.incrementAndGet()），而不是System.currentTimeMillis()这种方式，解决了运行时时间回拨问题。\npublic void paddingBuffer() { LOGGER.info(\u0026#34;Ready to padding buffer lastSecond:{}. {}\u0026#34;, lastSecond.get(), ringBuffer); // is still running  if (!running.compareAndSet(false, true)) { LOGGER.info(\u0026#34;Padding buffer is still running. {}\u0026#34;, ringBuffer); return; } boolean isFullRingBuffer = false; while (!isFullRingBuffer) { //调用nextIdsForOneSecond方法获取UID list，注意此处的时间使用lastSecond.incrementAndGet()方法时间自增的方式获取而不是System.currentTimeMillis()获取，解决了运行时时间回拨问题  List\u0026lt;Long\u0026gt; uidList = uidProvider.provide(lastSecond.incrementAndGet()); for (Long uid : uidList) { //调用put方法加入RingBuffer of UID中  isFullRingBuffer = !ringBuffer.put(uid); if (isFullRingBuffer) { break; } } } // not running now  running.compareAndSet(true, false); LOGGER.info(\u0026#34;End to padding buffer lastSecond:{}. {}\u0026#34;, lastSecond.get(), ringBuffer); } /** * 指定秒内获取UID List，Size为max sequence（0~max sequence） * * @param currentSecond * @return UID list, size of {@link BitsAllocator#getMaxSequence()} + 1 */ protected List\u0026lt;Long\u0026gt; nextIdsForOneSecond(long currentSecond) { // Initialize result list size of (max sequence + 1)  int listSize = (int) bitsAllocator.getMaxSequence() + 1; List\u0026lt;Long\u0026gt; uidList = new ArrayList\u0026lt;\u0026gt;(listSize); // Allocate the first sequence of the second, the others can be calculated with the offset  long firstSeqUid = bitsAllocator.allocate(currentSecond - epochSeconds, workerId, 0L); for (int offset = 0; offset \u0026lt; listSize; offset++) { uidList.add(firstSeqUid + offset); } return uidList; } public synchronized boolean put(long uid) { //获取Tail与Cursor指针  long currentTail = tail.get(); long currentCursor = cursor.get(); // 1. 判断Ring是否已满，满了执行Put拒绝策略  long distance = currentTail - (currentCursor == START_POINT ? 0 : currentCursor); if (distance == bufferSize - 1) { rejectedPutHandler.rejectPutBuffer(this, uid); return false; } // 2. check标志是否为CAN_PUT_FLAG  int nextTailIndex = calSlotIndex(currentTail + 1); if (flags[nextTailIndex].get() != CAN_PUT_FLAG) { rejectedPutHandler.rejectPutBuffer(this, uid); return false; } // 3. 将UID插入下一个slots中  // 4. 设置标志为CAN_TAKE_FLAG  // 5. tail指针+1  slots[nextTailIndex] = uid; flags[nextTailIndex].set(CAN_TAKE_FLAG); tail.incrementAndGet(); return true; } RingBuffer.take（取值） CachedUidGenerator获取UID通过RingBuffer.take()方法直接从RingBuffer中取出，获取流程如下图所示\npublic long take() { // 获取当前Cursor指针与下一个可用Cursor指针并进行check  long currentCursor = cursor.get(); long nextCursor = cursor.updateAndGet(old -\u0026gt; old == tail.get() ? old : old + 1); Assert.isTrue(nextCursor \u0026gt;= currentCursor, \u0026#34;Curosr can\u0026#39;t move back\u0026#34;); // 1. 判断如果达到阈值，则以异步模式触发填充  long currentTail = tail.get(); if (currentTail - nextCursor \u0026lt; paddingThreshold) { LOGGER.info(\u0026#34;Reach the padding threshold:{}. tail:{}, cursor:{}, rest:{}\u0026#34;, paddingThreshold, currentTail, nextCursor, currentTail - nextCursor); bufferPaddingExecutor.asyncPadding(); } // 2. 当前Cursor指针等于下一个可用Cursor指针，Ring为空，执行Take拒绝策略  if (nextCursor == currentCursor) { rejectedTakeHandler.rejectTakeBuffer(this); } // 3. 检查下一个slot是否为CAN_TAKE_FLAG  int nextCursorIndex = calSlotIndex(nextCursor); Assert.isTrue(flags[nextCursorIndex].get() == CAN_TAKE_FLAG, \u0026#34;Curosr not in can take status\u0026#34;); // 4. 从slot中获取UID  // 5. 设置标志为CAN_PUT_FLAG.  long uid = slots[nextCursorIndex]; flags[nextCursorIndex].set(CAN_PUT_FLAG); return uid; } 总结 百度uid-generator项目针对snowflake算法落地进行了一些改造，主要是对workId的获取上，对分布式集群环境下面，实例自动伸缩，docker容器化的场景，通过每次重启获取新的workId进行优化。提供了DefaultUidGenerator实时生成UID与CachedUidGenerator预生成UID两种方式。CachedUidGenerator通过借用未来时间来解决雪花算法sequence存在的并发限制，而且通过时间值递增的方式解决雪花算法存在的时间回拨问题。\n分布式ID开源项目分析-美团Leaf项目 Leaf是美团技术部推出的一个分布式ID生成服务，通过Java实现, 基于数据库号段模式和snowflake算法模式的UID获取。\nLeaf项目地址：https://github.com/Meituan-Dianping/Leaf\n项目结构 这里主要关注core模块：\n├── common\t- 公共包 │ ├── CheckVO.java\t│ ├── PropertyFactory.java\t- leaf.properties配置获取 │ ├── Result.java\t- 统一返回值 │ ├── Status.java\t- 状态值 │ ├── Utils.java\t- 工具类 │ └── ZeroIDGen.java\t- 生成0号ID\t├── IDGen.java\t- ID生成接口(I) ├── segment\t│ ├── dao\t- 号段相关Dao操作 │ │ ├── IDAllocDao.java │ │ ├── IDAllocMapper.java │ │ └── impl │ │ └── IDAllocDaoImpl.java │ ├── model\t- 号段相关model │ │ ├── LeafAlloc.java │ │ ├── SegmentBuffer.java │ │ └── Segment.java │ └── SegmentIDGenImpl.java\t- 号段相关ID生成实现 └── snowflake\t├── exception │ ├── CheckLastTimeException.java │ ├── CheckOtherNodeException.java │ └── ClockGoBackException.java ├── SnowflakeIDGenImpl.java\t- SnowFlakeID生成实现 └── SnowflakeZookeeperHolder.java\t- Zookeeper相关操作类 Leaf-segment号段模式 Leaf-segment号段模式是对上文的基于数据库号段模式的一种实现与优化。 每次获取一个segmen号段的值， 减轻数据库的压力 。利用 biz_tag字段来区分 业务，隔离影响，后续可以依据 biz_tag进行分库分表。\n数据库表结构 CREATE TABLE `leaf_alloc` ( `biz_tag` varchar(128) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;业务key\u0026#39;, `max_id` bigint(20) NOT NULL DEFAULT \u0026#39;1\u0026#39; COMMENT \u0026#39;当前已经分配了的最大id\u0026#39;, `step` int(11) NOT NULL COMMENT \u0026#39;初始步长，也是动态调整的最小步长\u0026#39;, `description` varchar(256) DEFAULT NULL COMMENT \u0026#39;业务key的描述\u0026#39;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \u0026#39;数据库维护的更新时间\u0026#39;, PRIMARY KEY (`biz_tag`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 可知，在进行UID获取之前，需要先插入数据进行biz_tag的设置与其他数据的初始化。\n实现方式 Leaf-segment的实现类是SegmentIDGenImpl\n 初始化  @Override public boolean init() { //同步biz_tag到cache中  updateCacheFromDb(); initOK = true; //开启定时任务定时调度updateCacheFromDb()  updateCacheFromDbAtEveryMinute(); return initOK; } private void updateCacheFromDb() { //获取数据库表中所有的biz_tag  List\u0026lt;String\u0026gt; dbTags = dao.getAllTags(); if (dbTags == null || dbTags.isEmpty()) { return; } //缓存的biz_tag  List\u0026lt;String\u0026gt; cacheTags = new ArrayList\u0026lt;String\u0026gt;(cache.keySet()); Set\u0026lt;String\u0026gt; insertTagsSet = new HashSet\u0026lt;\u0026gt;(dbTags); Set\u0026lt;String\u0026gt; removeTagsSet = new HashSet\u0026lt;\u0026gt;(cacheTags); //将db中新加的biz_tags加入cache中，并初始化SegmentBuffer  for(int i = 0; i \u0026lt; cacheTags.size(); i++){ String tmp = cacheTags.get(i); if(insertTagsSet.contains(tmp)){ insertTagsSet.remove(tmp); } } for (String tag : insertTagsSet) { SegmentBuffer buffer = new SegmentBuffer(); buffer.setKey(tag); Segment segment = buffer.getCurrent(); segment.setValue(new AtomicLong(0)); segment.setMax(0); segment.setStep(0); cache.put(tag, buffer); } //将db中被删除的biz_tag(已失效)从cache删除  for(int i = 0; i \u0026lt; dbTags.size(); i++){ String tmp = dbTags.get(i); if(removeTagsSet.contains(tmp)){ removeTagsSet.remove(tmp); } } for (String tag : removeTagsSet) { cache.remove(tag); } } SegmentIDGenImpl的init方法可以看出主要是对biz_tag的同步，同步到应用的缓存中并且初始化对应的Segment Buffer。并且会开启调度线程进行定时的cache刷新。\n 双Buffer  初始化cache的时候，会对每一个biz_tag初始化一个SegmentBuffer，通过其代码的实现可以看SegmentBuffer 中包含了一个号段数组，包含两个 Segment，第一个Segment已下发10%时，如果下一个Segment未更新，则另启一个更新线程去更新下一个Segment，这就是Leaf的双Buffer特性。\n这样做有两个好处：\n一是segment长度如果设置为服务高峰期发号QPS的600倍（10分钟），即使DB宕机，Leaf仍能持续发号10-20分钟不受影响。\n二是每次请求都会判断下个号段的状态，从而更新此号段，避免偶发网络抖动影响下个号段的更新。\npublic class SegmentBuffer { private String key;// 数据库的biz_tag  private Segment[] segments; //双buffer  private volatile int currentPos; //当前的使用的segment的index  private volatile boolean nextReady; //下一个segment是否处于可切换状态  private volatile boolean initOk; //是否初始化完成  private final AtomicBoolean threadRunning; //线程是否在运行中  private final ReadWriteLock lock; private volatile int step;// 动态调整的step  private volatile int minStep; // 最小step  private volatile long updateTimestamp;// 更新时间戳 } public class Segment { private AtomicLong value = new AtomicLong(0); private volatile long max; private volatile int step; private SegmentBuffer buffer; }  UID取值  UID通过调用get方法获取，具体实现逻辑大致如下\npublic Result get(final String key) { //SegmentIDGenImpl是否初始化  。。。 //cache中是否已经缓存了对应biz_id的数据  if (cache.containsKey(key)) { //检查SegmentBuffer中的Segment是否初始化  if (!buffer.isInitOk()) { //初始化Segment  updateSegmentFromDb(key, buffer.getCurrent()); buffer.setInitOk(true); } } return getIdFromSegmentBuffer(cache.get(key)); } updateSegmentFromDb方法从数据库表中读取数据更新SegmentBuffer中的Segment。在第一次初始号段与第二次双buffer的异步准备另一个号段会被调用，注意前两次调用之后后续双buffer的异步准备另一个号段会动态调整申请号段的区间大小，调整规则主要跟号段申请频率有关，具体逻辑大致如下：\npublic void updateSegmentFromDb(String key, Segment segment) { StopWatch sw = new Slf4JStopWatch(); SegmentBuffer buffer = segment.getBuffer(); LeafAlloc leafAlloc; /** * 第一次初始号段 * 1. 更新数据库中key对应记录的maxId（maxId=maxId+step) * 2. 设置buffer的Step(步长)与MinStep */ if (!buffer.isInitOk()) { leafAlloc = dao.updateMaxIdAndGetLeafAlloc(key); buffer.setStep(leafAlloc.getStep()); buffer.setMinStep(leafAlloc.getStep());//leafAlloc中的step为DB中的step  } /** * 第二次初始号段（双Buffer异步准备） * 1. 更新数据库中key对应记录的maxId（maxId=maxId+step) * 2. 设置buffer的Step(步长)与MinStep */ else if (buffer.getUpdateTimestamp() == 0) { leafAlloc = dao.updateMaxIdAndGetLeafAlloc(key); buffer.setUpdateTimestamp(System.currentTimeMillis()); buffer.setStep(leafAlloc.getStep()); buffer.setMinStep(leafAlloc.getStep());//leafAlloc中的step为DB中的step  } /** * 后续双Buffer异步准备 * 动态调整step * 1. duration \u0026lt; 15 分钟 : step 变为原来的2倍， 最大为 MAX_STEP * 2. 15分钟 \u0026lt;= duration \u0026lt; 30分钟 : nothing * 3. duration \u0026gt;= 30 分钟 : 缩小step, 最小为DB中配置的step * 更新数据库中key对应记录的maxId（maxId=maxId+step) * 设置buffer的Step(步长)与MinStep */ else { //更新时间差  long duration = System.currentTimeMillis() - buffer.getUpdateTimestamp(); int nextStep = buffer.getStep(); if (duration \u0026lt; SEGMENT_DURATION) { if (nextStep * 2 \u0026gt; MAX_STEP) { //do nothing  } else { nextStep = nextStep * 2; } } else if (duration \u0026lt; SEGMENT_DURATION * 2) { //do nothing with nextStep  } else { nextStep = nextStep / 2 \u0026gt;= buffer.getMinStep() ? nextStep / 2 : nextStep; } logger.info(\u0026#34;leafKey[{}], step[{}], duration[{}mins], nextStep[{}]\u0026#34;, key, buffer.getStep(), String.format(\u0026#34;%.2f\u0026#34;,((double)duration / (1000 * 60))), nextStep); LeafAlloc temp = new LeafAlloc(); temp.setKey(key); temp.setStep(nextStep); leafAlloc = dao.updateMaxIdByCustomStepAndGetLeafAlloc(temp); buffer.setUpdateTimestamp(System.currentTimeMillis()); buffer.setStep(nextStep); buffer.setMinStep(leafAlloc.getStep());//leafAlloc的step为DB中的step  } /** * 准备当前Segment号段，设置value初始值与max/step值为buffer的max与step */ long value = leafAlloc.getMaxId() - buffer.getStep(); segment.getValue().set(value); segment.setMax(leafAlloc.getMaxId()); segment.setStep(buffer.getStep()); sw.stop(\u0026#34;updateSegmentFromDb\u0026#34;, key + \u0026#34; \u0026#34; + segment); } 最后的getIdFromSegmentBuffer方法会调用segment.getValue().getAndIncrement()从segment中获取value得到当前的UID返回并进行自增，这里在获取之前会判断当前Segment已经使用超过10%，如果超过会异步准备双buffer的另一个Segment。如果当前号段已经用完，切换另一个Segment号段使用。流程图如下图所示：\nLeaf-snowflake模式 Leaf-snowflake数据结构上沿用了snowflake的设计，正数位（占1比特）+ 时间戳（占41比特）+ 机器ID（占5比特）+ 机房ID（占5比特）+ 自增值（占12比特），主要针对workId的生成与时间回拨问题进行了优化处理。\n实现方式  初始化WorkId生成  Leaf-snowflake依靠Zookeeper生成workId[机器ID+ 机房ID]，Leaf中workId是基于ZooKeeper的顺序Id来生成的，每个应用在使用Leaf-snowflake时，启动时都会都在Zookeeper中生成一个顺序Id，相当于一台机器对应一个顺序节点，也就是一个workId。\n启动步骤如下：\n 启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）。 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务。 如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务。  弱化对Zookeeper的依赖：Leaf-snowflake除了每次会去ZooKeeper拿数据以外，也会在本机文件系统上缓存一个workerID文件。当ZooKeeper出现问题，而且机器需要重启，能保证服务能够正常启动，做到了对三方组件的弱依赖。一定程度上提高了SLA。\n 时间回拨问题解决  为了解决Snowflake的时间回拨问题，leaf主要从两个地方进行优化，一个是启动时对自身系统时间做校验，主要比较对象为系统节点历史记录的时间与其余节点的系统时间。启动流程如下：\n服务启动时首先检查自己是否写过ZooKeeper leaf_forever节点：\n 若写过，则用自身系统时间与leaf_forever/${self}节点记录时间做比较，若小于leaf_forever/${self}时间则认为机器时间发生了大步长回拨，服务启动失败并报警。 若未写过，证明是新服务节点，直接创建持久节点leaf_forever/${self}并写入自身系统时间，接下来综合对比其余Leaf节点的系统时间来判断自身系统时间是否准确，具体做法是取leaf_temporary下的所有临时节点(所有运行中的Leaf-snowflake节点)的服务IP：Port，然后通过RPC请求得到所有节点的系统时间，计算sum(time)/nodeSize。 若abs( 系统时间-sum(time)/nodeSize ) \u0026lt; 阈值，认为当前系统时间准确，正常启动服务，同时写临时节点leaf_temporary/${self} 维持租约。 否则认为本机系统时间发生大步长偏移，启动失败并报警。 每隔一段时间(3s)上报自身系统时间写入leaf_forever/${self}。  二是运行时对时间回拨做了等待处理，时间偏差大小小于5ms，则等待两倍时间，还是小于则做异常处理,具体逻辑如下。\n//发生了回拨，此刻时间小于上次发号时间  if (timestamp \u0026lt; lastTimestamp) { long offset = lastTimestamp - timestamp; if (offset \u0026lt;= 5) { try { //时间偏差大小小于5ms，则等待两倍时间  wait(offset \u0026lt;\u0026lt; 1);//wait  timestamp = timeGen(); if (timestamp \u0026lt; lastTimestamp) { //还是小于，抛异常并上报  throwClockBackwardsEx(timestamp); } } catch (InterruptedException e) { throw e; } } else { //throw  throwClockBackwardsEx(timestamp); } } 总结 Leaf-segment是基于数据库号段模式的优化，规避网络波动：通过双Buffer的预生成方式规避了网络抖动对UID生成的影响；容灾性高：DB不可用情况下一段时间依旧可以进行发号（时间与segment步长有关）。\nLeaf-snowflake是基于Snowflake算法的优化，在workId的获取上，对分布式集群环境下面，实例自动伸缩场景，通过对zookeeper的弱依赖实现WorkId的获取。针对时间回拨问题处理对比百度uid-generator比较粗暴，做了启动校验与时间等待的处理。\n"
            }
    
        ,
            {
                "id": 15,
                "href": "https://chinalhr.github.io/post/software-architecture-patterns/",
                "title": "软件架构模式分析",
                "section": "post",
                "date" : "2020.03.12",
                "body": " 基于O\u0026rsquo;Reilly 《Software Architecture Patterns》,整理分析几种常见软件架构模式\n 五种最常见的软件架构 软件架构（software architecture）即软件的基本结构。软件架构经常伴随着架构模式，架构模式是一个通用的、可重用的解决方案，用于在给定上下文中的软件体系结构中经常出现的问题。架构模式与软件设计模式类似，但具有更广泛的范围。\n参考：software-architecture-patterns, 中文版连接 software-architecture-patterns中文版\n常见的架构模式有如下五种：\n 分层架构 事件驱动架构 微核架构（插件化架构） 微服务架构 云架构  分层架构（Layered Architecture） 模式说明 分层架构也叫N层架构，是一种很常见的架构模式，是大多数Jave EE单体应用的实际标准。分层架构模式里的组件被分成几个平行的层次，每一层都代表了应用的一个功能，大多数的结构都分成四个层次:展示层，业务层，持久层，和数据层。\n在传统MVC模式中各层职责如下：\n 展示层：处理界面展示与交互逻辑 业务层：负责处理请求对应的业务 持久层：提供数据，操作数据 数据层：数据存储  特点 关注点分离，每一层只会处理本层的逻辑，每一层都是封闭的，层层传递，这是分层架构最重要的特点。很容易做到层隔离，某一层的改变不会影响到其他层。应用更容易开发，重构，测试，管理与维护。\n基本请求流程：\n污水池反模式（architecture sinkhole anti-pattern） 需要注意防止架构陷入污水池反模式 ，这种反模式描述了请求流只是简单的穿过层次，但没做任何处理或者只处理了很少的事（例如从展示层到数据层只做了Request的传递，然后到持久层进行简单sql查询后返回查询的数据，不做其他操作处理）。利用80-20原则可以帮助确定架构是否陷入污水池反模式。大概有百分之二十的请求仅仅是做简单的穿透，百分之八十的请求会做一些业务逻辑操作是正常的情况。然而，如果这个比例反过来，大部分的请求都是仅仅穿过层，不做逻辑操作，架构就陷入了污水池反模式，可以对一些架构层进行开放或者减少层级关系。\n巨石应用（Monolith） 分层架构容易演变为巨石应用（Monolith），导致代码库难以维护。 将所有功能都部署在一个web容器中运行的系统就叫做巨石型应用。巨石型应用的好处是IDE都是为开发单个应用设计的容易开发、容易测试，容易部署——直接打包为一个完整的包，拷贝到web容器的某个目录下即可运行。\n但对于大规模的复杂应用，巨石型应用会显得特别笨重：\n 要修改一个地方就要将整个应用全部部署 编译时间过长；回归测试周期过长 开发效率降低等 巨石应用不利于更新技术框架（全部重写）  模式分析    Type Reviews     整体灵活性 较低（组件之间是强耦合的）   部署难易度 较低（小改动就需要整体发布）   可测试性 较高（因为层隔离的关系，可以对各层进行隔绝测试，模拟其他层数据）   性能 较低（因为一次Request要穿透所有架构层，会带来不必要的操作）   伸缩性 较低（分层架构构建的程序一般是单体应用，把各个层分成单独的物理模块或者把整个程序分成多个节点来扩展分层架构，但是总体的关系过于紧密，很难扩展）   易开发性 较高（实现难度低，结构简单，容易理解，不同技能的程序员可以分工，负责不同的层）    事件驱动架构（Event-Driven Architecture） 模式说明 事件驱动架构模式是一种主流的异步分发事件架构模式，用于创建可伸缩的应用程序。事件驱动架构模式由高度解耦、单一目的的事件处理组件构成，这些组件负责异步接收和处理事件。\n事件驱动架构模式包含了两种主要的拓扑结构：中介拓扑结构（Mediator Topology）和代理拓扑结构（Broker Topology）。中介拓扑结构会在一个事件内用一个核心中介分配协调多个步骤之间的关系，执行顺序；代理拓扑结构则不需要中介进行协调。\n中介拓扑结构（Mediator Topology） 中介拓扑结构适合用于拥有多个步骤，并需要在处理事件时能通过某种程度的协调将事件进行分层处理的场景。\n主要有四种组件：\n 事件队列（event queue） 事件中介（event mediator） 事件通道（event channel） 事件处理器（event processor）  组件功能（事件流处理过程）：\n客户端将一个事件发送到某个事件队列中，消息队列将其运输给事件中介进行处理和分发。事件中介接收到事件消息后进行分配、协调，通过将额外的异步事件发送给事件通道，事件处理器监听事件通道消息，对其进行消费处理。\n事件驱动架构模式中主要有两种事件：初始事件（中介所接收到的最原始的事件）待处理事件（由事件中介生成，由事件处理器接收的事件）。事件中介负责分配、协调初始事件中的各个待执行步骤，为每一个初始事件中的步骤发送一个特定的待处理事件到事件通道中，触发事件处理器接收和处理该待处理事件。\n事件中介通过事件通道将与初始事件每一个执行步骤相关联的特定待处理事件传递给事件处理器。事件队列既可以是消息队列，也可以是消息topic，大部分是消息topic，这样可以由多个消息处理器处理同一个消息。\n事件处理器作为事件驱动架构中的组件，不依赖于其他组件，独立运作，高度解耦，在应用或系统中完成特定的任务。一般来说，每一个事件处理器组件都只完成一项唯一的业务工作，在完成其特定的业务工作时不能依赖其他事件处理器（事件处理器是解耦合的）。\n相关实现：\nEclipse基金会下的Vert.x就是典型的基于中介拓扑结构的应用框架，Vert.x中的部署单位称为Verticle，Verticle通过事件循环处理传入事件到事件队列中（event queue），其中Event Loop对事件进行高速分发（event mediator与event channel），通过Context关联的Handler进行事件异步处理（event processor）。\n代理拓扑结构（Broker Topology） 代理拓扑结构不使用任何集中的编排，没有核心的事件中介，而是在事件处理器之间使用简单的队列或者集线器，事件处理器知道处理事件的下一个事件处理器。所有的事件通过一个轻量级的消息中间件如RabbitMQ，ActiveMQ等串联起来。如果你的消息比较简单，不需要重新编排，就可以使用这种结构。\n主要有两种组件：\n 代理（可被集中或相互关联在一起使用，包含所有事件流中使用的事件通道） 事件处理器 事件通道（存在于代理组件中的事件通道可以是消息队列，消息主题,或者是两者的组合）  组件功能：\n代理拓扑结构大致如下图，包含两个组件代理（broker）和 事件处理器（event processor）。其中没有一个核心的事件中介组件控制和分发初始事件；相反，每一个事件处理器只负责处理一个事件，并向外发送一个事件，以标明其刚刚执行的动作。\n特点 实现事件驱动架构模式相对于实现其他架构模式会更困难一些，因为它通过异步处理进行事件分发。使用这种架构模式会面对，比如网络分区、中介分发失败、重新连接逻辑等。\n需要注意，处理单个业务逻辑时，这种架构模式不能处理细粒度的事务。因为事件处理器都高度解耦、并且广泛分布，使得在这些事件处理器中维持一个业务单元变得非常困难。因此，使用这种架构模式时，需要考虑哪些事件能单独被处理，哪些不能，并为此设计相应事件处理器的处理粒度。如果特别依赖事务，可以选择引入一些分布式事务的框架进行处理。\n模式分析    Type Reviews     整体灵活性 高（事件处理器组件是单一的，独立的，高度解耦的）   部署难易度 较低（事件处理器可以分开部署）   可测试性 较低（事件驱动架构模式是异步进行事件分发的，异步处理带来测试难度）   性能 高（高度解耦，异步并行操作大大减少了传递消息过程中带来的时间开销）   伸缩性 高（因为高度解耦、相互独立的事件处理器组件的存在，架构具有高扩展性）   易开发性 较低（架构的异步处理机制、协议创建流程，并且需要对事件处理器和操作失败的代理提供错误控制，提高了开发难度）    微内核架构-插件架构（Microkernel Architecture） 微内核架构（Microkernel architecture）模式也被称为插件架构（plugin architecture）模式，可以用来实现基于产品的应用程序，微内核架构模式可以通过插件的形式添加额外的特性到核心系统中，提供了很好的扩展性，使得新特性与核心系统隔离开来。例如一些IDE类的产品，如Eclipse基于插件化开发的，eclipse核心是一个微内核，其他的功能如android，c++，J2EE，UML等支持可以通过安装插件的形式添加到eclipse中。\n模式说明 微内核架构主要包含两种组件:\n 核心系统 插件模块  应用逻辑被划分为独立的插件模块和核心系统，提供良好的可扩展性、灵活性，应用的新特性和自定义处理逻辑也会被隔离。\n如下图所示，微内核架构的核心系统一般情况下只包含一个能够使系统运作起来的最小化模块。核心系统通常是为特定的使用场景、规则、或者复杂条件处理定义了通用的业务逻辑，而插件模块根据这些规则实现了具体的业务逻辑。\n核心模块：了解插件模块的可用性与获取插件的方式。可以通过插件注册表的方式维护插件的信息（名称，数据规约，访问协议\u0026hellip;）。\n插件模块：绑定到核心模块，可以通过OSGi 、消息机制、web服务或者点对点的绑定。\n特点 微内核架构模式可以嵌入或用作另一种架构模式的一部分。例如可以利用微内核架构解决应用中一部分易变领域的特定的问题。\n微内核架构对渐进式设计和增量开发提供了很好的支持。可以先构建一个单纯的核心系统，随着应用的演进逐渐添加越来越多的特性和功能，插件化的迭代升级不会引起核心系统的重大变化。\nAPI网关的插件化 API网关特别适合基于插件架构进行设计。其大部分功能如鉴权、WAF、黑白名单、限流、路由、协议转换、熔断等都是适用于各个场景的，很多时候对于一些服务网关层并不需要这些功能的实现，因此对这部分功能进行插件化与热插拔功能实现可以大大提升网关自身的灵活性与可扩展性。像一些著名的网关如Kong，SpringCloud Gateway，Soul都是基于插件架构设计的。\n拦截过滤器模式（Intercepting Filter Pattern）用于对应用程序的请求或响应做一些预处理/后处理。定义过滤器，并在把请求传给实际目标应用程序之前应用在请求上。Soul网关就是基于这个设计模式进行插件架构设计。\n具体模式实现可参考：https://www.tutorialspoint.com/design_pattern/intercepting_filter_pattern.htm\n模式分析    Type Reviews     整体灵活性 高（插件模块的松耦合实现，可以将变化隔离起来，并且快速满足需求）   部署难易度 较低（插件模块可以在运行时被动态地添加到核心系统中 （ 比如，热部署 ）,避免重复部署）   可测试性 高（插件模块能够被独立的测试，而且很容易模拟演示）   性能 高（高度解耦，高度可定制性可以只加载需要的功能减小性能消耗）   伸缩性 低（微内核架构的实现是基于产品的，通常以独立单元的形式实现，伸缩性较低）   易开发性 较低（需要详尽周全的设计和规约管理，插件的注册，粒度，连接选择等导致架构实现较复杂）    微服务架构（Microservices architecture） 模式说明 微服务架构的几个概念：\n 单独部署单元：微服务架构的每个组件都作为一个独立单元进行部署，让每个单元可以通过有效、简化的传输管道进行通信，且具有扩展性，应用和组件之间高度解耦，使得部署更为简单。 服务组件：服务组件包含一个或多个模块，提供一个单一功能（如，根据地区获取天气情况）或作为一个大型商业应用的一个独立部分（费率计算），需要权衡考虑如何正确设计服务组件的粒度。 分布式的架构：架构内部的所有组件之间是完全解耦的，通过某种远程访问协议（如，REST,Dubbo，Grpc等）进行访问。 微服务架构由其他常见架构模式存在的问题演化来的（非作为一个解决方案被创造出来），主要是从分层架构模式的单体应用和面向服务架构的分布式应用，由持续交付开发促成。  如图所示，每一个微服务的组件都被分隔成一个独立的单元。服务组件（service component）从粒度上讲它可以是单一的模块或者多个模块组成的应用程序，代表单一功能。\n实现微服务架构，可以通过如下三个主要的拓扑模式：\nAPI REST-based拓扑结构 基于REST的API拓扑通过某些API（application programming interface）对外提供小型的、自包含的服务，由粒度非常细的服务组件组成，这些服务组件包含一个或两个模块并独立于其他服务来执行特定业务功能。这些细粒度的服务组件通常被REST-based的接口访问，而这个接口是通过一个单独部署的web API层实现的。常见于基于云的RESTful web service。\nApplicaiton REST-based拓扑结构 Applicaiton REST-based不同于上面的架构，客户端看到的是web界面或者富客户端程序，而不是调用API层。应用的用户接口层（user interface layer）是一个web应用，可以通过简单的REST-based接口访问单独部署的服务组件（业务功能）。与API REST-based拓扑结构不同，服务组件往往会更大、粒度更粗、代表整个业务应用程序的一小部分，而不是细粒度的、单一操作的服务。常见于Saas类的企业应用。\n集中式消息拓扑结构 集中式消息拓扑结构与Applicaiton REST-based拓扑结构类似，但是使用一个轻量级的消息broker取代RESTful的服务调用（例如MQ中间件）。不同于SOA，轻量级消息代理（Lightweight Message Broker）不执行任何编排,转换,或复杂的路由;它只是一个轻量级访问远程服务组件的传输工具。\n集中式消息拓扑结构通常应用在较大的业务应用程序中，具有排队机制、异步消息传递、监控、错误处理和更好的负载均衡和可扩展性。\n服务组件间通信 服务组件间通信一般可以通过两种大的方式\n Remote Procedure Call：基于HTTP协议或者私有协议的RPC调用（同步/异步） AMQP-based：基于消息队列的异步消息处理机制（异步）  模式分析    Type Reviews     整体灵活性 高（服务组件隔离变化，松耦合的架构，更好地支持持续交付）   部署难易度 较低（单独部署单元，更好地支持持续集成持续发布）   可测试性 高（业务功能被分离成独立的应用模块,可以在局部范围内进行测试）   性能 低（整体上由于微服务架构模式的分布式特性带来的远程调用，链路边长等性能损耗，并不适用于高性能的应用程序）   扩展性 高（应用程序被分为单独的部署单元,每个服务组件可以单独扩展，扩展性较高）   易开发性 高（功能被分隔成不同的服务组件，开发范围更小且被隔离，开发变得更简单而且可以减少开发人员或开发团队之间的协调）    基于空间的架构-云架构（Space-Based Architecture） 一般基于web的应用随着用户负载的增加会出现瓶颈，先在web服务器层，后是应用服务器层，最后到数据库层。一般的解决办法就是向外扩展，例如扩展服务器数量。最终会陷入一个金字塔式的情形，在金字塔最下面是web服务器，它会出现最多的问题，但也最好伸缩。金字塔顶部是数据库服务器，问题不多，但最难伸缩。虽然有各种缓存技术和数据库伸缩产品都在帮助解决这个问题，但数据库难以伸缩的现实并没有改变。\n基于空间的架构模型是为了解决伸缩性和并发问题而设计的。目的在架构上解决这个伸缩性问题。\n模型说明 基于空间的架构模型（云架构模型）旨在减少限制应用伸缩的因素，名字来源于分布式共享内存中的 tuple space（数组空间）概念。通过去除中心数据库的限制做到高伸缩性，数据基于分布式共享内存进行数据共享。进程可以动态的随着用户数量增减而启动或结束，去除了中心数据库瓶颈的限制，以此来解决伸缩性问题和扩展性问题。\n主要模块：\n 处理单元（processing unit）  处理单元中包含着应用模块、内存中数据框架、处理异步数据恢复的组件和复制引擎的处理单元架构。\n 虚拟化中间件（virtualized middleware）  虚拟化中间件负责保护自身以及通信。它包含用于数据同步和处理请求的模块，以及通信框架，数据框架，处理框架和部署管理器。\n模块间合作（虚拟化中间件组件）  通信框架（Messaging Grid）  通信框架管理输入请求和会话信息。当有请求进入虚拟化中间件，通信框架就决定有哪个处理单元可用，并将请求通过负载算法传递给这个处理单元。\n 数据框架（Data Grid）  数据框架与各个处理单元的数据复制引擎交互，在数据更新时来管理数据复制功能。\n 处理框架（Processing Grid）  处理框架负责管理在有多个处理单元时的分布式请求处理。\n 部署管理器（Deployment Manager）  部署管理器根据负载情况管理处理单元的动态启动和关闭。它持续监控响应时间和用户负载，在负载增加时启动新的处理单元，在负载下降时关闭处理单元，以达到实现架构的高伸缩性。\n特点 基于空间的架构是一个复杂而昂贵的模式，适合于小型的负载可变的web应用，对于拥有大量的传统大规模关系型数据库应用不适用。\n虽然基于空间的架构模型不需要集中式的数据储存，但是需要一个进行初始化内存中数据框架，和异步的更新各处理单元数据的框架，通常会创建一个单独的分区以减少处理单元之间对对方内存数据的依赖。\n模式分析    Type Reviews     整体灵活性 高（处理单元隔离变化，松耦合的架构，更好地支持持续交付）   部署难易度 较低（单独部署处理单元，更好地支持持续集成持续发布）   可测试性 低（测试高用户负载很昂贵而且很耗时）   性能 高（数据在内存中存取以及支持缓存机制）   伸缩性 高（几乎不依赖集中式的数据库，可以轻松进行单元扩展）   易开发性 低（分布式内存数据管理框架提高开发难度）   "
            }
    
        ,
            {
                "id": 16,
                "href": "https://chinalhr.github.io/post/spring-webflux-principle/",
                "title": "Spring WebFlux 原理与适用场景",
                "section": "post",
                "date" : "2020.02.13",
                "body": " Spring WebFlux源码分析与适用场景\n 关于WebFlux Spring Framework 5提供了完整的端到端响应式编程的支持。这是一种不同于Servlet的全新的编程范式和技术栈，它基于异步非阻塞的特性，能够借助EventLoop以少量线程应对高并发的访问，对微服务架构也颇有助益。\n支持Spring 5的Spring Boot 2.0来说，新加入的响应式技术栈是其主打核心特性。具体来说，Spring Boot 2支持的响应式技术栈包括如下：\n Spring Framework 5提供的非阻塞web框架Spring Webflux； 遵循响应式流规范的兄弟项目Reactor； 支持异步I/O的Netty、Undertow等框架，以及基于Servlet 3.1+的容器（如Tomcat 8.0.23+和Jetty 9.0.4+）； 支持响应式的数据访问Spring Data Reactive Repositories； 支持响应式的安全访问控制Spring Security Reactive；  WebFlux对比SpringMVC并发模型 SpringMVC基于的Servlet并发模型 servlet由servlet container进行生命周期管理。container启动时构造servlet对象并调用servlet init()进行初始化；container关闭时调用servlet destory()销毁servlet；container运行时接受请求，并为每个请求分配一个线程（一般从线程池中获取空闲线程）然后调用service()。\n处理请求的时候同步操作，一个请求对应一个线程来处理，并发上升，线程数量就会上涨（上线文切换，内存消耗大）影响请求的处理时间。现代系统多数都是IO密集的，同步处理让线程大部分时间都浪费在了IO等待上面。虽然Servlet3.0后提供了异步请求处理与非阻塞IO支持，但是使用它会远离Servlet API的其余部分，比如其规范是同步的（Filter, Servlet）或阻塞的（getParameter,getPart），而且其对响应的写入仍然是阻塞的。\nWebFlux并发模型 WebFlux模型主要依赖响应式编程库Reactor，Reactor 有两种模型，Flux 和 Mono，提供了非阻塞、支持回压机制的异步流处理能力。WebFlux API接收普通Publisher作为输入，在内部使其适配Reactor类型，使用它并返回Flux或Mono作为输出。\nWebFlux 使用Netty作为默认的web服务器，其依赖于非阻塞IO，并且每次写入都不需要额外的线程进行支持。\n也可以使用Tomcat、Jetty容器，不同与SpringMVC依赖于Servlet阻塞IO，并允许应用程序在需要时直接使用Servlet API，WebFlux依赖于Servlet 3.1非阻塞IO。使用Undertow作为服务器时，WebFlux直接使用Undertow API而不使用Servlet API。\n当WebFlux运行在Netty服务器上，其线程模型如下：\nNettyServer的Boss Group线程池内的事件循环会接收这个请求，然后把完成TCP三次握手的连接channel交给Worker Group中的某一个事件循环线程来进行处理（该事件处理线程会调用对应的controller进行处理）。所以WebFlux的handler执行是使用Netty的IO线程进行执行的，所以需要注意如果handler的执行比较耗时，会把IO线程耗尽导致不能再处理其他请求，可以通过Reactor的publishOn操作符切换到其他线程池中执行。\n范例 参考\nhttps://docs.spring.io/spring/docs/current/spring-framework-reference/web-reactive.html#webflux\n性能 响应式和非阻塞并不是总能让应用跑的更快，况且将代码构建为非阻塞的执行方式本身还会带来少量的成本。但是在类似于WEB应用这样的高并发、少计算且I/O密集的应用中，响应式和非阻塞往往能够发挥出价值。\n对比SpringMVC使用的Servlet模型，增加Servlet容器处理请求的线程数量可以缓解这一问题，但是增加线程是有成本的，JVM中默认情况下在创建新线程时会分配大小为1M的线程栈，所以更多的线程意味着需要更多的内存；更多的线程会带来更多的线程上下文切换成本。\nwebflux对比SpringMVC的性能测试 对于运行在异步IO的之上的WebFlux应用来说，其工作线程数量始终维持在一个固定的数量上，通常这个固定的数量等于CPU核数。从测试图中可以看到，随着用户数的增多，webflux吞吐量基本呈线性增多的趋势，95%的响应都在100ms+的可控范围内返回了，并未出现延时的情况。而SpringMVC线程数达到200/400前，95%的请求响应时长是正常的，之后呈直线上升的态势；\n结论：非阻塞的处理方式规避了线程排队等待的情况，从而可以用少量而固定的线程处理应对大量请求的处理，提升应用的吞吐量和伸缩性。\n参考链接\nWebFlux请求分发 reactor的map，flatMap，concatMap  map  map是同步非阻塞的1对1的转换数据处理。map方法签名接受Function\u0026lt;T, U\u0026gt; 返回Flux\u0026lt;U\u0026gt;。\n//同步执行乘法操作 Flux.just(1,2,3,4,5) .log() .map(i-\u0026gt;{ try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } return i*10; }) .subscribe(c-\u0026gt;log.info(\u0026#34;getInt:{}\u0026#34;,c));  flatMap  flatMap是异步非阻塞的1对N的转换数据处理。flayMap方法签名接受Function\u0026lt;T, Publisher\u0026lt;V\u0026gt;\u0026gt; 返回 Flux\u0026lt;V\u0026gt;。\n//异步执行乘法 Flux.just(1,2,3,4,5) .log() .flatMap(i-\u0026gt; Flux.just(i*10).delayElements(Duration.ofSeconds(1))) .subscribe(c -\u0026gt;log.info(\u0026#34;getInt:{}\u0026#34;,c)); 总结：flatMap的转换Function要求返回一个Publisher，这个Publisher代表一个作用于元素的异步的转换操作；而map仅仅是同步的元素转换操作。\n concatMap  concatMap 操作符的作用是把流中的每个元素转换成一个流, 再把所有流进行合并. 与 flatMap不同的是,concatMap会根据原始流中的元素顺序依次把转换之后的流进行合并。\nWebFlux 的DispatcherHandler Spring MVC 的前端控制器是 DispatcherServlet，而WebFlux 的前端控制器是 DispatcherHandler，它实现了 WebHandler接口。DispatcherHandler完成 handler 的查找、调用和结果处理等步骤，关联的Bean如下：\n   Bean 类型 解释     HandlerMapping 将请求映射到对应的 handler。主要的 HandlerMapping 实现有处理 @RequestMapping 注解的 RequestMappingHandlerMapping ，处理函数路由的RouterFunctionMapping，以及处理简单 URL 映射的 SimpleUrlHandlerMapping。   HandlerAdapter 帮助 DispatcherHandler 调用请求对应的 handler，而不用关心该 handler 具体的调用方式。例如，调用一个通过注解的方式定义的 controller 就需要寻找对应的注解，而 HandlerAdapter 的主要目的就是为了帮助 DispatcherHandler 屏蔽类似的细节.   HandlerResultHandler 处理 handler 调用后的结果，并生成最后的响应。参考 Result Handling。    public interface WebHandler { /** * Handle the web server exchange. * @param exchange the current server exchange * @return {@code Mono\u0026lt;Void\u0026gt;} to indicate when request handling is complete */ Mono\u0026lt;Void\u0026gt; handle(ServerWebExchange exchange); } public class DispatcherHandler implements WebHandler, ApplicationContextAware { ... @Override public Mono\u0026lt;Void\u0026gt; handle(ServerWebExchange exchange) { //流程1\t if (this.handlerMappings == null) { return createNotFoundError(); } return Flux.fromIterable(this.handlerMappings) //流程2 \t.concatMap(mapping -\u0026gt; mapping.getHandler(exchange)) .next() //流程3 \t.switchIfEmpty(createNotFoundError()) //流程4 \t.flatMap(handler -\u0026gt; invokeHandler(exchange, handler)) //流程5 \t.flatMap(result -\u0026gt; handleResult(exchange, result)); } } ServerWebExchange对象每一次 HTTP 请求的信息（包括请求参数，路径，Cookie等）\n从DispatcherHandler的handle实现可以看出WebFlux的请求分发流程：\n 判断整个接口映射 mappings集合是否为空，空则创建一个 Not Found 的请求错误响应； 根据具体的请求地址获取对应的 handlerMapping（处理方法）; handlerMapping为空的话找不到对应的处理方法，创建一个 Not Found 的请求错误响应； 通过 invokeHandler 方法找到对应的 HandlerAdapter 来完成调用 由 HandlerResultHandler 对结果进行处理，并生成响应  delay原理 对于运行在异步IO的之上的WebFlux应用来说，其工作线程数量始终维持在一个固定的数量上，程序逻辑中有阻塞（如io阻塞等）需要进行异步化。如新出的WebClient工具就是将http请求io异步化，用delay方法代替sleep方法将延时异步化。\ndelay原理实现 public static Mono\u0026lt;Long\u0026gt; delay(Duration duration) { return delay(duration, Schedulers.parallel()); } public static Mono\u0026lt;Long\u0026gt; delay(Duration duration, Scheduler timer) { return onAssembly(new MonoDelay(duration.toMillis(), TimeUnit.MILLISECONDS, timer)); } 查看delay方法源码，可以看到它里面其实构造一个MonoDelay类，并通过传入全局公用的调度器Schedulers.parallel()来调度里面的异步任务。\n查看MonoDelay类的subscribe方法\npublic void subscribe(CoreSubscriber\u0026lt;? super Long\u0026gt; actual) { MonoDelayRunnable r = new MonoDelayRunnable(actual); actual.onSubscribe(r); try { r.setCancel(timedScheduler.schedule(r, delay, unit)); } catch (RejectedExecutionException ree) { if(r.cancel != OperatorDisposables.DISPOSED) { actual.onError(Operators.onRejectedExecution(ree, r, null, null, actual.currentContext())); } } } 代码timedScheduler.schedule(r, delay, unit)方法，通过timedScheduler来调度延时任务。\n查看timedScheduler的schedule方法\n@Override public Disposable schedule(Runnable task, long delay, TimeUnit unit) { return Schedulers.directSchedule(pick(), task, null, delay, unit); } static Disposable directSchedule(ScheduledExecutorService exec, Runnable task, @Nullable Disposable parent, long delay, TimeUnit unit) { task = onSchedule(task); SchedulerTask sr = new SchedulerTask(task, parent); Future\u0026lt;?\u0026gt; f; if (delay \u0026lt;= 0L) { f = exec.submit((Callable\u0026lt;?\u0026gt;) sr); } else { f = exec.schedule((Callable\u0026lt;?\u0026gt;) sr, delay, unit); } sr.setFuture(f); return sr; } 通过directSchedule可以看出，delay方法之所以没有阻塞主线程，因为它的延时处理的逻辑包装成SchedulerTask，交给了ScheduledExecutorService执行器处理，调用delay方法的主线程就直接返回了，当delay\u0026gt;0是使用ScheduledExecutorService进行延迟调度。\n结论 WebFlux将部分阻塞的逻辑修改为类似于delay方法的实现，利用调度执行器去异步执行阻塞的逻辑，不阻塞EventLoop线程，使得少量的工作线程可以承载更多的请求。\n适用场景 使用 Spring WebFlux，下游使用的安全认证层、数据访问层框架都必须使用 Reactive API 保证上下游都是匹配的，非阻塞的。然而Spring Data Reactive Repositories 目前只支持 MongoDB、Redis 和Couchbase 等几种不支持事务管理的 NOSQL，技术选型时需要权衡利弊和风险。\n Spring MVC能满足场景的，就不需要更改为 Spring WebFlux，毕竟Reactive写法对比原本同步执行的程序写法很不同，而且很多基于Servlet线程模型的库将无法使用，如Spring Transaction\u0026hellip;\u0026hellip;。 需要底层容器的支持（Netty和Servlet3.1+）。 适合应用在 IO 密集型的服务中（IO 密集型包括：磁盘IO密集型, 网络IO密集型），微服务网关就属于网络 IO 密集型，使用异步非阻塞式编程模型，能够显著地提升网关对下游服务转发的吞吐量。 "
            }
    
        ,
            {
                "id": 17,
                "href": "https://chinalhr.github.io/post/api-auth-program/",
                "title": "API认证与授权方案",
                "section": "post",
                "date" : "2020.02.11",
                "body": " Api认证授权方案总结\n 关于认证，授权与凭证  认证（authentication）  认证指的是当前用户的身份，当用户登陆过后系统便能追踪到他的身份做出符合相应业务逻辑的操作。\n认证技术解决的是我是谁的问题。\n 授权（authorization）  授权指的是什么样的身份被允许访问某些资源，在获取到用户身份后继续检查用户的权限。单一的系统授权往往是伴随认证来完成的，但是在开放 API 的多系统结构下，授权可以由不同的系统来完成，例如 OAuth。\n授权技术解决的是我能干什么的问题。\n 凭证（credentials）  凭证是认证和授权的基础，用来标记访问者的身份或权利，例如服务器为每一个访问者颁发 session ID 存放到 cookie，SSH 登录的密匙、JWT 令牌\u0026hellip;\nAPI开发中常用的认证，授权技术 HTTP Basic Authentication 流程  用户在浏览器输入用户名和密码 组合用户名和密码然后 Base64 编码 给编码后的字符串添加 Basic 前缀，然后设置名称为 Authorization 的 header 头部  API 也可以提供 HTTP Basic Authentication 认证方式，那么客户端可以很简单通过 Base64 传输用户名和密码即可。\n缺陷 BASE64仅仅是编码而非加密，如果以HTTP明文传输会有泄露风险。\nHMAC（AK/SK）认证 很多开放API平台都会选择基于一个AK/SK的认证方式。这种基于 AK/SK 的认证方式主要是利用散列的消息认证码 (Hash-based MessageAuthentication Code) 来实现的。HMAC是利用带有 key 值的哈希算法生成消息摘要。\n基本过程如下：\n签名生成 基于参数timestamp,AK,请求PATH,请求参数进行key自然排序，然后进行字段与字段值拼接最后再拼接上SK（注意SK不传递），MD5加密生成Sign，将Sign加入请求参数/请求头中发送请求。\n鉴权 分解请求参数得出：timestamp,AK,请求PATH,请求参数。\n  是对timestamp进行临界值判断（例如过滤掉5分钟之前的请求，规避重放攻击）。\n  是对当前请求的PATH进行权限判断，判断用户AK是否有当前PATH的请求权限。\n  是根据AK在服务端（如数据库）获取到用户的SK，对参数timestamp,AK,请求PATH,请求参数进行key自然排序，然后进行字段与字段值拼接最后再拼接上SK，MD5加密生成Sign与传递过来的Sign进行对比鉴权。\n  质疑/应答算法 基于时间的一次性密码认证并不能严格避免重放攻击，质疑/应答算法需要客户端先请求一次服务器，获得一个 401 未认证的返回，并得到一个随机字符串（nonce）。将 nonce 附加到按照上面说到的方法进行 HMAC 签名，服务器使用预先分配的 nonce 同样进行签名校验，这个 nonce 在服务器只会被使用一次，因此可以提供唯一的摘要。\nOAuth2 OAuth2 是一个开放授权标准，它允许用户让第三方应用访问该用户在某服务的特定私有资源。\nOAuth2角色  Resource Owner：资源拥有者(如图片资源的所有者) Resource Server：资源服务器(如存储图片资源的服务器) Client: 任何可以消费资源服务器的第三方应用 Authorization Server ：授权服务器，管理Resource Owner，Client和Resource Server的三角关系的中间层。  OAuth2的部署与授权流程  增加一个Authorization server，提供授权的实现，一般由Resource server 来提供。 Resource server 为第三方应用程序提供注册接口。 Resource server 开放相应的受保护资源的API。 Client 注册成为Resource server的第三方应用。 Client 消费这些API。  在一般情况下，Resource server提供Authorization server服务，提供授权接口与获取访问令牌接口。\n流程：\n （A）用户访问第三方网站，第三方网站向资源服务器发起授权请求； （B）资源服务器接受第三方网站的授权请求，并返回授权许可给PP； （C）第三方网站使用授权许可向Authorization server发起请求**；** （D）Authorization server验证第三方网站的身份和授权许可，发送访问令牌给第三方网站； （E）第三方网站用访问令牌请求用户存储在资源服务器的资源； （F）资源服务器根据访问令牌，返回用户的信息给第三方网站。  四种授权许可方式（Authorization Grant） 授权许可方式对应上述的A,D,C,D阶段，授权许可是一个代表资源所有者授权（访问受保护资源）的凭据，客户端用它来获取访问令牌。授权许可是用户授予第三方获得资源服务器访问令牌的一个凭据。\n Authorization Code：授权码； Implicit：隐式许可； Resource Owner Password Credentials：资源所有者密码凭据； Client Credentials ：客户端凭据。  Authorization Code  （A）Client使用浏览器（用户代理）访问Authorization server。也就是用浏览器访问一个URL，这个URL是Authorization server提供的，访问的时候Client需要提供（客户端标识，请求范围，本地状态和重定向URL）这些参数。 （B）Authorization server验证Client在（A）中传递的参数信息，如果无误则提供一个页面供Resource owner登陆，登陆成功后选择Client可以访问Resource server的哪些资源以及读写权限。 （C）在（B）无误后返回一个授权码（Authorization Code）给Client。 （D）Client拿着（C）中获得的授权码（Authorization Code）和（客户端标识、重定向URL等信息）作为参数，请求Authorization server提供的获取访问令牌的URL**。** （E）Authorization server返回访问令牌和可选的刷新令牌以及令牌有效时间等信息给Client。  Implicit 是Authorization Code的简化版本。其中省略掉了颁发授权码（Authorization Code）给客户端的过程，而是直接返回访问令牌和可选的刷新令牌。\nResource Owner Password Credentials 和Authorzation Code类型下重要的区分就是省略了Authorization Request和Authorization Response。而是Client直接使用Resource owner提供的username和password来直接请求access_token（直接发起Access Token Request然后返回Access Token Response信息）。\nClient Credentials Client直接已自己的名义而不是Resource owner的名义去要求访问Resource server的一些受保护资源。\nOAuth2参考 https://www.yuque.com/lihanrong/za12dz/mvf82u\nJWT JSON Web Token JSON Web Token是一套开放的标准（RFC 7519），它定义了一套简洁（compact）且 URL 安全（URL-safe）的方案，以安全地在客户端和服务器之间传输 JSON 格式的信息。\n无状态原则：用户登录之后，服务器会返回一串 token 并保存在本地，在这之后的服务器访问都要带上这串 token，来获得访问相关路由、服务及资源的权限。比如单点登录就比较多地使用了 JWT，因为它的体积小，并且经过简单处理（使用 HTTP 头带上 Bearer 属性 + token ）就可以支持跨域操作。\nJWT工作流程  首先，某 client 使用自己的账号密码发送 post 请求 login，由于这是首次接触，服务器会校验账号与密码是否合法 如果一致，则根据密钥生成一个 token 并返回 client 收到这个 token 并保存在本地。在这之后，需要访问一个受保护的路由或资源时，只要附加上 token（通常使用 Header 的 Authorization 属性）发送到服务器 服务器就会检查这个 token 是否有效，并做出响应。  JWT组成 // Header { \u0026quot;alg\u0026quot;: \u0026quot;HS256\u0026quot;, \u0026quot;typ\u0026quot;: \u0026quot;JWT\u0026quot; } // Payload { // reserved claims \u0026quot;iss\u0026quot;: \u0026quot;a.com\u0026quot;, \u0026quot;exp\u0026quot;: \u0026quot;1d\u0026quot;, // public claims \u0026quot;http://a.com\u0026quot;: true, // private claims \u0026quot;company\u0026quot;: \u0026quot;A\u0026quot;, \u0026quot;awesome\u0026quot;: true } // $Signature HS256(Base64(Header) + \u0026quot;.\u0026quot; + Base64(Payload), secretKey) // JWT JWT = Base64(Header) + \u0026quot;.\u0026quot; + Base64(Payload) + \u0026quot;.\u0026quot; + $Signature 三部分组成：\n  经过 Base64 编码的 Header。Header 是一个 JSON 对象，对象里有一个值为 “JWT” 的 typ 属性，以及 alg 属性，值为 HS256，表明最终使用的加密算法是 HS256。\n  经过 Base64 编码的 Payload。Payload 被定义为实体的状态，就像 token 自身附加元数据一样，claim 包含我们想要传输的信息，以及用于服务器验证的信息，一般有 reserved/public/private 三类。\n  $Signature。它由 Header 指定的算法 HS256 加密产生。该算法有两个参数，第一个参数是经过 Base64 分别编码的 Header 及 Payload 通过 . 连接组成的字符串，第二个参数是生成的密钥，由服务器保存。\n  JWT 仅仅是对 payload 做了简单的 sign 和 encode 处理，并未被加密，并不能保证数据的安全性。\n服务端验证 服务端接收到 token 之后，会逆向构造过程，decode 出 JWT 的三个部分，这一步可以得到 sign 的算法及 payload，结合服务端配置的 secretKey，可以再次进行 $Signature 的生成得到新的 $Signature，与原有的 $Signature 比对以验证 token 是否有效，完成用户身份的认证，验证通过才会使用 payload 的数据。\n服务端最终只是为了验证 $Signature 是否仍是自己当时下发给 client 的那个，如果验证通过，则说明该 JWT 有效并且来自可靠来源，否则说明可能是对应用程序的潜在攻击，以此完成认证。\n"
            }
    
        ,
            {
                "id": 18,
                "href": "https://chinalhr.github.io/post/mongo-back/",
                "title": "MongoDB备份与还原",
                "section": "post",
                "date" : "2020.02.08",
                "body": " 记录MongoDB备份与还原知识点与个人站点的MongoDB备份脚本\n 备份 MongoDB数据备份的几种方式  mongodump命令 系统快照 cp or rsync  官方参考文档：https://docs.mongodb.com/manual/core/backups/\nmongodump命令 在Mongodb中可以使用mongodump命令来备份MongoDB数据,mongodump命令可以从 MongoDB 数据库读取数据，并生成 BSON 文件，mongodump 适合用于备份和恢复数据量较小的 MongoDB 数据库，不适用于大数据量备份。\nmongodump 仅备份数据库中的文档，不备份索引，所以还原数据后需要重新生成索引。\nmongodump 备份过程中会对 mongod 服务的性能产生影响，建议在业务低峰期进行操作。\n dump命令参数  # 命令格式 mongodump \u0026lt;options\u0026gt; --host \u0026lt;hostname\u0026gt;\u0026lt;:port\u0026gt;, -h \u0026lt;hostname\u0026gt;\u0026lt;:port\u0026gt; # 指定备份的主机ip和端口号，默认值localhost:27017 --port # 指定端口号 默认27017 --username \u0026lt;username\u0026gt;, -u \u0026lt;username\u0026gt; # 指定用户名 --password \u0026lt;password\u0026gt;, -p \u0026lt;password\u0026gt; # 指定密码 --authenticationDatabase \u0026lt;dbname\u0026gt; # 指定认证的数据库 --authenticationMechanism \u0026lt;name\u0026gt; # 指定认证的算法 ，默认值 SCRAM-SHA-1 --db \u0026lt;database\u0026gt;, -d \u0026lt;database\u0026gt; # 指定备份的数据库，未指定的话，备份所有的数据库，但不包含local库 --collection \u0026lt;collection\u0026gt;, -c \u0026lt;collection\u0026gt; # 指定备份的集合，未指定则备份指定库中的所有集合。 --query \u0026lt;json\u0026gt;, -q \u0026lt;json\u0026gt; # 指定 json 作为查询条件。来备份我们过滤后的数据。 --queryFile \u0026lt;path\u0026gt; # 指定 json 文档路径，以该文档的内容作为查询条件，来备份我们过滤后的数据。 --quit # 通过抑制 MongoDB的复制，连接等活动，来实现备份。 --gzip # 开启压缩，3.2版本后可以使用，输出为文件的话会带有后缀.gz --out \u0026lt;path\u0026gt;, -o \u0026lt;path\u0026gt; # 输出的目录路径 --repir # 修复数据时使用 下面有详细介绍 --oplog # mongodump 会将 mongodump 执行期间的 oplog 日志 输出到文件 oplog.bson，这就意味着从备份开始到备份结束的数据操作我们都可以记录下来。 --archive \u0026lt;file\u0026gt; # 输出到单个存档文件或者是直接输出。 --dumpDbUsersAndRoles # 只有在 使用 --db 时才适用，备份数据库的包含的用户和角色。 --excludeCollection string # 排除指定的集合，如果要排除多个，使用多个--excludeCollection --numParallelCollections int, -j int # 并行导出的集合数，默认为4 --ssl # 指定 TLS/SSL 协议 --sslCAFile filename # 指定认证文件名 --sslPEMKeyFile \u0026lt;filename\u0026gt; --sslPEMKeyPassword \u0026lt;value\u0026gt; --sslCRLFile \u0026lt;filename\u0026gt; --sslAllowInvalidCertificates --sslAllowInvalidHostnames --sslFIPSMode 可以通过mongodump \u0026ndash;help查看options\n系统快照 如果MongoDB是部署在阿里云ECS之类的云服务器，可以利用系统快照与生成策略进行定时备份。\ncp or rsync 直接使用cp或者rsync等工具进行mongodb的data文件复制，自复制多个文件不是一个原子操作,所以在复制前必须停止对 MongoDB 的操作。 否则复制的文件会处于无效状态。\n还原 mongorestore 在Mongodb中可以使用mongorestore 命令来恢复备份的数据。\nmongorestore可以创建新的数据库或将数据添加到现有的数据库，但是 mongorestore 仅仅执行insert操作，不执行update操作。这就意味着如果将文档还原到现有的数据库，现有的数据库中的文档的_id的值和要还原的文档中的_id 值是一样的，是不会将数据库原有的值覆盖的。\n restore命令参数  mongorestore \u0026lt;options\u0026gt; \u0026lt;directory or file to restore\u0026gt; --help # 查看帮助 --quiet # 通过抑制 MongoDB的复制，连接等活动，来实现数据恢复。 --host \u0026lt;hostname\u0026gt;\u0026lt;:port\u0026gt;, -h \u0026lt;hostname\u0026gt;\u0026lt;:port\u0026gt; # 指定恢复的主机ip和端口号，默认值localhost:27017 --port # 指定端口号 默认27017 --username \u0026lt;username\u0026gt;, -u \u0026lt;username\u0026gt; # 指定用户名 --password \u0026lt;password\u0026gt;, -p \u0026lt;password\u0026gt; # 指定密码 --authenticationDatabase \u0026lt;dbname\u0026gt; # 指定认证的数据库 --authenticationMechanism \u0026lt;name\u0026gt; # 指定认证的算法 ，默认值 SCRAM-SHA-1 --objcheck # 开启验证，验证还原操作，确保没有无效的文档插入数据库。会有较小的性能影响 --oplogReplay # 恢复备份数据并将 mongodump 执行期间的操作(记录在导出的日志)恢复。 --oplogLimit # 指定恢复 --oplogFile # 指定 Oplog 路径 --keepIndexVersion # 阻止mongorestore在还原过程中将索引升级到最新版本。 --restoreDbUsersAndRoles # 还原指定的数据库用户和角色。 --maintainInsertionOrder # 默认值为False,如果为 True,mongorestore 将按照输入源的文档顺序插入，否则是 随机执行插入。 --numParallelCollections int, -j int # 指定并行恢复的集合数。 --numInsertionWorkersPerCollection int # 默认值为 1，指定每个集合恢复的并发数，大数据量导入增加该值可提高 恢复速度。 --gzip # 从压缩文档中 恢复。 --archive # 从归档文件中恢复。 --dir # 指定还原数据储存目录。 可以通过mongorestore \u0026ndash;help查看options\nMongoDB定时备份Shell脚本实现 可以通过Shell脚本与linux的crontab实现定时备份，目的是做到如下几点：\n 使用dump命令备份七天内的mongoDB数据 需要对备份的数据进行压缩归档处理（mongodump并不提供压缩归档功能） 过期备份数据清理，只保留七天内  定时备份Shell脚本如下：\n#-------------------------------------------- # mongodb定时备份脚本 #-------------------------------------------- #! /bin/bash # 命令执行路径 MONGOD=/usr/bin/mongodump OUT_DIR=/data/backup/mongo/mongod_bak_tmp # 压缩后的备份存放路径 TAR_DIR=/data/backup/mongo/mongod_bak_list # 压缩时间为当前系统时间/删除时间为七天前 TAR_DATE=$(date +%F) DEL_DATE=$(date +%F -d \u0026#34;-7 day\u0026#34;) # 数据库配置 DB_HOST=ip:port DB_NAME==****** DB_AUTHSOURCE=admin DB_USERNAME=****** DB_PASSWORD=****** if [[ ! -d ${OUT_DIR} ]];then mkdir -p ${OUT_DIR} fi if [[ ! -d ${TAR_DIR} ]];then mkdir -p ${TAR_DIR} fi TAR_BAK=\u0026#34;mongo_bak_${TAR_DATE}.tar.gz\u0026#34; cd ${OUT_DIR} rm -rf ${OUT_DIR}/* ${MONGOD} -h ${DB_HOST} -u ${DB_USERNAME} -p ${DB_PASSWORD} --authenticationDatabase ${DB_AUTHSOURCE} -d ${DB_NAME} -o ${OUT_DIR} # 压缩归档 tar -zcvPf ${TAR_DIR}/${TAR_BAK} ${OUT_DIR} # 清除历史归档(七天前) for i in `find ${TAR_DIR} -maxdepth 1 \\( -type d -o -type l \\)`; do find -L $i -maxdepth 1 -type f \\( -name \u0026#34;*${DEL_DATE}*\u0026#34; -a -name \u0026#34;*.tar.gz\u0026#34; \\) -exec rm -f {} \\; done 其他：可以将添加crontab定时任务使用Shell编写，基于Jenkins控制发布，做到自动化运维与减少误操作。一般执行crontab -e命令都是直接往/var/spool/cron下创建一个文件，这个文件的名称就是你的当前用户名，内容就是你添加的任务具体内容。依据这一点可以做到自动化的crontab发布，Shell脚本大致如下：\ncrontab_reload(){ echo \u0026#34;30 0 * * * ${SCRIPT_DIR}/auto/crontab/mongo_back.sh\u0026#34; \u0026gt; /var/spool/cron/root # 重启crontab /sbin/service crond restart service crond status echo \u0026#34;get current crontab\u0026#34; crontab -l echo \u0026#34;crontab reload done\u0026#34; } 注意（echo \u0026gt; 是输出重定向，类似于insert，echo \u0026raquo; 是输出追加重定向，类似于append）\n"
            }
    
        ,
            {
                "id": 19,
                "href": "https://chinalhr.github.io/post/linux-logrotate/",
                "title": "Linux-logrotate详解",
                "section": "post",
                "date" : "2020.01.31",
                "body": " logrotate学习记录\n 关于logrotate logrotate 是一个 linux 系统日志的管理工具。可以对单个日志文件或者某个目录下的文件按时间 / 大小进行切割，压缩操作；指定日志保存数量；还可以在切割之后运行自定义命令。\nlogrotate 是基于 crontab 运行的，查询crontab 的配置文件 /etc/anacrontab可以查询具体的配置。logrotate的配置文件，例如每天运行的在 /etc/cron.daily/logrotate配置文件里面。\nlogrotate运行机制 测试系统为Centos7 ，查询/etc/anacrontab\n1\t5\tcron.daily\tnice run-parts /etc/cron.daily 7\t25\tcron.weekly\tnice run-parts /etc/cron.weekly @monthly 45\tcron.monthly\tnice run-parts /etc/cron.monthly 会根据配置的周期运行/etc/cron.daily，/etc/cron.weekly和/etc/cron.monthly目录下的脚本文件，如cron.daily下的logrotate文件，内容如下：\n#!/bin/sh  /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \u0026#34;ALERT exited abnormally with [$EXITVALUE]\u0026#34; fi exit 0 这个脚本主要做的事就是以 /etc/logrotate.conf 为配置文件执行了 logrotate。就是这样实现了每天执行一次 logrotate。\nlogrotate.conf包含了/etc/logrotate.d目录下的配置文件，很多程序的会用到 logrotate 滚动日志，比如 nginx。它们安装后，会在 /etc/logrotate.d 这个目录下增加自己的 logrotate 的配置文件。logrotate.conf配置如下：\n# see \u0026#34;man logrotate\u0026#34; for details # rotate log files weekly weekly # keep 4 weeks worth of backlogs rotate 4 # create new (empty) log files after rotating old ones create # use date as a suffix of the rotated file dateext # uncomment this if you want your log files compressed #compress # RPM packages drop log rotation information into this directory include /etc/logrotate.d # no packages own wtmp and btmp -- we\u0026#39;ll rotate them here /var/log/wtmp { monthly create 0664 root utmp minsize 1M rotate 1 } /var/log/btmp { missingok monthly create 0600 root utmp rotate 1 } logrotate配置 logrotate运行命令 logrotate [OPTION...] \u0026lt;configfile\u0026gt; -d, --debug ：debug 模式，测试配置文件是否有错误。 -f, --force ：强制转储文件。 -m, --mail=command ：压缩日志后，发送日志到指定邮箱。 -s, --state=statefile ：使用指定的状态文件。 -v, --verbose ：显示转储过程。 例如，通过logrotate -f /etc/logrotate.d/nginx进行手动执行。\n配置参数 示例：\n/var/log/log_file { monthly rotate 5 compress delaycompress missingok notifempty create 644 root root postrotate /usr/bin/killall -HUP rsyslogd endscript } 常用参数说明：\n daily ：指定转储周期为每天 weekly ：指定转储周期为每周 monthly ：指定转储周期为每月 rotate count ：指定日志文件删除之前转储的次数，0 指没有备份，5 指保留 5 个备份 tabooext [+] list：让 logrotate 不转储指定扩展名的文件，缺省的扩展名是：.rpm-orig, .rpmsave, v, 和～ missingok：在日志轮循期间，任何错误将被忽略，例如 “文件无法找到” 之类的错误。 size size：当日志文件到达指定的大小时才转储，bytes (缺省) 及 KB (sizek) 或 MB (sizem) compress： 通过 gzip 压缩转储以后的日志 nocompress： 不压缩 copytruncate：用于还在打开中的日志文件，把当前日志备份并截断 nocopytruncate： 备份日志文件但是不截断 create mode owner group ： 转储文件，使用指定的文件模式创建新的日志文件 nocreate： 不建立新的日志文件 delaycompress： 和 compress 一起使用时，转储的日志文件到下一次转储时才压缩 nodelaycompress： 覆盖 delaycompress 选项，转储同时压缩。 errors address ： 专储时的错误信息发送到指定的 Email 地址 ifempty ：即使是空文件也转储，这个是 logrotate 的缺省选项。 notifempty ：如果是空文件的话，不转储 mail address ： 把转储的日志文件发送到指定的 E-mail 地址 nomail ： 转储时不发送日志文件 olddir directory：储后的日志文件放入指定的目录，必须和当前日志文件在同一个文件系统 noolddir： 转储后的日志文件和当前日志文件放在同一个目录下 prerotate/endscript： 在转储以前需要执行的命令可以放入这个对，这两个关键字必须单独成行  示例（10天周期，nginx日志压缩处理）：\n/var/log/nginx/*log { create 0664 nginx root daily rotate 10 missingok notifempty compress sharedscripts postrotate /bin/kill -USR1 `cat /run/nginx.pid 2\u0026gt;/dev/null` 2\u0026gt;/dev/null || true endscript } Shell脚本实现 如下，对java应用产生的日志进行压缩，清理操作：\n#-------------------------------------------- # 自动化日志每日压缩,清理 # gzip 压缩一天前日志 # 删除两天前的压缩文件 #-------------------------------------------- #! /bin/bash ZIPDATE=$(date +%F -d \u0026#34;-1 day\u0026#34;); DELDATE=$(date +%F -d \u0026#34;-2 day\u0026#34;); SECOND=$(echo $RANDOM | cut -c1-3) sleep $SECOND # 清理 /volume1/docker/java/log下hour目录下的日志, # type d和l(目录/链接文件) -o 相当于 ||, for i in `find /volume1/docker/java/log -maxdepth 2 \\( -type d -o -type l \\) -name hour`; do find -L $i -maxdepth 1 -type f \\( -name \u0026#34;*${ZIPDATE}*\u0026#34; -a ! -name \u0026#34;*.gz\u0026#34; \\) -exec gzip {} \\; find -L $i -maxdepth 1 -type f \\( -name \u0026#34;*${DELDATE}*\u0026#34; -a -name \u0026#34;*.gz\u0026#34; \\) -exec rm -f {} \\; done "
            }
    
        ,
            {
                "id": 20,
                "href": "https://chinalhr.github.io/post/agile-devops/",
                "title": "认知-敏捷开发与DevOps",
                "section": "post",
                "date" : "2020.01.13",
                "body": " 关于敏捷开发与DevOps实践——腾讯云公开课记录\n 软件工程—从瀑布到敏捷 软件工程从瀑布到敏捷，是对软件工程效率和软件交付效率的提升。\n瀑布模型 瀑布模型（Waterfall Model）将软件生命周期划分为6个阶段：计划、需求分析、设计、编码、测试、维护，顺序固定，如同瀑布逐级下落。作为早期软件工程方法，瀑布在20世纪80年广泛使用，但存在致命的缺点：流程是线性的，到最后才测试和交付开发成果，一旦发现问题为时已晚，所以没能很好的解决软件危机，2003年的统计报告显示82%的项目延期，和1995年的84%几乎没有好转。\n敏捷开发 敏捷开发是循序渐进的开发方式，在尽量短的周期内持续测试和交付“可运行的软件”，再加上团队沟通和客户沟通，从而做到了“拥抱变化”。在敏捷开发中，软件项目在构建初期被切分成多个迭代，各个迭代的成果都经过测试，具备可视、可集成和可运行使用的特征。\n敏捷开发框架：\nScrum（“橄榄球传球” ）：成员包括产品负责人、 5-9人的开发团队、 敏捷教练，采用每日站会、 迭代回顾等方法； Kanban（看板）：来自丰田精益生产，可视化工作流、 限制在制品数量、 显式化流程规则（内建质量）……； XP（极限编程）：结对编程、 TDD（测试驱动开发）、 客户驻场，偏重编程实践，而 Scrum 和 Kanban 面向工作流。\n**Scrum **：\n敏捷开发工作流与分工 敏捷开发之持续交付实现（DevOps） DevOps理念 核心是自动化\n权限管理 自动化上线 代码质量保证 "
            }
    
        ,
            {
                "id": 21,
                "href": "https://chinalhr.github.io/post/jenkins-java-ci-cd/",
                "title": "Jenkins-pipeline-Docker 实现CI/CD",
                "section": "post",
                "date" : "2019.12.29",
                "body": " 基于Jenkins和Docker，对于Java应用CI/CD实践的记录。\n 关于CI/CD 持续集成(CI) 持续集成开发人员能够频繁地将其代码集成到公共代码仓库的主分支中。 开发人员能够在任何时候多次向仓库提交作品，而不是独立地开发每个功能模块并在开发周期结束时一一提交。主要目的是尽早发现集成错误，使团队更加紧密结合，更好地协作。\n目的\n 减少集成的开销，如代码冲突 将集成简化成一个简单、易于重复的日常开发任务，降低总体的构建成本，及早发现缺陷  持续交付(CD) 持续交付(CD)是CI的扩展，指软件交付流程的进一步自动化，可以随时自动化地部署到生产环境。使用CD后，开发团队可以在日常开发的任何时间进行产品级的发布，而不需要详细的发布方案或者特殊的后期测试。\n持续交付与流水线\n CD 集中依赖于部署流水线，通过流水线自动化测试和部署过程 流水线包含了 构建-测试-部署 在流水线的每个阶段，如果无法构建通过或者关键测试失败会向团队发出警报，流水线的最后一个部分会将构建部署到和生产环境等效的环境中  持续部署(CD) 持续部署扩展了持续交付，以便软件构建在通过所有测试时自动部署。在这样的流程中， 不需要人为决定何时及如何投入生产环境。CI/CD 系统的最后一步将在构建后的组件/包退出流水线时自动部署。 此类自动部署可以配置为快速向客户分发组件、功能模块或修复补丁，并准确说明当前提供的内容。\n目的\n 将新功能快速传递给用户，得到用户对于新版本的快速反馈，并且可以迅速处理任何明显的缺陷。  jenkins安装/配置(基于Docker) 安装 # docker获取jenkins镜像 docker pull jenkins/jenkins # 启动Jenkins docker run -u root -itd --name jenkins \\ -p 10080:8080 \\ -v $(which docker):/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock \\ -e TZ=\u0026quot;Asia/Shanghai\u0026quot; -v /etc/localtime:/etc/localtime \\ -e JAVA_OPTS='-Xms1024m -Xmx1024m -XX:NewSize=512m' \\ -v /volume1/docker/jenkins:/var/jenkins_home jenkins/jenkins # -p 10080:8080 映射容器的8080端口到外部主机的10080端口 # -v $(which docker):/usr/bin/docker -v /var/run/docker.sock:/var/run/docker.sock 使jenkins内部可以使用docker命令 # -e TZ=\u0026quot;Asia/Shanghai\u0026quot; -v /etc/localtime:/etc/localtime 配置Jenkins容器的时区 # -v /volume1/docker/jenkins:/var/jenkins_home jenkins/jenkins 将Jenkins的配置映射到外部主机卷/volume1/docker/jenkins（删除容器后可保留） 插件推荐    插件名 作用     Blue Ocean Jenkins的一种新视图，能够通过图形化的界面创建和编辑Jenkinsfile，实现pipeline as code   Pipeline Maven Integration Plugin 在pipeline中集成maven，即可使用withMaven{}命令   Config File Provider Plugin 可创建并管理Maven的settings文件及其他配置文件   JUnit Attachments Plugin 可以对单元测试生成的测试结果在Jenkins中进行展示   HTTP Request Plugin 发送HTTP请求    配置  配置Git凭据   配置maven setting文件  需要安装插件Config File Provider Plugin\nJenkins Pipeline 什么是Pipline pipeline 是jenkins2.X 最核心的特性， 帮助jenkins 实现从CI 到 CD与 DevOps的转变。pipeline 是一套运行于jenkins上的工作流框架，将原本独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排与可视化。\npipeline优势  代码：pipeline 以代码的形式实现，通过被捡入源代码控制， 使团队能够编译，审查和迭代其cd流程 可连续性：jenkins 重启 或者中断后都不会影响pipeline job 停顿：pipeline 可以选择停止并等待人工输入或者批准，然后在继续pipeline运行 多功能：pipeline 支持现实世界的复杂CD要求， 包括fork、join子进程，循环和并行执行工作的能力 可扩展：pipeline 插件支持其DSL的自动扩展以及其插件集成的多个选项。  pipeline组成  基本结构  pipeline { agent { docker 'maven:3.3.3' } stages { stage('build') { steps { sh 'mvn --version' } } } post { always { echo 'This will always run' } success { echo 'This will run only if successful' } failure { echo 'This will run only if failed' } ... } }  agent  agent指定整个Pipeline或特定stage将在Jenkins环境中执行的位置，具体取决于该agent 部分的位置。该部分必须在pipeline块内的顶层定义，stage块内的agent是可选的。\n post  post定义将在Pipeline运行或stage结束时运行的操作。\n stage stages  包含一个或多个stage指令的序列，该stages部分是Pipeline 描述的大部分“工作”所在的位置。\n steps  steps部分定义了在给定stage指令中执行的一系列一个或多个步骤。\nJenkins Pipeline实现CI/CD 基本流程 架构图：\n基本流程：\n步骤：\n Jenkins拉取Git代码 静态代码分析 mavem构建项目 单元测试 build镜像 push镜像到镜像仓库 远程部署(完成下载镜像，执行镜像的命令)  Pipeline实现 Jenkins拉取Git代码 可以使用checkout插件进行Git代码拉取（利用pipeline-syntax生成pipeline流水线脚本）\n steps { //checkout到master分支进行代码拉取 checkout([$class: 'GitSCM', branches: [[name: 'master']], doGenerateSubmoduleConfigurations: false, gitTool: 'Default', extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '8f240564-XXX-40b7-9faf-c08167aa27a3', url: 'git@gitee.com:XXX/XXX.git']]]) } 适配git-flow分支模型，实现发布前进行merge操作和push代码操作。勾选参数化构建过程,选择Git Parameter,然后在pipeline实现相关参数。\nparameters { gitParameter branchFilter: \u0026#39;origin/(.*)\u0026#39;, defaultValue: \u0026#39;master\u0026#39;, name: \u0026#39;BRANCH\u0026#39;, type: \u0026#39;PT_BRANCH\u0026#39;, sortMode:\u0026#39;DESCENDING\u0026#39; } stages { stage(\u0026#39;Pull Source code\u0026#39;) { steps { //checkout到master 和 merge 指定分支操作  checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;master\u0026#39;]], doGenerateSubmoduleConfigurations: false, gitTool: \u0026#39;Default\u0026#39;, extensions: [ [$class: \u0026#39;PreBuildMerge\u0026#39;, options: [mergeRemote: \u0026#39;origin\u0026#39;, mergeStrategy: \u0026#39;RECURSIVE\u0026#39;, mergeTarget: \u0026#34;${params.BRANCH}\u0026#34;]] ], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;8f240564-XXX-40b7-9faf-c08167aa27a3\u0026#39;, url: \u0026#39;git@gitee.com:XXX/XXX.git\u0026#39;]]]) //代码推送到master操作(可以将此步骤下放到远程部署后或者post阶段，确保一个完整的成功的发布后才进行分支合并提交) \tsshagent([\u0026#39;8f240564-XXX-40b7-9faf-c08167aa27a3\u0026#39;]) { sh \u0026#34;\u0026#34;\u0026#34; git config --global user.email \u0026#34;******\u0026#34; git config --global user.name \u0026#34;******\u0026#34; git commit -m \u0026#34;header changed ci commit automatically\u0026#34; || true git push origin HEAD:refs/heads/master \u0026#34;\u0026#34;\u0026#34; } } } ... } mavem构建项目 stage(\u0026#39;Build\u0026#39;) { steps { withMaven(maven: \u0026#39;Maven3.6.3\u0026#39;, mavenSettingsConfig: \u0026#39;f6f97751-3c04-4394-876f-cee0879cfc02\u0026#39;) { //这里使用了maven-resources-plugin插件对资源进行统一的COPY到path目录  sh \u0026#39;mvn clean package -Dmaven.test.skip=true -Dbuild.path=/var/jenkins_home/build/bfreeman -f pom.xml\u0026#39; } } } 单元测试 简单的单元测试可以基于maven-shade-plugin插件，配合Junit跑单元测试。也可以 配合jacoco插件和 maven-jacoco-plugin 进行测试覆盖率的检测。\nstage(\u0026#39;Unit testing\u0026#39;) { steps { withMaven(maven: \u0026#39;Maven3.6.3\u0026#39;, mavenSettingsConfig: \u0026#39;f6f97751-3c04-4394-876f-cee0879cfc02\u0026#39;) { //这里使用了maven-shade-plugin插件进行单元测试  sh \u0026#39;mvn clean test\u0026#39; } } } build docker镜像  编写Dockerfile  FROMopenjdk:8u212-jdkCOPY /dev/* /build/conf/dev/COPY /production/* /build/conf/production/COPY api-1.0.0-fat.jar /build/api.jarENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;,\u0026#34;/build/api.jar\u0026#34;,\u0026#34;--vertx-id=\u0026#39;bfreeman-api\u0026#39;\u0026#34;,\u0026#34;--java-opts=\u0026#39;-Xms512m -Xmx512m -XX:NewSize=256m\u0026#39;\u0026#34;,\u0026#34;-conf /build/conf/production/conf.json\u0026#34;]大致为COPY jar和配置文件到指定目录，ENTRYPOINT指定JVM参数和vertx参数\n 执行docker build相关Shell脚本  #! /bin/bash  cd /var/jenkins_home/build/bfreeman docker build -t bfreeman/api:1.0.0 . push docker镜像到镜像仓库 这里使用的是阿里云容器镜像服务，参考文档\n相关Shell脚本如下\n#! /bin/bash  # 1. 获取构建的image id IMAGES_ID=`docker images|grep -i bfreeman/api|awk \u0026#39;{print $3}\u0026#39;` # 2. 设置repository相关信息 REPOSITORY=\u0026#34;bfreeman/web\u0026#34; TAG=\u0026#34;1.0.0\u0026#34; # 3. 登录Docker Registry ，使用docker tag命令重命名镜像并推送至登录Docker Registry docker login -u=xx -p=xx registry.cn-hangzhou.aliyuncs.com docker tag ${IMAGES_ID} registry.cn-hangzhou.aliyuncs.com/${REPOSITORY}:${TAG} docker push registry.cn-hangzhou.aliyuncs.com/${REPOSITORY}:${TAG} 远程部署(镜像Pull，重新部署)  jenkins pipeline sshagent实现远程执行shell脚本  生成ssh密钥对，并将私钥配置到jenkins的凭证中，将公钥配置到对应部署服务器的.ssh/authorized_keys文件中，pipeline脚本如下：\nsshagent([\u0026#39;feb51b70-****-45a5-a69b-319e41f7f146\u0026#39;]) { sh \u0026#34;\u0026#34;\u0026#34; ... \u0026#34;\u0026#34;\u0026#34; }  镜像Pull  相关Shell脚本如下\n#! /bin/bash  # 1. 设置repository相关信息 REPOSITORY=\u0026#34;bfreeman/web\u0026#34; TAG=\u0026#34;1.0.0\u0026#34; # 2. 登录Docker Registry ，从Docker Registry中拉取镜像 docker login -u=xx -p=xx registry.cn-hangzhou.aliyuncs.com docker pull registry.cn-hangzhou.aliyuncs.com/${REPOSITORY}:${TAG}  重新部署  相关Shell脚本如下\n#! /bin/bash  # 1. 设置repository相关信息 REPOSITORY=\u0026#34;bfreeman/web\u0026#34; TAG=\u0026#34;1.0.0\u0026#34; docker stop bfreeman-api docker container rm bfreeman-api docker run -u root -d --name bfreeman-api \\ -p 10010:10010 \\ -p 10030:10030 \\ -v /volume1/docker/java:/data registry.cn-hangzhou.aliyuncs.com/${REPOSITORY}:${TAG} 此处可以基于k8s进行改造，实现jenkins部署到k8s集群。\n相关pipeline pipeline { agent any environment { imageid = \u0026quot;\u0026quot; } parameters { gitParameter branchFilter: 'origin/(.*)', defaultValue: 'master', name: 'BRANCH', type: 'PT_BRANCH', sortMode:'DESCENDING' } stages { //拉取代码 stage('Pull Source Code') { steps { //checkout到master 和 merge 指定分支操作 checkout([$class: 'GitSCM', branches: [[name: 'master']], doGenerateSubmoduleConfigurations: false, extensions: [ [$class: 'PreBuildMerge', options: [mergeRemote: 'origin', mergeStrategy: 'RECURSIVE', mergeTarget: \u0026quot;${params.BRANCH}\u0026quot;]] ], submoduleCfg: [], gitTool: 'Default', userRemoteConfigs: [[credentialsId: '0baad767-754c-****-8639-be2559a124ac', url: 'git@gitee.com:****/****.git']]] ) } } //代码静态分析 stage('Static Analysis') { steps { echo 'Program Static Analysis' } } //构建代码 stage('Build') { steps { withMaven(maven: 'Maven3.6.3', mavenSettingsConfig: 'f6f97751-****-4394-876f-cee0879cfc02') { sh 'mvn clean package -Dmaven.test.skip=true -Dbuild.path=/var/jenkins_home/build/bfreeman -f pom.xml' } } } //单元测试 stage('Unit testing') { steps { withMaven(maven: 'Maven3.6.3', mavenSettingsConfig: 'f6f97751-****-4394-876f-cee0879cfc02') { sh 'mvn clean test' } } } //构建Docker镜像 stage('Build Docker Image') { steps { sh \u0026quot;\u0026quot;\u0026quot; cd /var/jenkins_home/script/auto/deploy sh docker_build.sh \u0026quot;\u0026quot;\u0026quot; } } //推送Docker镜像到远程仓库 stage('Push Remote Repositry') { steps { sh \u0026quot;\u0026quot;\u0026quot; cd /var/jenkins_home/script/auto/deploy sh docker_push.sh \u0026quot;\u0026quot;\u0026quot; } } //部署代码 stage('Deploy') { steps { sshagent(['feb51b70-****-45a5-a69b-319e41f7f146']) { sh \u0026quot;\u0026quot;\u0026quot; ssh -p 16022 root@**** \u0026quot;sh /home/bfreeman/script/auto/deploy/docker_pull.sh\u0026quot; ssh -p 16022 root@**** \u0026quot;sh /home/bfreeman/script/auto/deploy/docker_run.sh\u0026quot; \u0026quot;\u0026quot;\u0026quot; } } } //推送代码 stage('Push Code') { steps { //代码推送到master操作 sshagent(['0baad767-****-4122-8639-be2559a124ac']) { sh \u0026quot;\u0026quot;\u0026quot; git config --global user.email \u0026quot;******\u0026quot; git config --global user.name \u0026quot;******\u0026quot; git commit -m \u0026quot;header changed ci commit automatically\u0026quot; || true git push origin HEAD:refs/heads/master \u0026quot;\u0026quot;\u0026quot; } } } } } 参考 https://jenkins.io/zh/doc/\nhttps://www.jianshu.com/nb/30544461\n"
            }
    
        ,
            {
                "id": 22,
                "href": "https://chinalhr.github.io/post/git-flow/",
                "title": "Git Flow 分支策略",
                "section": "post",
                "date" : "2019.11.26",
                "body": " Git-flow分支策略相关整理\n Git Flow Git Flow是由Vincent Driessen基于Git这一版本控制系统所总结出来的一套可行的、能够满足很多项目开发需求（注意：不是大多数项目）的敏捷开发流程。\n 分支模型图  Git Flow分支  master分支  master分支上是最近发布到生产环境的代码，最近发布的可用的release，仅可以从release分支和hotfix分支合并，不可随意修改\n develop分支  develop分支是主开发分支，合并自feature，包含所有要发布到下一个Release的代码（一般在提测阶段使用，提测结束后发往release分支）\n feature分支  develop分支是一个特性分支，用来开发一个新的功能，一旦开发完成，我们合并到develop分支进入下一个release\n release分支  release为版本分支，发布一个新release的时候，我们基于develop分支创建一个release分支，完成release后，合并到master和develop分支(一般在提测完成后，release为灰度环境版本，当灰度完成后合并到master和develop)\n hotfix分支  当在线上环境发现bug后，我们需要创建一个hotfix分支, 完成bug fix后，会合并回master和develop分支，所以hotfix的改动会进入下一个release。\n基于Git Flow分支模型的简单实践(代码示例)  特性分支  # 创建特性分支 feature-x\rgit checkout -b feature-x\r# 完成特性分支 feature-x 合并到develop进行提测\rgit checkout develop\rgit merge --no-ff feature-x\rgit push origin develop\r## -- 可选(删除feature特性分支)\rgit branch -d feature-x\rgit push origin --delete feature-x\r# 提测完成，develop合并到release进行灰度\rgit checkout -b release-0.1.0 develop\r# 灰度完成，release合并到master，并打上tag，发布线上\rgit checkout master\rgit merge --no-ff release-0.1.0\rgit push\rgit checkout develop\rgit merge --no-ff release-0.1.0\rgit push\rgit tag -a v0.1.0 master\rgit push --tags\r## -- 可选（删除release分支）\rgit branch -d release-0.1.0\rgit push origin --delete release-0.1.0\r hotfix  # 创建hotfix分支进行修复\rgit checkout -b hotfix-0.1.1 master\r# 合并到develop分支进行提测\rgit checkout develop\rgit merge --no-ff hotfix-0.1.1\rgit push\r# 提测完成后合并master，发布线上\rgit checkout master\rgit merge --no-ff hotfix-0.1.1\rgit push\rgit tag -a v0.1.1 master\rgit push --tags\r## -- 可选（删除hotfix分支）\rgit branch -d hotfix-0.1.1\rgit push origin --delete hotfix-0.1.1\r很多时候标准方案并不是最好的，选择合适的最重要\nGit Flow工具 https://www.sourcetreeapp.com/\n"
            }
    
        ,
            {
                "id": 23,
                "href": "https://chinalhr.github.io/post/linux-security-permission/",
                "title": "Linux用户-权限",
                "section": "post",
                "date" : "2019.10.23",
                "body": " 读《Linux命令行与shell脚本编程大全》记录\n 安全性  /etc/passwd  Linux系统使用/etc/passwd文件来将用户的登录名匹配到对应的UID值。root管理员账户固定分配给它的UID是0。\n/etc/passwd包含的信息\n登录用户名 用户密码（x显示） 用户账户的UID（数字形式） 用户账户的组ID（GID）（数字形式） 用户账户的文本描述（称为备注字段） 用户HOME目录的位置 用户的默认shell 例如 root:x:0:0:root:/root:/bin/bash  /etc/shadow  Linux系统使用/etc/shadow文件对Linux系统密码管理提供了更多的控制。只有root用户才能访问/etc/shadow文件。\n/etc/shadow包含的信息\n与/etc/passwd文件中的登录名字段对应的登录名 加密后的密码 自上次修改密码后过去的天数密码（自1970年1月1日开始计算） 多少天后才能更改密码 多少天后必须更改密码 密码过期前提前多少天提醒用户更改密码 密码过期后多少天禁用用户账户 用户账户被禁用的日期（用自1970年1月1日到当天的天数表示） 预留字段给将来使用 例如 rich:$1$.FfcK0ns$f1UgiyHQ25wrB/hykCn020:11627:0:99999:7::: 用户 添加用户 useradd [选项][用户名] -c comment 给新用户添加备注 -d home_dir 为主目录指定一个名字（如果不想用登录名作为主目录名的话） -e expire_date 用YYYY-MM-DD格式指定一个账户过期的日期 -f inactive_days 指定这个账户密码过期后多少天这个账户被禁用； 0表示密码一过期就立即禁用， 1表示 禁用这个功能 -g initial_group 指定用户登录组的GID或组名 -G group ... 指定用户除登录组之外所属的一个或多个附加组 -k 必须和-m一起使用，将/etc/skel目录的内容复制到用户的HOME目录 -m 创建用户的HOME目录 -M 不创建用户的HOME目录（当默认设置里要求创建时才使用这个选项） -n 创建一个与用户登录名同名的新组 -r 创建系统账户 -p passwd 为用户账户指定默认密码 -s shell 指定默认的登录shell -u uid 为账户指定唯一的UID' # 更改默认参数 -b default_home 更改默认的创建用户HOME目录的位置 -e expiration_date 更改默认的新账户的过期日期 -f inactive 更改默认的新用户从密码过期到账户被禁用的天数 -g group 更改默认的组名称或GID -s shell 更改默认的登录shell useradd -m test 在创建新用户时，如果你不在命令行中指定具体的值， useradd命令就会使用-D选项所显示的那些默认值。\n删除用户 userdel [选项][用户名] -r 把用户的主目录一起删除 修改用户 linux提供了修改已有用户账户信息的工具。\n   命令 描述     usermod 修改用户账户的字段，还可以指定主要组以及附加组的所属关系   passwd 修改已有用户的密码   chpasswd 从文件中读取登录名密码对，并更新密码   chage 修改密码的过期日期   chfn 修改用户账户的备注信息   chsh 修改用户账户的默认登录shell    usermod用来修改/etc/passwd文件中的大部分字段\nusermod[选项][用户名] 参数参考useradd，新增改为修改 -l修改用户账户的登录名。 -L锁定账户，使用户无法登录。 -p修改账户的密码。 -U解除锁定，使用户能够登录 passwd\npasswd [选项][用户名] -l 锁定口令，即禁用账号。 -u 口令解锁。 -d 使账号无口令。 -f 强迫用户下次登录时修改口令。 # 修改test用户的密码 passwd test New password:******* Re-enter new password:******* 用户组 组权限允许多个用户对系统中的对象（比如文件、目录或设备等）共享一组共用的权限。\n/etc/group文件包含系统上用到的每个组的信息。\n组名:组密码:GID:属于该组的用户列表 root:x:0:root bin:x:1:root,bin,daemon daemon:x:2:root,bin,daemon sys:x:3:root,bin,adm adm:x:4:root,adm,daemon rich:x:500: mama:x:501: katie:x:502: jessica:x:503: mysql:x:27: test:x:504: 创建组 groupadd[选项][用户组] -g GID 指定新用户组的组标识号（GID）。 -o 一般与-g选项同时使用，表示新用户组的GID可以与系统已有用户组的GID相同。 # 创建组test groupadd test # usermod将用户添加进组 usermod -G test lhr 修改组 groupmod[选项][用户组] -g 修改已有组的GID -n 修改已有组的组名 # 修改 test 组为 tester groupmod -n tester test 删除组 groupdel[用户组] groupdel test 权限 文件权限符 drwxrwxr-x 2 rich rich 4096 2010-09-03 15:12 test1 第一个字符代表了对象的类型: -代表文件 d代表目录 l代表链接 c代表字符型设备 b代表块设备 n代表网络设备 后面3组三字符的编码，定义了3种访问权 r代表对象是可读的 w代表对象是可写的 x代表对象是可执行的 3组权限分别对应对象的3个安全级别 权限掩码 八进制模式的安全性设置先获取这3个rwx权限的值，然后将其转换成3位二进制值，用一个八进制值来表示。在这个二进制表示中，每个位置代表一个二进制位。因此，如果读权限是唯一 置位的权限，权限值就是r\u0026ndash;，转换成二进制值就是100，代表的八进制值是4。\n umask指定在建立文件时预设的权限掩码  umask [-S][权限掩码] 更改权限  命令  chmod options mode file # 通过权限掩码更改权限 chmod 760 newfile # 通过符号模式更改权限 [ugoa…][+-=][rwxXstugo…] [ugoa…]u代表用户g代表组o代表其他a代表上述所有 [+-=]在现有权限基础上增加/减少权限 [rwxXstugo…] X:如果对象是目录或者它已有执行权限，赋予执行权限。 s：运行时重新设置UID或GID。 t：保留文件或目录。 u：将权限设置为跟属主一样。 g：将权限设置为跟属组一样。 o：将权限设置为跟其他用户一样 chmod o+r newfile 更改所属关系 chown命令用来改变文件的属主，chgrp命令用来改变文件的默认属组。\nchown options owner[.group] file # 改变文件属主为dan chown dan newfile # 改变文件的默认属组为tester chgrp tester newfile "
            }
    
        ,
            {
                "id": 24,
                "href": "https://chinalhr.github.io/post/mongodb-user-auth/",
                "title": "MongoDB用户-权限",
                "section": "post",
                "date" : "2019.10.18",
                "body": " MongoDB用户/权限相关知识点整理\n 创建管理员账号(用于用户管理的账号) use admin db.createUser( { user: \u0026quot;admin\u0026quot;, pwd: \u0026quot;123456\u0026quot;, roles: [ { role: \u0026quot;userAdminAnyDatabase\u0026quot;, db: \u0026quot;admin\u0026quot;} ] } ) 配置权限认证/以认证的方式连接 security: authorization: enabled mongo -port 27017 -u \u0026quot;admin\u0026quot; -p \u0026quot;123456\u0026quot; --authenticationDatabase \u0026quot;admin\u0026quot; 为数据库创建账号并指定权限 db.createUser( { user : \u0026quot;my_tester\u0026quot;, pwd : \u0026quot;123456\u0026quot;, roles: [ { role : \u0026quot;readWrite\u0026quot;, db : \u0026quot;test\u0026quot; } , { role : \u0026quot;read\u0026quot;, db : \u0026quot;test2\u0026quot; } ] } ) 权限说明 # 数据库用户角色 read: 只读数据权限 readWrite:读写数据权限 # 数据库管理角色 dbAdmin: 在当前db中执行管理操作的权限 dbOwner: 在当前db中执行任意操作 userADmin: 在当前db中管理user的权限 # 备份和还原角色 backup restore # 跨库角色 readAnyDatabase: 在所有数据库上都有读取数据的权限 readWriteAnyDatabase: 在所有数据库上都有读写数据的权限 userAdminAnyDatabase: 在所有数据库上都有管理user的权限 dbAdminAnyDatabase: 管理所有数据库的权限 # 集群管理 clusterAdmin: 管理机器的最高权限 clusterManager: 管理和监控集群的权限 clusterMonitor: 监控集群的权限 hostManager: 管理Server # 超级权限 root: 超级用户 "
            }
    
        ,
            {
                "id": 25,
                "href": "https://chinalhr.github.io/post/mongodb-conf/",
                "title": "MongoDB配置相关",
                "section": "post",
                "date" : "2019.10.16",
                "body": " MongoDB配置相关知识点整理\n 两种MongoDB的启动方式 1.直接启动，配置参数写在命令中 mongod \u0026ndash;dbpath=data/db \u0026ndash;logpath=log/log.log \u0026ndash;fork 2.以配置文件启动 mongod -f /etc/mongod.conf或者mongod \u0026ndash;config /etc/mongod.conf(一般都采用这种方式)\nMongoDB的配置文件格式使用了YAML格式,mongod.conf的几个大块  配置模块  systemLog: #日志 storage: #存储 processManagement: #进程管理 net: #网络 security: #安全 operationProfiling: #性能分析器 replication: #主从复制 sharding: #架构 setParameter: #自定义变量 auditLog: #检测日志 snmp: #  单机常用配置  dbpath=/var/lib/mongodb logpath=/var/log/mongodb/mongodb.log pidfilepath=/var/log/mongodb/master.pid directoryperdb=true logappend=true bind_ip=127.0.0.1 port=27017 fork=true  集群常用配置  dbpath=/var/lib/mongodb logpath=/var/log/mongodb/mongodb.log pidfilepath=/var/log/mongodb/master.pid directoryperdb=true logappend=true replSet=name bind_ip=127.0.0.1 port=27017 fork=true noprealloc=true MongoDB的配置文件模块  systemLog(日志相关参数)  systemLog: verbosity: \u0026lt;int\u0026gt; #日志级别，默认0,1-5均会包含debug信息 quiet: \u0026lt;boolean\u0026gt; #安静，true时mongod将会减少日志的输出量 traceAllExceptions: \u0026lt;boolean\u0026gt; #打印异常详细信息 syslogFacility: \u0026lt;string\u0026gt; #指定用于登录时信息到syslog Facility水平，前提是启用syslog path: \u0026lt;string\u0026gt; #日志路径，默认情况下，MongoDB将覆盖现有的日志文件 logAppend: \u0026lt;boolean\u0026gt; #mongod重启后，在现有日志后继续添加日志，否则备份当前日志，然后创建新日志 默认false logRotate: rename|reopen #日志轮询，防止一个日志文件特别大。rename重命名日志文件，默认值；reopen使用Linuxrotate特性，关闭并重新打开日志文件，前提为logAppend: 默认true destination: \u0026lt;string\u0026gt; #日志输出目的地，可为file或syslog，若不指定，则会输出到 std out timeStampFormat: \u0026lt;string\u0026gt; #指定日志格式的时间戳，有 ctime, Iso869-utc, iso8691-local component: #为不同的组件指定各自的日志信息级别 accessControl: verbosity: \u0026lt;int\u0026gt; command: verbosity: \u0026lt;int\u0026gt;  storage  storage: dbPath: \u0026lt;string\u0026gt; #mongodb进程存储数据目录，此配置进队此mongod进程有效，你使用配置文件开启的mongod就可以指定额外的数据目录 indexBuildRetry: \u0026lt;boolean\u0026gt; #当构件索引时mongod意外关闭，那么在此启动是否重建索引，默认true repairPath: \u0026lt;string\u0026gt; #在repair期间使用此目录存储临时数据，repair结束后此目录下数据将被删除 journal: enabled: \u0026lt;boolean\u0026gt; #journal日志持久存储，journal日志用来数据恢复，通常用于故障恢复，建议开启 commitIntervalMs: \u0026lt;num\u0026gt; #mongod日志刷新值，范围1-500毫秒，默认100，不建议修改 directoryPerDB: \u0026lt;boolean\u0026gt; #是否将不同的数据存储在不同的目录中，dbPath子目录 syncPeriodSecs: \u0026lt;int\u0026gt; #fsync操作将数据flush到磁盘的时间间隔，默认为60秒，不建议修改 engine: \u0026lt;string\u0026gt; #存储引擎 mmapv1: #mmapv1存储引擎，3.2前默认 preallocDataFiles: \u0026lt;boolean\u0026gt; nsSize: \u0026lt;int\u0026gt; quota: enforced: \u0026lt;boolean\u0026gt; maxFilesPerDB: \u0026lt;int\u0026gt; smallFiles: \u0026lt;boolean\u0026gt; journal: debugFlags: \u0026lt;int\u0026gt; commitIntervalMs: \u0026lt;num\u0026gt; wiredTiger: #WiredTiger存储引擎，3.2后默认 engineConfig: cacheSizeGB: \u0026lt;number\u0026gt; #最大缓存大小 journalCompressor: \u0026lt;string\u0026gt; #日志压缩算法，可选值有 none，snappy(默认)，zlib directoryForIndexes: \u0026lt;boolean\u0026gt; #是否将索引和collections数据分别存储在dbPath单独的目录中 collectionConfig: blockCompressor: \u0026lt;string\u0026gt; #collection数据压缩算法，可选none, snappy，zlib indexConfig: prefixCompression: \u0026lt;boolean\u0026gt; #是否对索引数据使用前缀压缩。对那些经过排序的值存储有很大帮助，可有效减少索引数据的内存使用量。 inMemory: #inMemory内存存储引擎，bate版 engineConfig: inMemorySizeGB: \u0026lt;number\u0026gt;  processManagement(进程相关参数)  processManagement: fork: \u0026lt;boolean\u0026gt; #是否以fork模式运行mongod进程，默认情况下，mongod不作为守护进程运行 pidFilePath: \u0026lt;string\u0026gt; #将mongod进程ID写入指定文件，如未指定，将不会创建PID文件  net(网络相关参数)  net: prot: \u0026lt;int\u0026gt; #监听端口，默认27017 bindIp: \u0026lt;string\u0026gt; #绑定IP，如果此值是“0.0.0.0”则绑定所有接口,默认配置为127.0.0.1,若不限制IP，务必确保认证安全，多个Ip用逗号分隔 maxIncomingConnections: \u0026lt;int\u0026gt; #mongod进程允许的最大连接数，如果此值超过系统配置的连接数阈值，将不会生效(ulimit) wireObjectCheck: \u0026lt;boolean\u0026gt; #当客户端写入数据时，检查数据的有效性（BSON）。如果数据格式不良，update,insert等操作将会被拒绝 ipv6: \u0026lt;boolean\u0026gt; #是否支持多实例之间使用ipv6 unixDomainSocker: #适用于Unix系统 enabled: \u0026lt;boolean\u0026gt;\t#默认true pathPrefix: \u0026lt;string\u0026gt;\t#路径前缀，默认/temp filePermissions: \u0026lt;int\u0026gt;\t#文件权限 默认0700 http: #警告 确保生产环境禁用HTTP status接口、REST API以及JSON API以防止数据暴露和漏洞攻击 enabled: \u0026lt;boolean\u0026gt;\t#是否启用HTTP接口、启用会增加网络暴露。3.2版本后停止使用HTTP interface JSONEnabled: \u0026lt;boolean\u0026gt;\t#JSONP的HTTP接口 RESTInterfaceEnabled: \u0026lt;boolean\u0026gt;\t#REST API接口 ssl: sslOnNormalPorts: \u0026lt;boolean\u0026gt; mode: \u0026lt;string\u0026gt; PEMKeyFile: \u0026lt;string\u0026gt; PEMKeyPassword: \u0026lt;string\u0026gt; clusterFile: \u0026lt;string\u0026gt; clusterPassword: \u0026lt;string\u0026gt; CAFile: \u0026lt;string\u0026gt; CRLFile: \u0026lt;string\u0026gt; allowConnectionsWithoutCertificates: \u0026lt;boolean\u0026gt; allowInvalidCertificates: \u0026lt;boolean\u0026gt; allowInvalidHostnames: \u0026lt;boolean\u0026gt; disabledProtocols: \u0026lt;string\u0026gt; FIPSMode: \u0026lt;boolean\u0026gt; compression: compressors: \u0026lt;string\u0026gt;  security(安全相关参数)  security: authorization: enabled #enabled/disabled #开启客户端认证 keyFile: /path/mongo.key #MongoDB副本集节点身份验证密钥文件 clusterAuthMode: \u0026lt;string\u0026gt; #集群members间的认证模式 transitionToAuth: \u0026lt;boolean\u0026gt; javascriptEnabled: \u0026lt;boolean\u0026gt; #启用或禁用服务器端JavaScript执行 redactClientLogData: \u0026lt;boolean\u0026gt; sasl: hostName: \u0026lt;string\u0026gt; serviceName: \u0026lt;string\u0026gt; saslauthdSocketPath: \u0026lt;string\u0026gt; enableEncryption: \u0026lt;boolean\u0026gt; encryptionCipherMode: \u0026lt;string\u0026gt; encryptionKeyFile: \u0026lt;string\u0026gt; kmip: keyIdentifier: \u0026lt;string\u0026gt; rotateMasterKey: \u0026lt;boolean\u0026gt; serverName: \u0026lt;string\u0026gt; port: \u0026lt;string\u0026gt; clientCertificateFile: \u0026lt;string\u0026gt; clientCertificatePassword: \u0026lt;string\u0026gt; serverCAFile: \u0026lt;string\u0026gt; ldap: servers: \u0026lt;string\u0026gt; bind: method: \u0026lt;string\u0026gt; saslMechanism: \u0026lt;string\u0026gt; queryUser: \u0026lt;string\u0026gt; queryPassword: \u0026lt;string\u0026gt; useOSDefaults: \u0026lt;boolean\u0026gt; transportSecurity: \u0026lt;string\u0026gt; timeoutMS: \u0026lt;int\u0026gt; userToDNMapping: \u0026lt;string\u0026gt; authz: queryTemplate: \u0026lt;string\u0026gt;  operationProfiling(慢查询相关)  operationProfiling: slowOpThresholdMs: \u0026lt;int\u0026gt; #数据库profiler判定一个操作是“慢查询”的时间阈值，单位毫秒。mongod会把慢查询记录到日志中，默认100ms mode: \u0026lt;string\u0026gt; #数据库profiler级别，操作的性能信息将会被写入日志文件中，可选值“off”--关闭profiling，“slowOp”--只包包含慢操作，“all”--记录所有操作 #数据库profiling会影响性能，建议只在性能调试阶段开启  replication(副本集)  replication: oplogSizeMB: \u0026lt;int\u0026gt; #replication操作日志的最大尺寸，如果太小，secondary将不能通过oplog来同步数据，只能全量同步 replSetName: \u0026lt;string\u0026gt; #副本集名称，副本集中所有的mongod实例都必须有相同的名字，Sharding分布式下，不同的sharding应该使用不同的repSetName secondaryIndexPrefetch: \u0026lt;string\u0026gt; #副本集中的secondary，从oplog中应用变更操作之前，将会先把索引加载到内存 enalbeMajorityReadConcern: \u0026lt;boolean\u0026gt; #允许readConcern的级别为“majority”  sharding(分片相关参数)  sharding: clusterRole: \u0026lt;string\u0026gt; #在sharding集群中，此mongod实例可选的角色。configsvr,默认监听27019端口 和 shardsvr,默认监听27018端口 archiveMovedChunks: \u0026lt;boolean\u0026gt; #当chunks因为“负载均衡”而迁移到其他节点时，mongod是否将这些chunks归档，并保存在dbPath/movechunk目录下，mongod不会删除moveChunk下的文件  setParameter(自定义变量)  setParameter: \u0026lt;parameter1\u0026gt;: \u0026lt;value1\u0026gt; \u0026lt;parameter2\u0026gt;: \u0026lt;value2\u0026gt; enableLocalhostAuthBypass: false #例子  auditLog(审计相关参数)  auditLog: destination: \u0026lt;string\u0026gt; #指定审计记录的输出方式，有syslog, console, file format: \u0026lt;string\u0026gt; #输出格式，有JSON 和 BSON path: \u0026lt;string\u0026gt; #如果审计时间输入为文件，那么就需要指定文件完整路径及文件名 filter: \u0026lt;string\u0026gt; #过滤器，可限制审计系统记录的操作类型，该选项需要一个表单的查询文档的字符串表示形式 "
            }
    
        ,
            {
                "id": 26,
                "href": "https://chinalhr.github.io/post/seo-frontend/",
                "title": "SEO-前端优化实践",
                "section": "post",
                "date" : "2019.10.09",
                "body": " SEO与前端开发中的实践\n 关于SEO SEO由英文Search Engine Optimization缩写而来， 中文意译为“搜索引擎优化”。SEO是指从自然搜索结果获得网站流量的技术和过程，是在了解搜索引擎自然排名机制的基础上， 对网站进行内部及外部的调整优化， 改进网站在搜索引擎中的关键词自然排名， 获得更多流量， 从而达成网站销售及品牌建设的目标。\nSEO优化点  网站标题、关键字、描述 网站内容优化(原创/持续更新) 合理设置Robot.txt文件 生成对搜索引擎友好的网站地图 增加外链引用 网站结构布局优化 网页代码优化  网站标题、关键字、描述 对应HTML文档中head的title,meta name=\u0026quot;keywords\u0026quot;和meta name=\u0026quot;description\u0026rdquo;\n Title  网站标题，显示在浏览器最上边的文字，也是搜索结果页每条搜索结果的标题，标题中的关键词相对keywords、description权重都要高得多。\ntitle写法\n首页:网站名称 – 主做的几个词语或简单描述 分类页:分类名 – 网站名称 内容页:内容标题 – 分类名 – 网站名称  Keywords  网页关键词，用户不能直接在网页中看到，主要是把网页的内容用几个简明扼要的词来反馈给搜索引擎。写法就是直接提取几个对应网页相关且主要优化的几个关键词。(3~5个)\n Description  网页的描述，用一句完整通顺的话，对网页进行简要概括，用户在网页上看不到，但是在搜索结果页一般标题下面展现的内容就是description。\ndescription写法\n首页,分类页:根据网页主要展示的内容并穿插一些主要优化的关键词，整理成一句通顺、让用户有点击欲望的话 内容页:简介或者导语，200个字符以内 Robot.txt robots协议（也称爬虫协议、机器人协议等），“全称是网络爬虫扫除规范“（Robots Exclusion Protocol）。经过robots协议告诉搜索引擎哪些页面能够抓取，哪些页面不能够抓取。\n说明\nUser-agent: * 代表所有搜索引擎，后面的*号可以自定义为想要设置的搜索引擎（如Baiduspider） Disallow：/目录名/（作用：隐藏相应目录） allow：/目录名/（作用：允许相应目录） Disallow: /abc 后面没有“/”表示abc目录下的所有文件（包括子目录）不被蜘蛛抓取。 Disallow: /abc/ 后面有“/”表示abc目录不被蜘蛛抓取，但不包括子目录。 Robot.txt文件最后可以写上站点地图的地址，方便蜘蛛更全更快的索引你的站点。 robot.txt在线生成\n站点地图site map Sitemap 可方便网站管理员通知搜索引擎他们网站上有哪些可供抓取的网页。最简单的 Sitemap 形式，就是XML 文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新的时间、更改的频率以及相对于网站上其他网址的重要程度为何等），以便搜索引擎可以更加智能地抓取网站。\nSitemap分为三种格式：txt文本格式、xml格式、Sitemap索引格式。\nSitemap格式参考 Sitemap在线生成\n外链 外链全称外部链接，需要运用互联网上在别的网站平台发布自己的网站链接，目的就是提高内页的权重。\n类型包括：纯文本URL/带超链接的URL/锚文本URL(关键字加链接)\n网站结构布局优化 使用扁平化结构。网站的目录层级要尽可能少，中小型网站不要超过3级。\n  控制首页的链接数量（网站首页的权重最高）。保证链接的有效性，爬虫通过首页的连接到达内页，如果没有有效的链接就会直接影响网站的收录数量。中小企业网站首页的链接数量应该在100个以内。链接的性质可以是页面导航、锚链。\n  扁平化的目录层次。尽量让Spider跳转3次就可以到达网站的任意一个内页。\n  导航SEO优化。导航可以分为主导航和父导航。导航应该使用文字，如果为了用户体验采用图片导航，则应该为图片设置必要的alt和title属性。  代码SEO优化  title创建唯一且准确的网页标题，使用 meta 的 keywords 元数据来提炼网页重要关键字，以及 description 元数据准确总结网页内容。 使用语义化元素  正文标题要使用\u0026lt;h1\u0026gt;标记，副标题要使用\u0026lt;h2\u0026gt;。可以让“蜘蛛”知道这是很重要的内容。 使用 \u0026lt;em\u0026gt; 或 \u0026lt;strong\u0026gt; 来表示强调。 导航使用\u0026lt;nav\u0026gt;,通过使用ol、ul组成路径集合。  利用 中的 alt 属性alt 属性可以在图片未成功显示时候，使用文本来代替图片的呈现，使“蜘蛛”可以抓取到这个信息。此外它还可以解决浏览器禁用图像或屏幕阅读器解析等问题。 a标记要加上说明（title属性） 设置rel='nofollow\u0026rsquo; 忽略跟踪如果某个a的链接不需要跟踪，那么添加 rel='nofollow\u0026rsquo; 即可通知“蜘蛛”忽略跟踪。 尽量保证 HTML 的纯粹和高质量——结构（HTML）、表现（CSS）及行为（JavaScript）三者分离。 尽量少使用iframe框架。 "
            }
    
        ,
            {
                "id": 27,
                "href": "https://chinalhr.github.io/post/redis-rdb-aof/",
                "title": "Redis 持久化 RDB-AOF",
                "section": "post",
                "date" : "2019.09.06",
                "body": " Redis持久化机制 RDB AOF\n Redis持久化 Redis提供了对持久化的支持，我们可以选择不同的方式(RDB或者AOF)将数据从内存中保存到硬盘当中，使数据可以持久化保存。\nRDB RDB是一种快照存储持久化方式，具体就是将Redis某一时刻的内存数据保存到硬盘的文件当中，默认保存的文件名为dump.rdb，而在Redis服务器启动时，会重新加载dump.rdb文件的数据到内存当中恢复数据。\n开启RDB方式  save命令  同步Redis数据到磁盘上，当客户端向服务器发送save命令请求进行持久化时，服务器会阻塞save命令之后的其他客户端的请求，直到数据同步完成。\n bgsave命令  异步保存数据集到磁盘上，当客户端发服务发出bgsave命令时，Redis服务器主进程会forks一个子进程来数据同步问题，在将数据保存到rdb文件之后，子进程会退出。子进程进行IO写入操作时，主进程仍然可以接收其他请求（但forks子进程是同步的，所以forks子进程时会阻塞其他客户端请求）。\n 配置触发  redis.conf配置，达到触发条件时，会forks一个子进程进行数据同步。\n# 900s内至少达到一条写命令 save 900 1 # 300s内至少达至10条写命令 save 300 10 # 60s内至少达到10000条写命令 save 60 10000 RDB文件 RDB默认生成的文件名为dump.rdb,生成过程如下：\n1.生成临时rdb文件，并写入数据。 2.完成数据写入，用临时文代替代正式rdb文件。 3.删除原来的db文件。 RDB优劣势   优点\n 与AOF方式相比，通过rdb文件恢复数据比较快。 rdb文件非常紧凑，适合于数据备份。 通过RDB进行数据备，由于使用子进程生成，所以对Redis服务器性能影响较小。    缺点\n 服务器宕机有数据缺失的风险 save会造成Redis服务阻塞，gsave命令在forks子进程会消耗内存    AOF AOF(Append-only file)，AOF持久化方式会记录客户端对服务器的每一次写操作命令，并将这些写操作以Redis协议追加保存到以后缀为aof文件末尾，在Redis服务器重启时，会加载并运行aof文件的命令，以达到恢复数据的目的。\n配置 # 开启aof机制 appendonly yes # aof文件名 appendfilename \u0026quot;appendonly.aof\u0026quot; # 写入策略,always表示每个写操作都保存到aof文件中,也可以是everysec或no appendfsync always # 默认不重写aof文件 no-appendfsync-on-rewrite no # 保存目录 dir ~/redis/ 写入策略  always  客户端的每一个写操作都保存到aof文件当，这种策略很安全，但是每个写请求都有IO操作。\n everysec  appendfsync的默认写入策略，每秒写入一次aof文件，因此，最多可能会丢失1s的数据。\n no  Redis服务器不负责写入aof，而是交由操作系统来处理什么时候写入aof文件。更快，但也是最不安全的选择，不推荐使用。\nAOF文件重写 可以通过配置的no-appendfsync-on-rewrite方式或者客户端向服务器发送bgrewriteaof命令\nAOF文件修复  定时备份aof文件 使用redis-check-aof命令修复aof文件 重启Redis服务器，加载已经修复的aof文件，恢复数据  AOF优劣势   优点\n AOF只是追加日志文件，因此对服务器性能影响较小，速度比RDB要快，消耗的内存较少。    缺点\n AOF方式生成的日志文件太大，即使通过AFO重写，文件体积仍然很大。 恢复数据的速度比RDB慢。    注意\n  当RDB与AOF两种方式都开启时，Redis会优先使用AOF日志来恢复数据，因为AOF保存的文件比RDB文件更完整。\n"
            }
    
        ,
            {
                "id": 28,
                "href": "https://chinalhr.github.io/post/redis-transaction-watch/",
                "title": "Redis 事务-Watch",
                "section": "post",
                "date" : "2019.09.03",
                "body": " Redis事务/Watch机制相关知识点整理\n Redis事务  关于Redis事务  Redis transaction是一组命令的集合。 transaction命令和普通的Redis命令一样是Redis的最小执行单位，具有原子性（一个事物中的命令，要么都执行，要么都不执行）\n原理:先将属于一个事务的命令MULTI发送给Redis，MULTI 执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行， 而是被放到一个队列中,然后当EXEC命令被调用时，再让Redis依次执行队列里的命令。\n事务从执行到开始三个阶段：\n 开始事务 命令入队 执行事务  Redis保证一个事务中的所有命令要么都执行，要么都不执行。如果在发送EXEC命令前客户端断线了，则Redis会清空事务队列，事务中的所有命令都不会执行。 而一旦客户端发送了EXEC命令，所有的命令就都会被执行，即使此后客户端断线也没关系，因为Redis中已经记录了所有要执行的命令。\nRedis保证一个事务内的命令依次执行而不被其他命令插入。(避免多台机器同时发送命令导致并发问题)\n 和传统RDBS事务的区别  Redis的事务没有提供rollback功能，如果一个事务中的某个命令执行出错，例如语法错误，运行错误\n 事务相关命令  # MULTI 命令的执行标记着事务的开始(客户端从非事务状态切换到事务状态) # EXEC 命令服务器根据客户端所保存的事务队列，以先进先出（FIFO）的方式执行事务队列中的命令 #DISCARD 命令用于取消一个事务， 它清空客户端的整个事务队列， 然后将客户端从事务状态调整回非事务状态，返回字符串 OK 给客户端 redis \u0026gt; multi OK redis \u0026gt; incr foo QUEUED redis \u0026gt; set t1 1 QUEUED redis \u0026gt; exec  Watch  WATCH 命令用于在事务开始之前监视任意数量的键： 当调用 EXEC 命令执行事务时， 如果任意一个被监视的键已经被其他客户端修改了， 那么整个事务不再执行， 直接返回失败。\n Watch相关命令  redis\u0026gt; WATCH name OK redis\u0026gt; MULTI OK redis\u0026gt; SET name peter QUEUED redis\u0026gt; EXEC (nil) Watch命令实现  watched_keys字典  在每个代表数据库的 redis.h/redisDb 结构类型中， 都保存了一个 watched_keys 字典， 字典的键是这个数据库被监视的键， 而字典的值则是一个链表， 链表中保存了所有监视这个键的客户端。\nWATCH 命令的作用， 就是将当前客户端和要监视的键在 watched_keys 中进行关联。\n通过 watched_keys 字典， 如果程序想检查某个键是否被监视， 那么它只要检查字典中是否存在这个键即可； 如果程序要获取监视某个键的所有客户端， 那么只要取出键的值（一个链表）， 然后对链表进行遍历即可。\n watch触发  在任何对数据库键空间（key space）进行修改的命令成功执行之后 （比如 FLUSHDB 、 SET 、 DEL 、 LPUSH 、 SADD 、 ZREM ，诸如此类）， multi.c/touchWatchedKey 函数都会被调用 —— 它检查数据库的 watched_keys 字典， 看是否有客户端在监视已经被命令修改的键， 如果有的话， 程序将所有监视这个/这些被修改键的客户端的 REDIS_DIRTY_CAS 选项打开：\n当客户端发送 EXEC 命令、触发事务执行时， 服务器会对客户端的状态进行检查：\n如果客户端的 REDIS_DIRTY_CAS 选项已经被打开，那么说明被客户端监视的键至少有一个已经被修改了，事务的安全性已经被破坏。服务器会放弃执行这个事务，直接向客户端返回空回复，表示事务执行失败。 如果 REDIS_DIRTY_CAS 选项没有被打开，那么说明所有监视键都安全，服务器正式执行事务。\n1. client3对 key1 进行了修改 2. 所有监视 key1 的客户端(client2,client5,client6)的 REDIS_DIRTY_CAS 选项都会被打开 3. 当client2,client5,client6执行 EXEC 的时候， 它们的事务都会失败 参考 https://redisbook.readthedocs.io/en/latest/feature/transaction.html#durability\n"
            }
    
        ,
            {
                "id": 29,
                "href": "https://chinalhr.github.io/post/mongo-limit-sort-index/",
                "title": "MongoDB分页-排序-索引",
                "section": "post",
                "date" : "2019.09.02",
                "body": " MongoDB分页/排序/索引相关知识点整理\n 分页  limit offset分页方法  MonogDB实现分页\n入参 pageNum=1 pageSize=5 skip()方法来跳过指定数量的数据 limit()方法来读取指定数量的数据 利用skip(pageSize*(n-1)).limit(pageSize)实现分页 当pageNum=1 pageSize=5 db.getCollection('user').find().skip(0).limit(5) 当pageNum=2 pageSize=5 db.getCollection('user').find().skip(5).limit(5) 缺陷：会进行Table Scan,扫描全部文档再返回结果，数据量大的时候会出现慢查询问题。\n Seek Method分页方法  seek method方式翻页,只用find + limit，不再使用skip,在find中加了一个条件：上一批的最后一个document的_id(比较的基准字段可以是任何有序的字段，例如时间戳，该字段必须有Index才有意义)\n//第一页 db.users.find().limit(pageSize); //找到此页面中最后一个文档的ID为lastId last_id = ... //第二页 db.user.find({ \u0026quot;_id\u0026quot;:{\u0026quot;$gt\u0026quot;:\u0026quot;5d6c7b8304f4757af5636e81\u0026quot;} }).limit(10) //使用此页面中最后一个文档的ID更新lastId last_id = ... 缺陷：无法像传统分页一样进行跳页，总页数展示\n排序  ObjectId有序性  ObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是： - 前 4 个字节表示创建 unix 时间戳,格林尼治时间 UTC 时间，比北京时间晚了 8 个小时 - 接下来的 3 个字节是机器标识码 - 紧接的两个字节由进程 id 组成 PID - 最后三个字节是随机数 MongoDB的ObjectId随着时间而增加，如果是同一台机器的同一个进程生成的对象，是有序的。  降序  id降序，第一页是最大的，下一页的id比上一页的最后的id还小\ndb.user.find({ \u0026quot;_id\u0026quot;:{\u0026quot;$lt\u0026quot;:\u0026quot;5d6c7ba804f4757af5636e84\u0026quot;} }) .sort({\u0026quot;_id\u0026quot;:-1}) .limit(5)  升序  id升序， 第一页最小，下一页的id比上一页的最后一条记录id还大\ndb.user.find({ \u0026quot;_id\u0026quot;:{\u0026quot;$gt\u0026quot;:\u0026quot;5d6c7ba804f4757af5636e84\u0026quot;} }) .sort({\u0026quot;_id\u0026quot;:1}) .limit(5)  排序性能  MongoDB的sort和find函数，先find查询符合条件的结果，然后在结果集中排序。从2.6开始，sort只排序100M以内的数据，超过将会报错。可以通过设置allowDiskUse来允许排序大容量数据。有索引的排序会比没有索引的排序快，MongoDB官方推荐为需要排序的key建立索引。\n索引  单key索引  db.records.createIndex( { a: 1 } ) 单key的排序使用单key索引，索引又分升序(1)和降序(-1)，索引定义的排序方向以及逆转方向可以支持sort。对于上述单key索引a，可以支持sort({a:1})升序和sort({a:-1})降序。\n 多key索引  db.records.createIndex( { a: 1, b:-1 } ) 对于多字段排序使用复合(compound index)索引。复合多字段索引的顺序要和sort的字段一致才可以走索引。比如索引{a:1, b:1}, 可以支持sort({a:1, b:1})和逆序sort({a:-1, b:-1})， 但不支持sort({b:1, a:1})。\n复合索引支持sort同排序和逆序：索引{a:1, b:-1} 可以支持sort({a:1, b:-1}), 也可以支持sort({a:-1, b:1})\n复合索引可以前缀子集支持sort {a:1, b:1, c:1}相当于{ a: 1 } + { a: 1, b: 1 } + { a: 1, b: 1, c: 1 }\n 示例  "
            }
    
        ,
            {
                "id": 30,
                "href": "https://chinalhr.github.io/post/linux-zerocopy/",
                "title": "Linux Zero-copy(零拷贝)机制",
                "section": "post",
                "date" : "2019.08.23",
                "body": " Linux Zero-copy机制与Java NIO对Zero-copy应用\n 关于Zero-copy(零拷贝)  维基百科  零复制（英语：Zero-copy；也译零拷贝）技术是指计算机执行操作时，CPU不需要先将数据从某处内存复制到另一个特定区域。这种技术通常用于通过网络传输文件时节省CPU周期和内存带宽。\n 原理  操作系统某些组件（例如驱动程序、文件系统和网络协议栈）若采用零复制技术，则能极大地增强了特定应用程序的性能，并更有效地利用系统资源。通过使CPU得以完成其他而非将机器中的数据复制到另一处的任务，性能也得到了增强。另外，零复制操作减少了在用户空间与内核空间之间切换模式的次数。\n举例来说，如果要读取一个文件并通过网络发送它，传统方式下每个读/写周期都需要复制两次数据和切换两次上下文，而数据的复制都需要依靠CPU。通过零复制技术完成相同的操作，上下文切换减少到两次，并且不需要CPU复制数据。\n零复制协议对于网络链路容量接近或超过CPU处理能力的高速网络尤为重要。在这种网络下，CPU几乎将所有时间都花在复制要传送的数据上，因此将成为使通信速率低于链路容量的瓶颈。\nLinux I/O机制  I/O中断  用户进程需要读取磁盘数据，需要CPU中断，发起IO请求，每次的IO中断，都带来CPU的上下文切换。\n DMA(Direct Memory Access，直接内存存取)  DMA允许不同速度的硬件装置来沟通，而不需要依赖于CPU 的大量中断负载。DMA控制器接管了数据读写请求，减少CPU的负担。\n涉及流程：\n传统Socket传送数据  大部分Web应用服务器都通过文件读取数据然后将数据通过网络传输给其他的程序的方式。  File.read(fileDesc,tmp_buf,len); File.send(socket,tmp_buf,len);  数据从文件到socket的过程   用户态和内核态的切换过程   涉及过程  1.read()的调用引起了从用户态到内核态的切换，内部是通过sys_read()(或者类似的方法)发起对文件数据的读取。数据的第一次复制是通过DMA(直接内存访问)将磁盘上的数据复制到内核空间的缓冲区中。 2.数据从内核空间的缓冲区复制到用户空间的缓冲区后，read()方法也就返回了。此时内核态又切换回用户态，现在数据也已经复制到了用户地址空间的缓冲区中。 3.socket的send()方法的调用又引起了用户态到内核态的切换，第三次数据复制有将数据从用户空间缓冲区复制到了内核空间的缓冲区，这次数据被放在了不同于之前的内核缓冲区中，这个缓冲区与数据将要被传输到socket关联。 4.send()系统调用返回后，就产生了第四次用户态和内核态的切换。随着DMA单独异步的将数据从内核态的缓冲区中传输到协议引擎发送到网络上，有了第四次数据的复制。   问题\n 传统数据传送所消耗的成本：4次拷贝(两次是DMA copy，两次是CPU copy)，4次上下文切换。 大量的数据复制并不是真正需要的。可以消除一些重复，以减少开销并提高性能。 内核缓冲区做中介的引入虽然改善了进程的性能(中介缓冲区可以用来实现异步功能,当缓冲区数据满了之后再写上去减少系统调用次数)，当应用程序读取的数据比这个中介缓冲区的容量大很多的时候，数据就会在磁盘、内核空间、用户空间之间复制多次后才最终被传给应用程序。    改善\n 零拷贝技术(Zero-copy)，在OS层面优化，减少IO流程中不必要的拷贝。    Linux Zero-copy mmap(内存映射) tmp_buf = mmap(file, len); write(socket, tmp_buf, len); 核心:从磁盘加载的数据通过DMA拷贝存储在内核缓冲区中。 然后将应用程序缓冲区的页面映射到内核缓冲区，以便省略内核缓冲区和应用程序缓冲区之间的数据复制。\nread-send模型\n优化： 拷贝次数优化(1次cpu copy，2次DMA copy)。 上下文切换四次。\nsendfile 在内核版本2.1中，引入了sendfile系统调用，以简化网络和两个本地文件之间的数据传输。sendfile的引入不仅减少了数据复制，还减少了上下文切换。\nsendfile(socket, file, len); 在调用sendfile（）系统调用时，数据从磁盘中获取并通过DMA复制复制到内核缓冲区中。 然后将数据直接从内核缓冲区复制到套接字缓冲区。 将所有数据复制到套接字缓冲区后，sendfile（）系统调用将返回以指示从内核缓冲区到套接字缓冲区的数据传输完成。 然后，数据将被复制到网卡上的缓冲区并传输到网络。\nread-send模型\n优化:拷贝次数优化(1次CPU copy,2次DMA copy) 上下文切换两次\nSendfile With DMA Scatter/Gather Copy 在Linux内核2.4以后的版本中, linux内核对socket缓冲区描述符做了优化. 通过这次优化, sendFile系统调用可以在只复制kernel buffer的少量元信息的基础上, 把数据直接从kernel buffer 复制到网卡的buffer中去.从而避免了从\u0026quot;内核缓冲区\u0026quot;拷贝到\u0026quot;socket缓冲区\u0026quot;的这一次拷贝.\nread-send模型\n优化:拷贝次数优化 (0次cpu copy，2次DMA copy) 上下文切换两次\nsplice Linux内核2.6.17 支持splice。splice不需要在内核空间和用户空间之间复制数据。使用此方法时，数据从磁盘读取到OS内核缓冲区后，在内核缓冲区直接可将其转成内核空间其他数据buffer，而不需要拷贝到用户空间。与Sendfile With DMA Scatter/Gather Copy不同，splice（）不需要硬件支持。\n与sendFile的区别：sendfile是将磁盘数据加载到kernel buffer后，需要一次CPU copy,拷贝到socket buffer。splice不需要CPU copy\n优化:拷贝次数优化 (0次cpu copy，2次DMA copy) 上下文切换两次\nZero-copy机制对比 Java NIO 对Linux Zero-copy机制支持 NIO中的mmap MappedByteBuffer mappedByteBuffer = new RandomAccessFile(file, \u0026#34;r\u0026#34;) .getChannel() .map(FileChannel.MapMode.READ_ONLY, 0, 1024);  FileChannle.map(MapMode mode,long position, long size)  将Channel文件的某个区域直接映射到内存中。采用了操作系统中的内存映射方式，底层就是调用Linux mmap()实现的。\n将内核缓冲区的内存和用户缓冲区的内存做了一个地址映射。这种方式适合读取大文件，同时也能对文件内容进行更改，但是如果其后要通过SocketChannel发送，还是需要CPU进行数据的拷贝。\n 注意点  使用mmap文件映射，只有在JVM在full gc时才会进行释放内存。当close时，需要手动清除内存映射文件，可以反射调用sun.misc.Cleaner方法。\nNIO中的sendfile  FileChannel.transferTo(long position, long count,WritableByteChannel target)  transferTo方法直接将当前通道内容传输到另一个通道，没有涉及到Buffer的任何操作，NIO中 的Buffer是JVM堆或者堆外内存(操作系统内核空间的内存)。Linux系统中通过系统调用sendfile() 实现。\n Zero-copy实现文件复制  public void copyFile(File src, File dest) { try (FileChannel srcChannel = new FileInputStream(src).getChannel(); FileChannel destChannel = new FileInputStream(dest).getChannel()) { srcChannel.transferTo(0, srcChannel.size(), destChannel); } catch (IOException e) { e.printStackTrace(); } } "
            }
    
        ,
            {
                "id": 31,
                "href": "https://chinalhr.github.io/post/tail-recursion/",
                "title": "尾递归-Java-Kotlin",
                "section": "post",
                "date" : "2019.08.04",
                "body": " 尾递归与递归的对比，以及在Java与Kotlin中如何运用\n 关于尾递归  尾递归  尾递归，比线性递归多一个参数(这个参数是上一次调用函数得到的结果)。关键点在于，尾递归每次调用都在收集结果，避免了线性递归不收集结果只能依次展开消耗内存的坏处。\n线性递归创建栈内存累积而后计算收缩，尾递归只会占用恒量的内存(类似迭代)。\n 尾递归优化  尾递归优化主要是对栈内存空间的优化,从O(n)到O(1)。对于时间的优化是由于对空间的优化导致内存分配的工作减少所产生的, 是一个常数级别的优化。\n 函数式编程中的递归与迭代  在一些函数式编程语言(例如Scala)中是鼓励使用递归，而不是循环来解决问题。这是因为循环会引入中间变量,而函数范式强调的是无副作用，强调函数计算的纯粹性，每个函数的执行都是没有副作用的， 函数所有功能就是返回一个新的值，没有其他行为，尤其是不得修改外部变量的值。\n尾递归示例  递归实现阶乘  public static int factorialRecursion(final int number) { if (number == 1) return number; else return number * factorialRecursion(number - 1); } 当调用factorialRecursion(5)时 栈的情况: factorialRecursion(5) 5 * factorialRecursion(4) 5 * (4 * factorialRecursion(3)) 5 * (4 * (3 * factorialRecursion(2))) 5 * (4 * (3 * (2 * factorialRecursion(1)))) 5 * (4 * (3 * (2 * 1))) 5 * (4 * (3 * 2)) 5 * (4 * 6) 5 * 24 120 这里就是典型的线性递归创建栈内存累积而后计算收缩，从左到右，达到顶峰，再从右到左收缩。使用迭代只占据常量栈，使用更新栈而非 扩展栈进行计算，因此迭代相比线性递归使用的内存更少。 为什么会扩展：因为在没有递归到底之前，程序的中间变量会一直保存着，因此每一次递归都需要开辟一个新的栈空间来保存中间变量  尾递归实现阶乘   public static int factorialTailRecursion(final int factorial, final int number) { if (number == 1) return factorial; else return factorialTailRecursion(factorial * number, number - 1); } 当调用factorialTailRecursion(1,5)时: factorialTailRecursion(1, 5) factorialTailRecursion(5, 4) factorialTailRecursion(20, 3) factorialTailRecursion(60, 2) factorialTailRecursion(120, 1) 120 分析上面递归函数栈累积的原因就是在每次return的时候都会附带一个变量，因此只需要在return的时候不附带这个变量即可。尾递归使用一个参数来保存上一轮递归的结果，把变化的参数传递给递归函数的变量了。 尾递归通过每轮递归结束后刷新当前的栈空间，复用了栈，克服了线性递归栈内存累积而后计算收缩，存在栈溢出风险。 总结尾递归：return后面不附带任何变量的递归写法，递归发生在函数最尾部,我们称之为'尾递归'。 尾递归优化-Java Tail-Recursion 依赖于编译器对尾递归写法的优化，Javac对Tail-Recursion并没有做特定的优化，使用尾递归的写法，该栈溢出还是栈溢出。\nJavac为什么没有Tail-Recursion优化，参考下面的链接,可以得知原因:\n因为在JDK许多类中，有许多安全敏感方法依赖于计算JDK库代码和调用代码之间的堆栈帧来确定谁在调用它们。tail-recursion优化会改变堆栈上帧数，会破坏它并导致计算错误。因此JDK开发人员已经取代了这种机制。\nhttps://softwareengineering.stackexchange.com/questions/272061/why-doesnt-java-have-optimization-for-tail-recursion-at-all\n 使用Lambda优化尾递归  设计一个函数接口代替递归中的栈帧，利用Stream将递归转换为迭代，\n/** * @Author : lhr * @Date : 11:15 2019/8/3 * * 尾递归函数式接口 */ @FunctionalInterface public interface TailRecursion\u0026lt;T\u0026gt; { /** * 用于递归栈帧之间的连接,惰性求值 * @return 下一个递归栈帧 */ TailRecursion\u0026lt;T\u0026gt; apply(); /** * 判断当前递归是否结束 * @return 默认为false,因为正常的递归过程中都还未结束 */ default boolean isFinished(){ return false; } /** * 获得递归结果,只有在递归结束才能调用,这里默认给出异常,通过工具类的重写来获得值 * @return 递归最终结果 */ default T getResult() { throw new Error(\u0026#34;递归还没有结束,调用获得结果异常!\u0026#34;); } /** * 及早求值,执行者一系列的递归,因为栈帧只有一个,所以使用findFirst获得最终的栈帧,接着调用getResult方法获得最终递归值 * @return 及早求值,获得最终递归结果 */ default T invoke() { return Stream.iterate(this, TailRecursion::apply) .filter(TailRecursion::isFinished) .findFirst() .get() .getResult(); } } 设计一个对外统一的尾递归包装类，目的是达到可以复用的效果，包装递归方法 1怎样调用下次递归,2递归的终止条件\n/** * @Author : lhr * @Date : 11:59 2019/8/3 * * 使用尾递归的类,目的是对外统一方法 * * 调用下次递归/结束本轮递归 * */ public class TailInvoke { /** * 统一结构的方法，获取当前递归的下一个递归 * @param nextFrame * @param \u0026lt;T\u0026gt; * @return */ public static \u0026lt;T\u0026gt; TailRecursion\u0026lt;T\u0026gt; call(final TailRecursion\u0026lt;T\u0026gt; nextFrame) { return nextFrame; } /** * 结束当前递归，重写对应的默认方法的值,完成状态改为true,设置最终返回结果,设置非法递归调用 * * @param value 最终递归值 * @param \u0026lt;T\u0026gt; T * @return 一个isFinished状态true的尾递归, 外部通过调用接口的invoke方法及早求值, 启动递归求值。 */ public static \u0026lt;T\u0026gt; TailRecursion\u0026lt;T\u0026gt; done(T value) { return new TailRecursion\u0026lt;T\u0026gt;() { @Override public TailRecursion\u0026lt;T\u0026gt; apply() { throw new Error(\u0026#34;递归已经结束,非法调用apply方法\u0026#34;); } @Override public boolean isFinished() { return true; } @Override public T getResult() { return value; } }; } } lamdba优化尾递归 阶乘计算示例\n/** * 阶乘计算 -- 使用尾递归接口完成 * * @param factorial 当前递归栈的结果值 * @param number 下一个递归需要计算的值 * @return 尾递归接口, 调用invoke启动及早求值获得结果 */ public static TailRecursion\u0026lt;Long\u0026gt; factorialTailRecursion(final long factorial, final int number) { if (number == 1) return TailInvoke.done(factorial); else return TailInvoke.call(() -\u0026gt; factorialTailRecursion(factorial + number, number - 1)); } public static void main(String[] args) { //调用Invoke启动迭代并获取结果  factorialTailRecursion(1, 10000000).invoke(); } 尾递归优化-Kotlin  tailrec关键字  Kotlin 支持尾递归，这允许一些通常用循环写的算法改用递归函数来写，而无堆栈溢出的风险。 当一个函数用 tailrec 修饰符标记并满足所需的形式时，编译器会优化该递归，留下一个快速而高效的基于循环的版本。\n 尾递归实现阶乘(Kotlin版本)  tailrec fun factorialTailRecursion(factorial: Int, number: Int): Int { return if (number == 1) factorial else factorialTailRecursion(factorial * number, number - 1) }  最终代码(经过Kotlin编译器优化)  public static final int factorialTailRecursion(int factorial, int number) { while(number != 1) { int var10000 = factorial * number; --number; factorial = var10000; } return factorial; } 参考 https://kotlintc.com/articles/4633\nhttps://www.cnblogs.com/invoker-/p/7723420.html\n"
            }
    
        ,
            {
                "id": 32,
                "href": "https://chinalhr.github.io/post/algorithm-shuffle/",
                "title": "洗牌算法Shuffle",
                "section": "post",
                "date" : "2019.07.31",
                "body": " Shuffle算法学习\n Fisher–Yates Shuffle算法  思想:算法思想就是从原始数组中随机抽取一个新的数字到新数组中 复杂度:O(n2) 实现  private static final int[] shuffledNums = new int[]{...}; /** * Fisher-Yates Shuffle算法 * @return */ public static int[] shuffle() { Random random = new Random(); List\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); //1.初始化新数组  for (int i = 0; i \u0026lt; shuffledNums.length; i++) { list.add(shuffledNums[i]); } int x, j = 0; //2.从还没处理的数组（假如还剩k个）中，随机产生一个[0, k)之间的数字p (假设数组从0开始)  //3.剩下的k个数中把第p个数取出  //重复2和3,直到数字取完  for (int i = list.size() - 1; i \u0026gt;= 0; i = list.size() - 1) { x = random.nextInt(i + 1); shuffledNums[j++] = list.get(x); list.remove(x); } //返回的数组为打乱的数组  return shuffledNums; } Knuth-Durstenfeld Shuffle算法  思想:Fisher–Yates Shuffle的改进,每次从未处理的数据中随机取出一个数字，然后把该数字放在数组的尾部，即数组尾部存放的是已经处理过的数字。在原始数组上对数字进行交互，省去了额外O(n)的空间。 复杂度:O(n) 实现  private static final int[] shuffledNums = new int[]{...}; public static int[] shuffle() { Random random = new Random(); int x, t; //1. 从未处理的数据中随机获取一个数字  //2. 把该数字放在数组的尾部，即尾部的数据已被处理了  //3. 从尾部开始处理，当处理完index=0的数据,得到洗牌后的数据  for (int i = shuffledNums.length - 1; i \u0026gt; 0; i--) { x = random.nextInt(i + 1); t = shuffledNums[i]; shuffledNums[i] = shuffledNums[x]; shuffledNums[x] = t; } return shuffledNums; } "
            }
    
        ,
            {
                "id": 33,
                "href": "https://chinalhr.github.io/post/linux-io-multiplexing/",
                "title": "Linux IO多路复用实现：select、poll与epoll",
                "section": "post",
                "date" : "2019.07.24",
                "body": " Unix IO模型, Linux IO多路复用 select poll epoll机制整理\n Unix IO模型 Blocking IO - 阻塞IO\rNoneBlocking IO - 非阻塞IO\rIO multiplexing - IO多路复用\rsignal driven IO - 信号驱动IO\rasynchronous IO - 异步IO\r  network IO涉及系统对象\n application(调用这个IO的进程) kernel(系统内核)    交互过程\n 阶段1:wait for data 等待数据准备(TCP/UDP包接受接收) 阶段2: copy data from kernel to user 将数据从内核拷贝到用户进程中    Blocking IO - 阻塞IO wait for data阶段与copy data from kernel to user阶段都是阻塞的。\nNoneBlockingIO - 非阻塞IO wait for data阶段非阻塞，用户进程发出recvfrom系统调用后，kernel中的数据如果没准备好，立即返回一个error。采用循环check的方式，每隔一段时间再次发送recvfrom。\ncopy data from kernel to user阶段都是阻塞的，一旦kernel中的数据准备好了，那么它马上就将数据拷贝到了用户内存，然后返回。\nIO multiplexing - IO多路复用(select为例子) 与blocking I/O相似度很高，但是可以等待多个数据报就绪（datagram ready），单个process可以同时处理多个连接。核空间内select会监听指定的多个datagram (如socket连接)，如果其中任意一个数据就绪了就返回。此时程序再进行数据读取操作，将数据拷贝至当前进程内。\nwait for data阶段是阻塞的(select调用阻塞，而非I/O阻塞)，select底层通过轮询机制来判断每个socket读写是否就绪。\ncopy data from kernel to user阶段都是阻塞的。\nIO多路复用的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。\nasynchronous IO - 异步IO aio_read(读取操作)会通知内核进行读取操作并将数据拷贝至进程中，该操作会立刻返回，程序可以进行其它的操作，所有的读取、拷贝工作都由内核去做，做完以后通知进程，进程调用绑定的回调函数来处理数据。\n对比 同步-异步-阻塞-非阻塞 同步-异步 关注点为消息通信机制。\n同步:发出一个调用时,在没有得到结果之前,该调用就不返回。但是一旦调用返回,就得到返回值了。\n异步:发出一个调用后,这个调用就直接返回了,被调用方通过状态、通知来通知调用者,或通过回调函数处理这个调用。\n阻塞与非阻塞 关注点是程序在等待调用结果（消息，返回值）时的状态。\n阻塞调用:调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。\n非阻塞调用:在不能立刻得到结果之前，该调用不会阻塞当前线程。会以另一种方式去check。\n操作系统相关 用户空间与内核空间 现在操作系统采用虚拟存储器，对于32位操作系统的寻址空间（虚拟存储空间）为4G（2的32次方）。操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。对于linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。\n进程切换 内核挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。\n进程切换：\n1. 保存处理机上下文，包括程序计数器和其他寄存器。\r2. 更新PCB信息。\r3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。\r4. 选择另一个进程执行，并更新其PCB。\r5. 更新内存管理的数据结构。\r6. 恢复处理机上下文。\r进程阻塞 正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作做等，则由系统自动执行阻塞原语(Block)，使自己由运行状态变为阻塞状态。当进程进入阻塞状态，是不占用CPU资源的。\n文件描述符fd 文件描述符（File descriptor）是一个用于表述指向文件的引用的抽象化概念。\n文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。\n缓存 I/O 缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。\n数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作会带来 CPU 以及内存开销。\nselect、poll与epoll I/O多路复用就是通过一种机制，可以将想要监视的文件描述符（File Descriptor）添加到select/poll/epoll函数中，由内核监视，函数阻塞。一个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，需要在读写事件就绪后自己负责进行(阻塞)读写。\nselect int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);\rselect 函数监视的文件描述符分3类，分别是writefds、readfds、和exceptfds。调用后select函数会阻塞，直到有描述符就绪（有数据 可读、可写、或者有except），或者超时（timeout指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset(集合,存放的是文件描述符)来找到就绪的描述符。\nselect()的机制中提供一种fd_set的数据结构(long类型的数组)，每一个数组元素都能与一打开的文件句柄建立联系，建立联系的工作由程序完成，当调用select()时，由内核根据IO状态修改fd_set的内容，由此来通知执行了select()的进程哪一socket准备就绪。\n缺陷:\n 每次调用select，都需要把fd集合从用户态拷贝到内核态，在fd很多时开销会很大 单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为32位机器默认为1024个，64位机器默认为2048 select需要在返回后，通过遍历文件描述符来获取已经就绪的流（因为无法区分哪几个流可读/可写）。同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降  poll int poll (struct pollfd *fds, unsigned int nfds, int timeout);\rstruct pollfd {\rint fd; /* file descriptor */\rshort events; /* requested events to watch */\rshort revents; /* returned events witnessed */\r};\rpoll使用一个 pollfd的指针实现。\npollfd结构包含了要监视的event和发生的event，不再使用select“参数-值”传递的方式。没有最大数量限制（但是数量过大后性能也是会下降）。 和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。\npoll改变了文件描述符集合的描述方式，使用了pollfd结构而不是select的fd_set结构，基于链表来存储，没有最大连接数的限制，解决了select文件描述符的数量限制问题。\n缺陷：\n存在和select一样的其他缺陷\nepoll select和poll的增强版本。没有描述符限制，epoll使用一个文件描述符管理多个描述符，使用一个文件描述符管理多个描述符，将关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。\nint epoll_create(int size)；\rint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；\rint epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);\r int epoll_create(int size)  创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。\r当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/能够看到这个fd，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。\r int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)  epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。\r- epfd：是epoll_create()的返回值,表示epoll句柄。\r- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。\r- fd：是需要监听的fd（文件描述符）\r- epoll_event：内核需要监的事件\r结构如下\rstruct epoll_event {\r__uint32_t events; /* Epoll events */\repoll_data_t data; /* User data variable */\r};\rtypedef union epoll_data {\rvoid *ptr;\rint fd;\r__uint32_t u32;\r__uint64_t u64;\r} epoll_data_t;\revents可以是以下几个宏的集合：\rEPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；\rEPOLLOUT：表示对应的文件描述符可以写；\rEPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；\rEPOLLERR：表示对应的文件描述符发生错误；\rEPOLLHUP：表示对应的文件描述符被挂断；\rEPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。\rEPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里\r int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);  等待事件的产生，类似于select()调用，最多返回maxevents个事件。\r参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。\r- epfd 是epoll句柄\r- events 表示从内核得到的就绪事件集合\r- maxevents 告诉内核events的大小\r- timeout 表示等待的超时事件\r 工作模式  水平触发（LT，Level Trigger）模式下，只要一个文件描述符就绪，就会触发通知，如果用户程序没有一次性把数据读写完，下次还会通知；\r边缘触发（ET，Edge Trigger）模式下，当描述符从未就绪变为就绪时通知一次，之后不会再通知，直到再次从未就绪变为就绪（缓冲区从不可读/写变为可读/写）。\r区别：边缘触发效率更高，减少了被重复触发的次数，函数不会返回大量用户程序可能不需要的文件描述符。\r为什么边缘触发一定要用非阻塞（non-block）IO：避免由于一个描述符的阻塞读/阻塞写操作让处理其它描述符的任务出现饥饿状态。\rselect·poll·epoll对比  select：将文件描述符放入一个集合中，调用select时，将这个集合从用户空间拷贝到内核空间，由内核根据就绪状态修改该集合的内容。集合大小有限制，32位机默认是1024（64位：2048）；采用水平触发机制。select函数返回后，通过遍历这个集合，找到就绪的文件描述符 poll：和select几乎没有区别，区别在于文件描述符的存储方式不同，poll采用链表的方式存储，没有最大存储数量的限制； epoll：通过内核和用户空间共享内存，避免了频繁内存从用户-内核空间拷贝的问题；支持的同时连接数上限很高（1G左右的内存支持10W左右的连接数）；文件描述符就绪时，采用回调机制，避免了轮询（回调函数将就绪的描述符添加到一个链表中，执行epoll_wait时，返回这个链表）；支持水平触发和边缘触发，采用边缘触发机制时，只有活跃的描述符才会触发回调函数。 "
            }
    
        ,
            {
                "id": 34,
                "href": "https://chinalhr.github.io/post/interface-idempotent/",
                "title": "Web-接口幂等性保证机制",
                "section": "post",
                "date" : "2019.07.08",
                "body": " 生成环境中Web接口幂等性保证实践记录\n 关于幂等性 在编程中一个幂等操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。幂等函数，或幂等方法，是指可以使用相同参数重复执行，并能获得相同结果的函数。 这些函数不会影响系统状态，也不用担心重复执行会对系统造成改变。\n Http规范中对幂等性的定义   A request method is considered “idempotent” if the intended effect on the server of multiple identical requests with that method is the same as the effect for a single such request. Of the request methods defined by this specification, PUT, DELETE, and safe request methods are idempotent.\n 如果对服务器的多个相同请求的预期效果与单个此类请求的效果相同，则请求方法被视为“幂等”。规范定义的请求方法中，PUT，DELETE和安全请求方法应该是具有幂等性的。\n分布式系统中接口幂等性问题  一个订单流程可能会遇到的问题  订单创建接口，第一次调用响应超时了，然后调用方进行了重试。 用户对订单进行了支付，服务端发生了扣钱操作，但是接口响应超时了，然后调用方进行了重试。 用户对订单进行了充值，第三方服务回调服务器，但因为第三方服务不稳定，并发调用了多次。 订单状态更新接口，调用方连续发送了两个消息，一个是已创建，一个是已付款。但是你先接收到已付款，然后又接收到了已创建。 订单完成之后，需要发送一条短信，当一台机器接收到短信发送的消息之后，处理较慢。消息中间件又把消息投递给另外一台机器处理。  解决之道  保证接口幂等性，接口可重复调用，在调用方多次调用的情况下，接口最终得到的结果是一致的。\n需要保证幂等性的接口：增加、更新、删除类型。\n天然幂等的接口：查询一次和多次，对于系统来说，没有任何影响，查出的结果也是一样。\n分布式系统中接口幂等性保证  全局唯一ID  根据业务的操作和内容生成一个全局ID，在执行操作前先根据这个全局唯一ID是否存在，来判断这个操作是否已经执行。如果不存在则把全局ID，存储到存储系统中，比如数据库、redis等。如果存在则表示该方法已经执行，保证幂等性。  使用去重表约束操作  适用于在业务中有唯一标识的场景。在支付的流程中，因为一个订单只会支付一次，订单ID为唯一标识。新建一张去重表，用于记录已支付订单，并把订单ID作为唯一索引，把支付成功与写入去重表放在一个事务中，利用数据库的UNIQUE KEY 进行约束保证幂等性。  多版本控制  适合于需要多次更新的场景，如商品信息，可以在更新的接口中增加一个版本号，每次请求携带新的版本号，避免重复提交带来的问题，保证幂等性。 update commodity set name=#{newName},version=#{version} where id=#{id} and version\u0026lt;${version}  状态机控制  适合在有状态机流转的情况下,如订单的状态迁移，通过表字段paystatus表示订单的状态，如UNPAID(待支付) SUCCESS(成功) FAIL(失败)，利用订单的状态迁移与单条UPDATE语句的原子性保证幂等性,通过SQL执行的成功与失败来进行下一步操作。 update recharge_order set paystatus = SUCCESS where orderId= 'orderid' and paystatus = UNPAID "
            }
    
        ,
            {
                "id": 35,
                "href": "https://chinalhr.github.io/post/nginx-cross/",
                "title": "跨域问题与解决方式",
                "section": "post",
                "date" : "2019.06.18",
                "body": " Nginx相关资料阅读整理\n 浏览器同源策略   同源策略:限制了从同一个源加载的文档或脚本如何与来自另一个源的资源进行交互。是一个用于隔离潜在恶意文件的重要安全机制。浏览器是从两个方面去做这个同源策略的，一是针对接口的请求，二是针对Dom的查询。\n  同源定义 两个页面协议/主机/端口(tuple)相同表示两个页面拥有相同的源。以http://www.lickdog.com为例子\n  https://www.lickdog.com ——\u0026gt; 不同协议 http://www.lickdog.com:9000 ——\u0026gt; 不同端口 http://www.lickpig.top ——\u0026gt; 不同域名 跨域解决方案  jsonp  利用HTML标签比如script、img这样的获取资源的标签没有跨域限制,后端返回一个直接执行的方法给前端，由于前端是用script标签发起的请求，所以返回了这个方法后相当于立马执行，并且把要返回的数据放在方法的参数里)。\n缺陷,JSONP只能发GET请求。\n window.name+iframe CORS  CORS是一个W3C标准，全称是\u0026quot;跨域资源共享\u0026rdquo;（Cross-origin resource sharing）,CORS有两种请求，简单请求和非简单请求。\n简单请求：\n只要同时满足以下两大条件，就属于简单请求。 （1) 请求方法是以下三种方法之一： HEAD GET POST （2）HTTP的头信息不超出以下几种字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 非简单请求会发出一次预检测请求，返回码是204，预检测通过才会真正发出请求，这才返回200。这里通过前端发请求的时候增加一个额外的headers来触发非简单请求。\n 代理  前端请求的时候还是用前端的域名，Nginx之类的代理把这个请求转发到真正的后端域名上\nNginx反向代理解决跨域 在一个服务器上配置多个前缀来转发http/https请求到多个真实的服务器即可。这样，这个服务器上所有url都是相同的域名、协议和端口。因此，对于浏览器来说，这些url都是同源的，没有跨域限制。而实际上，这些url实际上由物理服务器提供服务。这些服务器内的javascript可以跨域调用所有这些服务器上的url。\n例如：对本地请求lickdog.com/api 将其转发到11000端口下\nserver { listen 80; server_name lickdog.com www.lickdog.com; location /api/ { proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:11000/api/; set_real_ip_from 0.0.0.0/0; real_ip_header X-Forwarded-For; real_ip_recursive on; } location / { alias /data/web/lickdog/webapp/; } access_log /data/log/nginx/www.lickdog.com_access.log log_api; } "
            }
    
        ,
            {
                "id": 36,
                "href": "https://chinalhr.github.io/post/linux-oom/",
                "title": "Linux Out-Of-Memory killer",
                "section": "post",
                "date" : "2019.06.03",
                "body": " Linux OOM Killer排查记录\n Linux OOM Killer(Java进程异常退出排查)  dmesg命令  dmesg [options...] # 可以使用more,tail,less,grep文字处理工具处理dmesg输出 配合管道(pipe)将其输出 dmesg | more # grep查询相关内容 dmesg | grep \u0026quot;(java)\u0026quot; # 配合tail和head显示后和前n行数据 dmesg | grep \u0026quot;(java)\u0026quot; | tail -20  dmesg排查消失的进程  dmesg -T | grep \u0026quot;(java)\u0026quot;| tail -50 # 日志如下 [Wed May 29 10:28:40 2019] Out of memory: Kill process 15546 (java) score 539 or sacrifice child [Wed May 29 10:28:40 2019] Out of memory: Kill process 15546 (java) score 539 or sacrifice child [Wed May 29 10:28:41 2019] Out of memory: Kill process 15546 (java) score 539 or sacrifice child [Wed May 29 10:28:41 2019] Out of memory: Kill process 15546 (java) score 539 or sacrifice child [Wed May 29 10:28:41 2019] Killed process 15546 (java) total-vm:14579308kB, anon-rss:9022832kB, file-rss:0kB, shmem-rss:0kB 表示进程号15546对应的java进程被系统的OOM Killer给干掉了，得分为854. total_vm虚拟内存使用：13.9GB anon-rss 程序实际使用内存：8.6GB  OOM Killer(Out-Of-Memory killer)  OOM Killer:该机制会监控机器的内存资源消耗。当机器内存耗尽前，该机制会扫描所有的进程（按照一定规则计算，内存占用，时间等）， 挑选出得分最高的进程，然后杀死，从而保护机器。\n  解决\n 限制java进程的max heap 开启swap,给系统增加swap内存   "
            }
    
        ,
            {
                "id": 37,
                "href": "https://chinalhr.github.io/post/vertx-kotlin-coroutine/",
                "title": "Vertx Kotlin协程",
                "section": "post",
                "date" : "2019.05.23",
                "body": " Vert.x 利用 Kotlin协程特性优雅编写异步逻辑\n 关于协程 协程通过将复杂性放入库来简化异步编程。程序的逻辑可以在协程中顺序地表达，而底层库会为我们解决其异步性。使用协程库将用户代码的相关部分包装为回调、订阅相关事件、在不同线程（甚至不同机器）上调度执行，而代码则保持如同顺序执行一样简单。\n轻量级的线程。线程是由系统调度的，线程切换或线程阻塞的开销都比较大。而协程依赖于线程，但是协程挂起时不需要阻塞线程，几乎是无代价的，协程是由开发者控制的。所以协程也像用户态的线程，非常轻量级，一个线程中可以创建任意个协程。\nKotlin协程  suspend  suspend修饰符标记，修饰挂起函数。挂起函数能够以与普通函数相同的方式获取参数和返回值，但是调用函数可能挂起协程（如果相关调用的结果已经可用，库可以决定继续进行而不挂起），挂起函数挂起协程时，不会阻塞协程所在的线程。挂起函数执行完成后会恢复协程，后面的代码才会继续执行。\n挂起函数只能在协程中或其他挂起函数中调用(launch函数创建协程)\n CoroutineScope  协程本身\n CoroutineContext  协程上下文，包括 Job 和 CoroutineDispatcher 元素，可以代表一个协程的场景。\n CoroutineDispatcher  协程调度器，决定协程所在的线程或线程池。标准实现Dispatchers.Default、Dispatchers.IO，Dispatchers.Main和Dispatchers.Unconfined\n- Default一般用于 CPU 密集型的任务,特别是涉及到计算、算法的场景。它可以使用和 CPU 核数一样多的线程。 - IO一般用于输入/输出的场景。通常，涉及到会阻塞线程，需要等待另一个系统响应的任务，比如：网络请求、数据库操作、文件读写等，都可以使用它。因为它不使用 CPU ，可以同一时间运行多个线程，默认是数量为 64 的线程池。 - Main使用UI线程/主线程 - UnConfined不指定线程，使用的线程是不可控的   Coroutine builders  协程构建器\n//launch不阻塞当前线程，在后台创建一个新协程 //context 指定调度器 start 启动模式[CoroutineStart.DEAFAULT,LAZY(延迟启动)]  GlobalScope.launch(Dispatchers.Main) { //suspend fun  } fun main() { val job = GlobalScope.launch(start = CoroutineStart.LAZY) { println(\u0026#34;World!\u0026#34;) } println(\u0026#34;Hello,\u0026#34;) job.start() Thread.sleep(2000L) } //runBlocking {}是创建一个新的协程同时阻塞当前线程 runBlocking { launch { //suspend fun  } } //withContext不会创建新的协程，在指定协程上运行挂起代码块，并挂起该协程直至代码块运行完成 withContext {} //async和launch 一个效果，但是有返回值 CoroutineScope.async {}  Job \u0026amp; Deferred  launch 方法会返回一个 Job，Job 继承了协程上下文（CoroutineContext）。\nasync方法返回一个Deferred 允许并行地运行多个子线程任务，它不是一个可中断方法，所以当调用 async 启动子协程的同时，后面的代码也会立即执行。async 通常需要运行在另外一个协程内部，它会返回一个特殊的 Job，叫作 Deferred。\nJob没有返回值，Deferred有返回值。一个Job可以有一个父Job，父Job可以控制子Job。\nJob封装了协程中需要执行的代码逻辑。生命周期如下:\nJob相关方法：\n- job.join(中断与当前 Job 关联的协程，直到所有子 Job 执行完成) - job.cancel(取消所有与其关联的子 Job)  Deferred方法 - await(当我们需要获取 async 的结果时，需要调用 await() 方法等待结果。调用 await() 方法后，会中断当前协程，直到其返回结果)\nVertx 使用协程避免Callback Hell vert.x的非阻塞特性导致需要编写非阻塞API。vert.x的核心API使用回调函数的风格，也可以使用发布-订阅模型的RxJava。异步的API编程比同步的更复杂更容易出错，而且不可避免 Callback Hell。\n使用vertx-lang-kotlin-coroutines，编写同步风格的异步代码\n示例\n"
            }
    
        ,
            {
                "id": 38,
                "href": "https://chinalhr.github.io/post/linux-operating-record/",
                "title": "Linux命令记录",
                "section": "post",
                "date" : "2019.04.19",
                "body": " 日常Linux命令记录\n 日常操作 find  格式  find [path...] [expression] path:find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录 expression:expression可以分为——\u0026quot;-options [-print -exec -ok ...]\u0026quot; options:指定find命令的常用选项，下节详细介绍 print:find命令将匹配的文件输出到标准输出 exec:find命令对匹配的文件执行该参数所给出的shell命令 -ok:和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。  options  -type [查找某一类型的文件] b - 块设备文件。 d - 目录。 c - 字符设备文件。 p - 管道文件。 l - 符号链接文件。 f - 普通文件 -name [按照文件名查找文件] find /dir -name filename 在/dir目录及其子目录下面查找名字为filename的文件 -perm [按照文件权限来查找文件] find . -perm 755 –print 在当前目录下查找文件权限位为755的文件 -user[按照文件属主来查找文件] find ~ -user sam –print 在$HOME目录中查找文件属主为sam的文件 -group[按照文件所属的组来查找文件] find /apps -group gem –print 在/apps目录下查找属于gem用户组的文件 -mtime -n +n [按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+ n表示文件更改时间距现在n天以前] find /var/adm -mtime +3 –print 在/var/adm目录下查找更改时间在3日以前的文件 -size n：[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。  常用操作记录  #清除/root/log 目录或者子目录下 大于1G的log文件 find /root/logs/ -name \u0026#34;*.log\u0026#34; -size +1024M -exec rm -rf {} \\;  参考链接  https://www.cnblogs.com/skynet/archive/2010/12/25/1916873.html\nwhich which命令的作用是在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。\n 格式  which 可执行文件名称  命令参数  -n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。 -p 与-n参数相同，但此处的包括了文件的路径。 -w 指定输出时栏位的宽度。 -V 显示版本信息  例子  which docker /usr/bin/docker whereis whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。 whereis查找的速度很快，因为会从数据库中查找数据，而不是像find命令那样，通 过遍历硬盘来查找（注意： 数据库文件并不是实时更新，默认情况下时一星期更新一次）\n 命令格式  whereis [-bmsu] [BMS 目录名 -f ] 文件名  命令参数  -b 定位可执行文件。 -m 定位帮助文件。 -s 定位源代码文件。 -u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。 -B 指定搜索可执行文件的路径。 -M 指定搜索帮助文件的路径。 -S 指定搜索源代码文件的路径。  例子  whereis docker docker: /usr/bin/docker /etc/docker /usr/libexec/docker /usr/share/man/man1/docker.1.gz ps ps命令用来列出系统中当前运行的那些进程。\n 格式  ps[参数] a 显示所有进程 -a 显示同一终端下的所有程序 -A 显示所有进程 c 显示进程的真实名称 -N 反向选择 -e 等于“-A” e 显示环境变量 f 显示程序间的关系 -H 显示树状结构 r 显示当前终端的进程 T 显示当前终端的所有程序 u 指定用户的所有进程 -au 显示较详细的资讯 -aux 显示所有包含其他使用者的进程  常用操作记录  # ps 与grep 常用组合用法，查找特定进程 ps -ef|grep java # 显示进程的层级信息 ps -ef --forest tail tail 命令从指定点开始将文件写到标准输出.\n# 显示末尾的10行 tail logfile.log # 循环读取 tail -f logfile.log # 显示文件末尾内容(最后五行) tail -n 5 logfile.log # 从第5行开始显示文件 tail -n +5 logfile.log head head 用来显示档案的开头至标准输出中\nhead [参数]... [文件]... -q 隐藏文件名 -v 显示文件名 -c\u0026lt;字节\u0026gt; 显示字节数 -n\u0026lt;行数\u0026gt; 显示的行数 # 显示前5行 head -n 5 log2014.log more|less more命令从前向后读取文件（在启动时就加载整个文件）空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示。\nless命令d 向后翻半页 u 向前滚动半页 空格键 滚动一页\n查询:?关键字 n 上一位 N 下一位\nscp # 复制文件 scp local_file remote_username@remote_ip:remote_folder # 复制目录 scp -r local_folder remote_username@remote_ip:remote_folder crontab crond是linux下用来周期性的执行某种任务或等待处理某些事件的一个守护进程。在/etc目录下的crontab文件\n 格式  minute hour day month week command 其中： minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 crontab [-u user] [ -e | -l | -r ] -e：编辑某个用户的crontab文件内容。如果不指定用户，则表示编辑当前用户的crontab文件。 -l：显示某个用户的crontab文件内容，如果不指定用户，则表示显示当前用户的crontab文件内容。 -r：从/var/spool/cron目录中删除某个用户的crontab文件，如果不指定用户，则默认删除当前用户的crontab文件。  常用操作记录  查看所有用户的crontab cat /etc/passwd | cut -f 1 -d : |xargs -I {} crontab -l -u {} netstat 查询端口所占用的进程 netstat -ntulp | grep 10063 查询进程的详细信息 ps -ef|grep 4826 telnet telnet命令通常用来远程登录。telnet程序是基于TELNET协议的远程登录客户端程序。 telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式。\ntelnet命令除了远程登录，还被用于确定远程服务的状态，比如确定远程服务器的某个端口是否能访问\n 命令格式  telnet[参数][HOST][PORT] du du命令用来查看目录或文件所占用磁盘空间的大小 du常用的选项： -h：以人类可读的方式显示 -a：显示目录占用的磁盘空间大小，还要显示其下目录和文件占用磁盘空间的大小 -s：显示目录占用的磁盘空间大小，不要显示其下子目录和文件占用的磁盘空间大小 -c：显示几个目录或文件占用的磁盘空间大小，还要统计它们的总和 --apparent-size：显示目录或文件自身的大小 -l ：统计硬链接占用磁盘空间的大小 -L：统计符号链接所指向的文件占用的磁盘空间大小　du -sh : 查看当前目录总共占的容量。而不单独列出各子项占用的容量 du -lh --max-depth=1 : 查看当前目录下一级子文件和子目录占用的磁盘容量。 du -sh * | sort -n 统计当前文件夹(目录)大小，并按文件大小排序 du -sk filename 查看指定文件大小 ln 为某一个文件在另外一个位置建立一个同步的链接。链接又可分为两种 : 硬链接(hard link)与软链接(symbolic link)，硬链接的 意思是一个档案可以有多个名称，而软链接的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬链接是存在 同一个文件系统中，而软链接却可以跨越不同的文件系统。\n软链接： 1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 2.软链接可以 跨文件系统 ，硬链接不可以 3.软链接可以对一个不存在的文件名进行链接 4.软链接可以对目录进行链接 硬链接: 1.硬链接，以文件副本的形式存在。但不占用实际空间。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 ln命令会保持每一处链接文件的同步性  格式  ln [参数][源文件或目录][目标文件或目录] ln –s [源文件或目录][目标文件或目录] 软链接 ln [源文件或目录][目标文件或目录] 硬链接 kill Linux中的kill命令用来终止指定的进程的运行\n 格式  kill[参数][进程号] -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户 - 信号(默认终止信号(15)) 全部信号： kill -l 1) SIGHUP\t2) SIGINT\t3) SIGQUIT\t4) SIGILL\t5) SIGTRAP 6) SIGABRT\t7) SIGBUS\t8) SIGFPE\t9) SIGKILL\t10) SIGUSR1 11) SIGSEGV\t12) SIGUSR2\t13) SIGPIPE\t14) SIGALRM\t15) SIGTERM 16) SIGSTKFLT\t17) SIGCHLD\t18) SIGCONT\t19) SIGSTOP\t20) SIGTSTP 21) SIGTTIN\t22) SIGTTOU\t23) SIGURG\t24) SIGXCPU\t25) SIGXFSZ 26) SIGVTALRM\t27) SIGPROF\t28) SIGWINCH\t29) SIGIO\t30) SIGPWR 31) SIGSYS\t34) SIGRTMIN\t35) SIGRTMIN+1\t36) SIGRTMIN+2\t37) SIGRTMIN+3 38) SIGRTMIN+4\t39) SIGRTMIN+5\t40) SIGRTMIN+6\t41) SIGRTMIN+7\t42) SIGRTMIN+8 43) SIGRTMIN+9\t44) SIGRTMIN+10\t45) SIGRTMIN+11\t46) SIGRTMIN+12\t47) SIGRTMIN+13 48) SIGRTMIN+14\t49) SIGRTMIN+15\t50) SIGRTMAX-14\t51) SIGRTMAX-13\t52) SIGRTMAX-12 53) SIGRTMAX-11\t54) SIGRTMAX-10\t55) SIGRTMAX-9\t56) SIGRTMAX-8\t57) SIGRTMAX-7 58) SIGRTMAX-6\t59) SIGRTMAX-5\t60) SIGRTMAX-4\t61) SIGRTMAX-3\t62) SIGRTMAX-2 63) SIGRTMAX-1\t64) SIGRTMAX\t常用信号： HUP 1 终端断线 INT 2 中断（同 Ctrl + C） QUIT 3 退出（同 Ctrl + \\） TERM 15 终止 KILL 9 强制终止 CONT 18 继续（与STOP相反， fg/bg命令） STOP 19 暂停（同 Ctrl + Z） watch 监测一个命令的运行结果\n 命令格式  watch[参数][命令]  命令参数  -n或--interval watch缺省每2秒运行一下程序，可以用-n指定间隔的时间。 -d或--differences 用-d或--differences 选项watch 会高亮显示变化的区域。 而-d=cumulative选项会把变动过的地方(不管最近的那次有没有变动)都高亮显示出来。 -t 或-no-title 会关闭watch命令在顶部的时间间隔,命令，当前时间的输出。  常用命令记录  # 监测当file文件变化，频率5s，高亮显示 watch -n 5 -d'cat /file' alias 设置指令的别名。\n 命令格式  alias[别名]=[指令名称]  alias指令的有效期仅在于单次登录，可以在~/.bashrc(用户级别)或者/etc/bashrc(全局级别)文件追加配置登录指令  vim ~/.bashrc # 添加log路径的别名 alias cdlog=\u0026quot;cd /data/log\u0026quot; # 让配置文件生效 source ~/.bashrc nohup和\u0026amp;(后台运行)  nohup  用途：不挂断地运行命令。\n语法：nohup Command [ Arg … ] [　\u0026amp; ] 无论是否将 nohup 命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中。 如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中。 如果没有文件能创建或打开以用于追加，那么 Command 参数指定的命令不可调用。 退出状态：该命令返回下列出口值： 126 可以查找但不能调用 Command 参数指定的命令。 127 nohup 命令发生错误或不能查找由 Command 参数指定的命令。 否则，nohup 命令的退出状态是 Command 参数指定命令的退出状态。  \u0026amp;  用途：在后台运行,一般和nohup两个一起用，nohup command \u0026amp;\n SpringBoot例子  nohup java -jar xxx.jar \u0026gt;bootstrap.log\u0026amp; trap trap作用是捕捉信号和其他事件并执行命令。\n 命令格式  trap [-lp] [[arg] signal_spec ...]  命令参数  -l 打印信号名称以及信号名称对应的数字。 -p 显示与每个信号关联的trap命令。  常用命令  # 当shell收到 HUP INT PIPE QUIT TERM 这几个命令时，当前执行的程序会执行 exit 1 trap \u0026quot;exit 1\u0026quot; HUP INT PIPE QUIT TERM # 捕获Kill信号(SIGTERM 15)，执行方法_kill trap _kill SIGTERM xargs xargs命令的作用，是将标准输入转为命令行参数。\n 命令格式  xargs [-options] [command]  命令参数  -d 更改分隔符,如： $ echo -e \u0026#34;a\\tb\\tc\u0026#34; | xargs -d \u0026#34;\\t\u0026#34; echo a b c -p 打印出要执行的命令，询问用户是否要执行 -t 打印出最终要执行的命令 -L 指定多少行作为一个命令行参数 -n 指定每次将多少项，作为命令行参数 echo {0..9} | xargs -n 2 echo 0 1 2 3 4 5 6 7 8 9 --max-procs xargs默认只用一个进程执行命令。如果命令要执行多次，必须等上一次执行完，才能执行下一次。--max-procs参数指定同时用多少个进程并行执行命令  常用命令  # 配合find使用 find . -name \u0026#39;datasource.properties\u0026#39; |xargs -t cat 防火墙 centos7防火墙相关操作：\n1.查看已开放的端口(默认不开放任何端口) firewall-cmd --list-ports 2.开启80端口 firewall-cmd --zone=public(作用域) --add-port=80/tcp(端口和访问类型) --permanent(永久生效) firewall-cmd --zone=public --add-port=80/tcp --permanent 3.重启防火墙 firewall-cmd --reload 4.停止防火墙 systemctl stop firewalld.service 5.禁止防火墙开机启动 systemctl disable firewalld.service 6.删除 firewall-cmd --zone= public --remove-port=80/tcp --permanent 其他 如何删除大文件(正在写入的大文件)，会释放占用的磁盘 echo \u0026quot;\u0026quot; \u0026gt; filename 查看文件的大小 du -ms filename 处理数据相关 排序(sort)  命令格式  sort[选项][参数] -b：忽略每行前面开始出的空格字符； -c：检查文件是否已经按照顺序排序； -d：排序时，处理英文字母、数字及空格字符外，忽略其他的字符； -f：排序时，将小写字母视为大写字母； -i：排序时，除了040至176之间的ASCII字符外，忽略其他的字符； -m：将几个排序号的文件进行合并； -M：将前面3个字母依照月份的缩写进行排序； -n：依照数值的大小排序； -o\u0026lt;输出文件\u0026gt;：将排序后的结果存入制定的文件； -r：以相反的顺序来排序； -t\u0026lt;分隔字符\u0026gt;：指定排序时所用的栏位分隔字符； +\u0026lt;起始栏位\u0026gt;-\u0026lt;结束栏位\u0026gt;：以指定的栏位来排序，范围由起始栏位到结束栏位的前一栏位。  示例  # 统计当前文件夹(目录)大小，并按文件大小降序排序 du -sh * |sort -nr 搜索数据(grep) 文本搜索工具，用于过滤/搜索的特定字符,它能使用正则表达式搜索文本，并把匹配的行打印出来。\n 命令格式  grep [option] pattern file 参数 -a --text #不要忽略二进制的数据。 -A\u0026lt;显示行数\u0026gt; --after-context=\u0026lt;显示行数\u0026gt; #除了显示符合范本样式的那一列之外，并显示该行之后的内容。 -b --byte-offset #在显示符合样式的那一行之前，标示出该行第一个字符的编号。 -B\u0026lt;显示行数\u0026gt; --before-context=\u0026lt;显示行数\u0026gt; #除了显示符合样式的那一行之外，并显示该行之前的内容。 -c --count #计算符合样式的列数。 -C\u0026lt;显示行数\u0026gt; --context=\u0026lt;显示行数\u0026gt;或-\u0026lt;显示行数\u0026gt; #除了显示符合样式的那一行之外，并显示该行之前后的内容。 -d \u0026lt;动作\u0026gt; --directories=\u0026lt;动作\u0026gt; #当指定要查找的是目录而非文件时，必须使用这项参数，否则grep指令将回报信息并停止动作。 -e\u0026lt;范本样式\u0026gt; --regexp=\u0026lt;范本样式\u0026gt; #指定字符串做为查找文件内容的样式。 -E --extended-regexp #将样式为延伸的普通表示法来使用。 -f\u0026lt;规则文件\u0026gt; --file=\u0026lt;规则文件\u0026gt; #指定规则文件，其内容含有一个或多个规则样式，让grep查找符合规则条件的文件内容，格式为每行一个规则样式。 -F --fixed-regexp #将样式视为固定字符串的列表。 -G --basic-regexp #将样式视为普通的表示法来使用。 -h --no-filename #在显示符合样式的那一行之前，不标示该行所属的文件名称。 -H --with-filename #在显示符合样式的那一行之前，表示该行所属的文件名称。 -i --ignore-case #忽略字符大小写的差别。 -l --file-with-matches #列出文件内容符合指定的样式的文件名称。 -L --files-without-match #列出文件内容不符合指定的样式的文件名称。 -n --line-number #在显示符合样式的那一行之前，标示出该行的列数编号。 -q --quiet或--silent #不显示任何信息。 -r --recursive #此参数的效果和指定“-d recurse”参数相同。 -s --no-messages #不显示错误信息。 -v --revert-match #显示不包含匹配文本的所有行。 -V --version #显示版本信息。 -w --word-regexp #只显示全字符合的列。 -x --line-regexp #只显示全列符合的列。 -y #此参数的效果和指定“-i”参数相同。 规则表达式 ^ #锚定行的开始 如：'^grep'匹配所有以grep开头的行。 $ #锚定行的结束 如：'grep$'匹配所有以grep结尾的行。 . #匹配一个非换行符的字符 如：'gr.p'匹配gr后接一个任意字符，然后是p。 * #匹配零个或多个先前字符 如：'*grep'匹配所有一个或多个空格后紧跟grep的行。 .* #一起用代表任意字符。 [] #匹配一个指定范围内的字符，如'[Gg]rep'匹配Grep和grep。 [^] #匹配一个不在指定范围内的字符，如：'[^A-FH-Z]rep'匹配不包含A-R和T-Z的一个字母开头，紧跟rep的行。 \\(..\\) #标记匹配字符，如'\\(love\\)'，love被标记为1。 \\\u0026lt; #锚定单词的开始，如:'\\\u0026lt;grep'匹配包含以grep开头的单词的行。 \\\u0026gt; #锚定单词的结束，如'grep\\\u0026gt;'匹配包含以grep结尾的单词的行。 x\\{m\\} #重复字符x，m次，如：'0\\{5\\}'匹配包含5个o的行。 x\\{m,\\} #重复字符x,至少m次，如：'o\\{5,\\}'匹配至少有5个o的行。 x\\{m,n\\} #重复字符x，至少m次，不多于n次，如：'o\\{5,10\\}'匹配5--10个o的行。 \\w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：'G\\w*p'匹配以G后跟零个或多个文字或数字字符，然后是p。 \\W #\\w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \\b #单词锁定符，如: '\\bgrep\\b'只匹配grep。  常用案例  # 文件关键字提取查找(info日志查询mongo) cat info.log | grep -f mongo # 查找指定进程 ps -ef|grep git # 查找指定进程(不显示本身grep进程) ps -ef | grep java | grep -v \u0026quot;grep\u0026quot; # 从多个文件中查找关键字 grep 'mongodb' info1.log info2.log # 找出mongo开头的行内容 cat info.log |grep ^mongo # 找出mongo结尾的行内容 cat info.log |grep mongo$ # 显示包含api1或者cms1字符的内容行 cat info.log |grep -E \u0026quot;api1|cms1\u0026quot; 压缩数据(gzip) gzip是GNU压缩工具，用Lempel-Ziv编码压缩程序，压缩后使用\u0026rdquo;.gz\u0026quot;的扩展名\n 命令格式  gzip[参数][文件或者目录] 参数 -a或--ascii 使用ASCII文字模式。 -c或--stdout或--to-stdout 把压缩后的文件输出到标准输出设备，不去更动原始文件。 -d或--decompress或----uncompress 解开压缩文件。 -f或--force 强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接。 -h或--help 在线帮助。 -l或--list 列出压缩文件的相关信息。 -L或--license 显示版本与版权信息。 -n或--no-name 压缩文件时，不保存原来的文件名称及时间戳记。 -N或--name 压缩文件时，保存原来的文件名称及时间戳记。 -q或--quiet 不显示警告信息。 -r或--recursive 递归处理，将指定目录下的所有文件及子目录一并处理。 -S\u0026lt;压缩字尾字符串\u0026gt;或----suffix\u0026lt;压缩字尾字符串\u0026gt; 更改压缩字尾字符串。 -t或--test 测试压缩文件是否正确无误。 -v或--verbose 显示指令执行过程。 -V或--version 显示版本信息。 -num 用指定的数字num调整压缩的速度，-1或--fast表示最快压缩方法（低压缩比），-9或--best表示最慢压缩方法（高压缩比）。系统缺省值为6。  示例  # 压缩文件 gzip info.log # 解压目录 gzip -d info.log # 递归压缩目录 gzip -rv api # 递归解压目录 gzip -dr api 归档数据(tar) 打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 由于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令）， 然后再用压缩程序进行压缩（gzip命令）。\n 命令格式  tar[必要参数][选择参数][文件] -A 新增压缩文件到已存在的压缩 -B 设置区块大小 -c 建立新的压缩文件 -d 记录文件的差别 -r 添加文件到已经压缩的文件 -u 添加改变了和现有的文件到已经存在的压缩文件 -x 从压缩的文件中提取文件 -t 显示压缩文件的内容 -z 支持gzip解压文件 -j 支持bzip2解压文件 -Z 支持compress解压文件 -v 显示操作过程 -l 文件系统边界设置 -k 保留原有文件不覆盖 -m 保留文件不被覆盖 -W 确认压缩文件的正确性 可选参数如下： -b dir 设置区块数目 -C 切换到指定目录 -f file 指定压缩文件 --help 显示帮助信息 --version 显示版本信息 常见解压/压缩命令： tar 解包：tar xvf FileName.tar 打包：tar cvf FileName.tar DirName （注：tar是打包） .gz 解压1：gunzip FileName.gz 解压2：gzip -d FileName.gz 压缩：gzip FileName .tar.gz 和 .tgz 解压：tar zxvf FileName.tar.gz 压缩：tar zcvf FileName.tar.gz DirName .bz2 解压1：bzip2 -d FileName.bz2 解压2：bunzip2 FileName.bz2 压缩： bzip2 -z FileName .tar.bz2 解压：tar jxvf FileName.tar.bz2 压缩：tar jcvf FileName.tar.bz2 DirName .bz 解压1：bzip2 -d FileName.bz 解压2：bunzip2 FileName.bz 压缩：未知 .tar.bz 解压：tar jxvf FileName.tar.bz 压缩：未知 .Z 解压：uncompress FileName.Z 压缩：compress FileName .tar.Z 解压：tar Zxvf FileName.tar.Z 压缩：tar Zcvf FileName.tar.Z DirName .zip 解压：unzip FileName.zip 压缩：zip FileName.zip DirName .rar 解压：rar x FileName.rar 压缩：rar a FileName.rar DirName  示例  # 打包api1 api2目录 tar -cvf api.tar api1/ api2/ # 列除tar文件api.tar的内容，但不提取文件 tar -xvf api.tar # 打包后，以 gzip 压缩 tar -zcvf info.tar.gz info.log # 查阅压缩后的tar包内有哪些文件 tar -ztvf log.tar.gz # 解压缩tar包 tar -zxvf log.tar.gz # 排除文件夹不进行打包 tar --exclude api/service -zcvf api.tar.gz api/* 排查 排查Java CPU占用问题 1.0\n# top命令，定位高CPU占用Java进程PID top -c (将系统资源使用情况实时显示出来) P(输入大写 P 将应用按照 CPU 使用率排序，第一个为使用率最高的) # 显示线程列表(查看占用CPU时间,占用时间，TID) ps -mp pid -o THREAD,tid,time # 将线程ID TID转换为16进制 printf “%x\\n” tid # 使用jstack打印线程堆栈信息(pid=进程id tid=我们转换的tid) .../jvm/jstack pid |grep tid -A 60 2.0\n1. top -c 将系统资源使用情况实时显示出来 （-c 参数可以完整显示命令）大写 P 将应用按照 CPU 使用率排序，第一个就是使用率最高的程序。 2. top -Hp pid 然后输入 P 依然可以按照 CPU 使用率将线程排序。 3. 将线程ID TID转换为16进制printf “%x\\n” tid。 4. 通过 jstack pid \u0026gt;pid.log 生成日志文件，利用刚才保存的 16 进制进程 ID 去这个线程快照中搜索即可查看消耗 CPU 的线程日志。 5. jstat -gcutil pid 200 50 将内存使用、gc 回收状况打印出来（每隔 200ms 打印 50次）。 6. 排查Java高内存占用问题 # 查看当前Java进程创建的活跃对象数目和占用内存大小 jmap -histo:live [pid] \u0026gt;a.log # 当前Java进程的内存占用情况导出来，方便用专门的内存分析工具（例如：MAT）来分析。 jmap -dump:live,format=b,file=xxx.xxx [pid] 阿里云相关 挂载ecs数据盘  先将数据盘挂载到实例上 分区  fdisk /dev/vdb Command n [新建分区] Command p [选择分区类型为主分区] Selected partition 1 [选择分区号] Command w [退出]  格式化  mkfs.ext4 /dev/vdb1  修改 /etc/fstab  cp /etc/fstab /etc/fstab.bak mkdir -p /data echo /dev/vdb1 /data ext4 defaults 0 0 \u0026gt;\u0026gt; /etc/fstab  将分区挂载到对应的文件系统路径下  mount /dev/vdb1 /data 阿里云数据盘扩容操作  控制台操作对数据盘进行扩容 重启机器 确认分区和文件系统  # 确认磁盘是否分区(fdisk -lu \u0026lt;DeviceName\u0026gt;) fdisk -lu /dev/vdb 展示如下: Device Boot Start End Blocks Id System /dev/vdb1 63 1048575023 524287480+ 83 Linux # 确认文件系统(blkid \u0026lt;PartionName\u0026gt;) blkid /dev/vdb1 /dev/vdb1: UUID=\u0026quot;...\u0026quot; TYPE=\u0026quot;ext4\u0026quot; # 确认文件系统状态为clean e2fsck -n /dev/vdb1  卸载数据盘(原先/dev/vdb1是挂载到/data的)  umount /data  扩展已有分区  # 显示并记录旧分区的起始和结束的扇区位置 fdisk -lu /dev/vdb # 删除旧分区 fdisk -u /dev/vdb Command p [打印分区表] Command d [删除分区] Selected partition 1 [选择分区号] Command w [退出] # 新建分区 fdisk -u /dev/vdb Command p [打印分区表] Command n [新建分区] Command p [选择分区类型为主分区] Selected partition 1 [选择分区号] Command w [退出] # lsblk /dev/vdb确认分区表,显示如下 500G-\u0026gt;1000G NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vdb 252:16 0 1000G 0 disk └─vdb1 252:17 0 1000G 0 part # 确认文件系统状态为clean e2fsck -n /dev/vdb1 # 内核版本 ≥ 3.6的，通知内核更新分区表 partprobe \u0026lt;dst_dev_path\u0026gt;或者partx -u \u0026lt;dst_dev_path\u0026gt;  扩容文件系统  # 检测并修复文件系统的完整性 e2fsck -f /dev/vdb1 # 增大文件系统的大小 resize2fs -f /dev/vdb1 # 重新挂载/data mount /dev/vdb1 /data Centos环境相关配置 Docker # 查看当前的内核版本(需要3.1.0以上版本) uname -r # 更新yum包 sudo yum update # 安装需要的软件包 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 # 设置yum源 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # 可以查看所有仓库中所有docker版本，并选择特定版本安装 yum list docker-ce --showduplicates | sort -r # 安装docker sudo yum install docker-ce sudo yum install \u0026lt;FQPN docker-ce.x86_64 3:19.03.5-3.el7\u0026gt; # 启动并加入开机启动 sudo systemctl start docker sudo systemctl enable docker # 验证 docker version # 命令 sudo systemctl stop docker sudo systemctl start docker # 镜像加速器 ## 七牛云加速器 https://reg-mirror.qiniu.com/Azure 中国镜像 https://dockerhub.azk8s.cn vim /etc/docker/daemon.json ## 加入镜像地址 { \u0026quot;registry-mirrors\u0026quot;: [ \u0026quot;https://dockerhub.azk8s.cn\u0026quot;, \u0026quot;https://reg-mirror.qiniu.com\u0026quot; ] } ## 重启服务 sudo systemctl daemon-reload sudo systemctl restart docker ## docker info查看Registry Mirrors配置 OpenJDK1.8 yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel # 获取java home dirname $(readlink $(readlink $(which java))) # 设置环境变量 vim /etc/profile.d/env_export.sh export JAVA_HOME=上面dirname命令获取到的路径，去掉/jre/bin最后这段 export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/jre/lib:$JAVA_HOME/lib:$JAVA_HOME/lib/tools.jar # 使设置立即生效 source /etc/profile.d/env_export.sh Nginx # 配置 EPEL源 sudo yum install -y epel-release sudo yum -y update # 安装Nginx sudo yum install -y nginx # 常用命令 systemctl enable nginx.service\t设置开机启动 systemctl disable nginx.service\t停止开机自启动 systemctl start nginx.service\t启动nginx服务 systemctl status nginx.service\t查看服务当前状态 systemctl restart nginx.service\t重新启动服务 systemctl list-units --type=service\t查看所有已启动的服务 nginx -s reload nginx -s stop nginx -s quit nginx -t service nginx start mongoDB # 配置MongoDB的yum源 #cd /etc/yum.repos.d #vim mongodb-org-4.2.repo # 添加阿里云源 [mngodb-org] name=MongoDB Repository baseurl=http://mirrors.aliyun.com/mongodb/yum/redhat/7Server/mongodb-org/4.0/x86_64/ gpgcheck=0 enabled=1 # 安装 yum install -y mongodb-org # 常用命令 /usr/bin/mongod --config /etc/mongod.conf #启动 /usr/bin/mongod --shutdown --config /etc/mongod.conf #停止 mongo #进入控制台 maven # 下载maven wget http://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz # 解压安装 tar -zxvf apache-maven-3.3.9-bin.tar.gz # 配置环境变量 vim /etc/profile M2_HOME=/opt/tools/apache-maven-3.3.9 export PATH=${M2_HOME}/bin:${PATH} # 重载/etc/profile source /etc/profile Node.js # 确认系统是否已经安装了epel-release包 yum info epel-release # 安装epel-release yum install epel-release # 安装node.js sudo yum install nodejs # 安装n（node 版本管理器） npm install -g n # 安装最新版本 n latest # 安装指定版本 n 8.11.3 # 选择已安装的版本 n 其他 # 修改主机名 hostnamectl set-hostname name # 修改ssh端口 vim /etc/ssh/sshd_config（修改Port值） 链接 每日一linux命令\n"
            }
    
        ,
            {
                "id": 39,
                "href": "https://chinalhr.github.io/post/aspect-dynamic/",
                "title": " javassist实现Aspect动态Pointcut",
                "section": "post",
                "date" : "2019.04.18",
                "body": " Spriing Aop中动态Pointcut的实践\n 为什么需要AspectJ中具有动态值的Pointcut 工作中需要抽离出公司项目的一些监控组件打成一个公共包,需要对一些Aspect作动态适配(根据引入项目的ComponentScan进行特定package的Pointcut/配置PointCut)\n@Pointcut的用法  格式  execution(modifiers-pattern? ret-type-pattern declaring-type-pattern? name-pattern(param-pattern)throws-pattern?) 修饰符匹配（modifier-pattern?） 返回值匹配（ret-type-pattern）可以为*表示任何返回值,全路径的类名等 类路径匹配（declaring-type-pattern?） 方法名匹配（name-pattern）可以指定方法名 或者 *代表所有, set* 代表以set开头的所有方法 参数匹配（(param-pattern)）可以指定具体的参数类型，多个参数间用“,”隔开，各个参数也可以用“*”来表示匹配任意类型的参数，如(String)表示匹配一个String参数的方法；(*,String) 表示匹配有两个参数的方法，第一个参数可以是任意类型，而第二个参数是String类型；可以用(..)表示零个或多个任意参数 异常类型匹配（throws-pattern?） 其中后面跟着“?”的是可选项 1）execution(* *(..)) //表示匹配所有方法 2）execution(public * com. savage.service.UserService.*(..)) //表示匹配com.savage.server.UserService中所有的公有方法 3）execution(* com.savage.server..*.*(..)) //表示匹配com.savage.server包及其子包下的所有方法  Spring中的Pointcut(Pointcut表示式(expression)和Pointcut签名(signature))  //Pointcut表示式(可以使用\u0026amp;\u0026amp; || ! 这三个运算) @Pointcut(\u0026quot;execution(* com.savage.aop.MessageSender.*(..))\u0026quot;) //Point签名 private void log(){} 动态Pointcut  关于SpringAop  AspectJ方式织入的核心，是一个BeanPostProcess（会扫描所有的Pointcut与遍历所有Bean,并对需要的Bean进行织入-自动代理，当对象实例化的时候，为其生成代理对象并返回）\n 思路  在Aop的BeanPostProcess执行之前( springApplication.run之前),使用javassist修改目标Aop类字节码，动态设置@Pointcut,设置 value为我们自己动态查询到的值。\n 其他  由于Spring boot的类加载机制，运行时javassist会扫描不到包，要通过insertClassPath添加扫描路径\n修改@Pointcut的切点值后，通过toClass覆盖原有类，需要通过类加载器重新加载。\n 实现  @SpringBootApplication public class UidServerApplication extends SpringBootServletInitializer { public static void main(String[] args) { AspectPoincutScan(); SpringApplication springApplication = new SpringApplication(UidServerApplication.class); springApplication.addInitializers(new UidApplicationContextInitializer()); springApplication.run(args); } @Override protected SpringApplicationBuilder configure( SpringApplicationBuilder builder) { AspectPoincutScan(); return builder.sources(UidServerApplication.class) .initializers(new UidApplicationContextInitializer()) .listeners(new UidApplicationRefreshedListener()) .listeners(new UidApplicationCloseListener()); } private static void AspectPoincutScan() { try { ClassPool pool = ClassPool.getDefault(); // 添加包的扫描路径  ClassClassPath classPath = new ClassClassPath(UidServerApplication.class); pool.insertClassPath(classPath); //获取要修改的Class  CtClass ct = pool.get(\u0026#34;mobi.meishuo.uidserver.monitor.springaop.ServiceMonitorAop\u0026#34;); CtMethod[] cms = ct.getDeclaredMethods(); for (CtMethod cm : cms) { //找到@pointcut 注解的方法  if (cm.getName().equals(\u0026#34;pointcut\u0026#34;)) { MethodInfo methodInfo = cm.getMethodInfo(); ConstPool cPool = methodInfo.getConstPool(); AnnotationsAttribute attribute = new AnnotationsAttribute(cPool, AnnotationsAttribute.visibleTag); //获取@pointcut 注解，修改其value值  Annotation annotation = new Annotation(\u0026#34;org.aspectj.lang.annotation.Pointcut\u0026#34;, cPool); annotation.addMemberValue(\u0026#34;value\u0026#34;, new StringMemberValue(\u0026#34;execution(xxxx\u0026#34;, cPool)); attribute.setAnnotation(annotation); methodInfo.addAttribute(attribute); //覆盖原有类  ct.toClass(); //使用类加载器重新加载Aop类  pool.getClassLoader().loadClass(\u0026#34;mobi.meishuo.uidserver.monitor.springaop.ServiceMonitorAop\u0026#34;); } } } catch (Exception e) { e.printStackTrace(); } } } 编程式AOP实现动态PointCut  PonitCut  public static Pointcut getAdapterServicePointcut(){ AspectJExpressionPointcut adapterPointcut = new AspectJExpressionPointcut(); //从配置文件中获取PointCut表达式  adapterPointcut.setExpression(MonitorPropertyConfig.getPoinitcutAdapter()); return adapterPointcut;} //扩展Spring中AbstractBeanFactoryPointcutAdvisor  public class AdapterServiceAdvisor extends AbstractBeanFactoryPointcutAdvisor { @Override public Pointcut getPointcut() { return getAdapterServicePointcut(); } }  Advice  public class AdapterServiceMonitorInterceptor implements MethodInterceptor { @Override public Object invoke(MethodInvocation invocation) throws Throwable { //做一些操作...  } }  配置Advisor Bean  @Configuration public class MonitorProxyConfiguration { @Bean(name = \u0026#34;adapterServiceAdvisor\u0026#34;) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public AdapterServiceAdvisor adapterServiceAdvisor() { AdapterServiceAdvisor advisor = new AdapterServiceAdvisor(); advisor.setAdviceBeanName(\u0026#34;adapterServiceAdvice\u0026#34;); advisor.setAdvice(new AdapterServiceMonitorInterceptor()); advisor.setOrder(Ordered.HIGHEST_PRECEDENCE); return advisor; } } "
            }
    
        ,
            {
                "id": 40,
                "href": "https://chinalhr.github.io/post/maven-plugs-record/",
                "title": "Maven Plugin",
                "section": "post",
                "date" : "2019.04.17",
                "body": " Maven Plugin使用记录\n Maven插件机制 Maven的核心仅仅定义了抽象的生命周期，具体的任务是交由插件完成的，插件以独立的构件形式存在，Maven会在需要的时候下载并使用插件。\n插件机制的目的 插件本身，为了能够复用代码，它往往能够完成多个任务，为每个这样的功能编写一个独立的插件显然是不可取的，因为这些任务背后都有很多可以复用的代码，这些功能都聚集在一个插件里，每个功能就是一个插件目标。\n通用写法 compiler:compile(插件前缀:插件目标)\n插件的绑定 Maven生命周期的阶段与插件的目标相互绑定，以完成某个具体的构建任务。\n clean生命周期阶段插件绑定关系     生命周期阶段 插件目标 执行任务     pre-clean     clean maven-clean-plugin:clean 删除项目的输出目录   post-clean       site生命周期阶段与插件绑定关系     生命周期阶段 插件目标 执行任务     pre-site     site maven-site-plugin:site target/site生成站点   post-site     site-deploy maven-site-plugin:deploy 部署到web服务器     default生命周期阶段与插件绑定关系(jar)     生命周期阶段 插件目标 执行任务     process-resources maven-resources-plugin:resources 复制主资源文件至主输出目录   compile maven-compiler-plugin:compile 编译主代码至主输出目录   process-test-resources maven-resources-plugin:testResources 复制测试资源文件至测试输出目录   test-compile maven-compiler-plugin:testCompile 编译测试代码至测试输出目录   test maven-surefire-plugin:test 执行测试用例   package maven-jar-plugin:jar 创建项目jar包   install maven-install-plugin:install 将项目输出构件安装到本地仓库   deploy maven-deploy-plugin:deploy 将项目输出构件部署到远程仓库    插件配置  命令行配置  # –D 参数 是Java自带，功能是通过命令行设置一个Java系统属性，Maven简单地重用了该参数。 # maven-surefire-plugin maven-surefire-plugin 提供一个 maven.test.skip 参数，当其值为 true的时候，就会跳过执行测试。  pom 中插件全局配置  如指定jdk编译版本\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 其他插件 maven-release-plugin # 版本管理 # 功能: SNAPSHOT快照版本，RELEASE正式版本 正式发布流程： 1. 在trunk中，更新pom版本从1.0-SNAPSHOT到1.0 2. 对1.0打一个git tag 3. 针对tag进行mvn deploy，发布正式版本 4. 更新trunk从1.0到1.1-SNAPSHOT \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-release-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;autoVersionSubmodules\u0026gt;true\u0026lt;/autoVersionSubmodules\u0026gt; \u0026lt;tagNameFormat\u0026gt;v@{project.version}\u0026lt;/tagNameFormat\u0026gt; \u0026lt;generateReleasePoms\u0026gt;false\u0026lt;/generateReleasePoms\u0026gt; \u0026lt;arguments\u0026gt;-DskipTests\u0026lt;/arguments\u0026gt; ... \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;scm\u0026gt; \u0026lt;developerConnection\u0026gt;scm:git:xxx\u0026lt;/developerConnection\u0026gt; \u0026lt;/scm\u0026gt; 命令: mvn release:prepare release版本打tag,snapshot版本自动迭代 mvn release:perform release版本打包，分发到远程Maven仓库中 maven版本规则 \u0026lt;主版本\u0026gt;.\u0026lt;次版本\u0026gt;.\u0026lt;增量版本\u0026gt; maven-checkstyle-plugin # 代码风格检查 添加checkstyle配置文件 \u0026lt;checkstyle.config.location\u0026gt;checkstyle.xml\u0026lt;/checkstyle.config.location\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-checkstyle-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.9.1\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-jxr-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; mvn命令检测代码 mvn checkstyle:checkstyle 检查工程是否满足checkstyle mvn jxr:jxr 命令可以查看有错的对应代码 maven-shade-plugin # 绑定package生命周期目标,用户项目打可执行包，包含依赖，以及对依赖进行取舍过滤 maven-resources-plugin # Maven中的资源文件默认存在于src/main/resources,resources用于提供自定义资源打包功能 \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${maven-resources-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;copy-resources\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;copy-resources\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; //自定义输出目录，将resource/conf下的配置打包到输出目录/conf \u0026lt;outputDirectory\u0026gt;${build.path}/conf\u0026lt;/outputDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;../conf/\u0026lt;/directory\u0026gt; \u0026lt;filtering\u0026gt;true\u0026lt;/filtering\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; maven-source-plugin # 自动将源码打包并发布的功能, mvn install:将source install到repository mvn deploy:将source deploy到remote-repository mvn source:jar:单独打包源码 参考 http://maven.apache.org/plugins/index.html\n"
            }
    
        ,
            {
                "id": 41,
                "href": "https://chinalhr.github.io/post/mysql-deadlock-record/",
                "title": "Mysql死锁排查记录",
                "section": "post",
                "date" : "2019.04.08",
                "body": " 记一次MySql死锁排查\n 情况(uniq duplicate key检查) 业务 用户通过访问他人的个人主页删除访问用户-被访用户记录，并重新插入。(为什么删除再插入而不是update,由于另一套库里的访客不存在 customerId, targetCustomerId 的唯一索引, 所以肯定会有脏数据. 这里直接删掉规避更新异常)\n 伪代码  //外层Async调用  @Transactional(rollbackFor = Exception.class) @Override public void saveOrUpdate(Long customerId, Long visitCustomerId) { deleteVisit(customerId, visitCustomerId); customerVisitDao.insertVisit(customerId, visitCustomerId); } (伪)表结构 CREATE TABLE `customer_visit` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `created_date` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \u0026#39;创建时间\u0026#39;, `customer_id` bigint(20) NOT NULL DEFAULT \u0026#39;-1\u0026#39; COMMENT \u0026#39;访问用户\u0026#39;, `visit_customer_id` bigint(20) NOT NULL DEFAULT \u0026#39;-1\u0026#39; COMMENT \u0026#39;被访问用户\u0026#39;, PRIMARY KEY (`id`), UNIQUE KEY `uniq_customer_id_visit_customer_id` (`customer_id`,`visit_customer_id`), KEY `index_customer_id` (`customer_id`), KEY `index_visit_customer_id` (`visit_customer_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT=\u0026#39;访客记录\u0026#39;; 分析死锁原因 通过show engine innodb status查看最近一次的死锁日志\n------------------------ LATEST DETECTED DEADLOCK ------------------------ 2019-04-08 22:58:23 7f8d2fbff700 //事务1 编号694322167 活跃0.005s 事务状态:根据索引读取数据 *** (1) TRANSACTION: TRANSACTION 694322167, ACTIVE 0.005 sec starting index read //事务1使用一个表 表锁1 mysql tables in use 1, locked 1 //等待锁链表的长度为2 当前事务持有的行记录锁/gap(间隙锁)锁1 LOCK WAIT 2 lock struct(s), heap size 360, 1 row lock(s) LOCK BLOCKING MySQL thread id: 5828505 block 5835430 MySQL thread id 5835430, OS thread handle 0x7f8d6afff700, query id 4891428548 192.168.0.47 web_user updating //事务1正在等待锁的sql DELETE FROM customer_visit where customer_id = 735***68 and visit_customer_id =624***90 *** (1) WAITING FOR THIS LOCK TO BE GRANTED: //事务1 正在等待表customer_visit上索引uniq_customer_id_visit_customer_id的X锁(排他锁)-记录锁 RECORD LOCKS space id 47 page no 25368 n bits 616 index `uniq_customer_id_visit_customer_id` of table `huayan`.`customer_visit` trx id 694322167 lock_mode X locks rec but not gap waiting Record lock, heap no 502 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 8; hex 800000000462ed68; asc b h;; 1: len 8; hex 8000000003b97a96; asc z ;; 2: len 8; hex 8000000000eb5fb1; asc _ ;; //事务2 编号694322166 活跃0.005s 事务状态:插入数据 *** (2) TRANSACTION: TRANSACTION 694322166, ACTIVE 0.005 sec inserting //事务2使用一个表 表锁1 mysql tables in use 1, locked 1 4 lock struct(s), heap size 1184, 3 row lock(s), undo log entries 2 MySQL thread id 5828505, OS thread handle 0x7f8d2fbff700, query id 4891428551 192.168.0.48 web_user update insert customer_visit(customer_id,visit_customer_id) values (735***68,624***90) //事务2 持有表customer_visit上索引uniq_customer_id_visit_customer_id的X锁 记录锁 //由于是RC隔离模式下的基于唯一索引的等值查询，会申请一个记录锁 //此处是事务2通过delete fromcustomer_visit where customer_id = 735***68 and visit_customer_id =624***90申请的锁 *** (2) HOLDS THE LOCK(S): RECORD LOCKS space id 47 page no 25368 n bits 616 index `uniq_customer_id_visit_customer_id` of table `huayan`.`customer_visit` trx id 694322166 lock_mode X locks rec but not gap Record lock, heap no 502 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 8; hex 800000000462ed68; asc b h;; 1: len 8; hex 8000000003b97a96; asc z ;; 2: len 8; hex 8000000000eb5fb1; asc _ ;; //事务2 正在申请S锁(共享锁) //此处是事务2通过insert customer_visit(customer_id,visit_customer_id) values (735***68,624***90) 申请的 //insert语句在普通情况下是会申请排他锁，也就是X锁，但是这里出现了S锁。这是因为a字段是一个唯一索引， //所以insert语句会在插入前进行一次duplicate key的检查，为了使这次检查成功，需要申请S锁防止其他事务对a字段进行修改。 *** (2) WAITING FOR THIS LOCK TO BE GRANTED: RECORD LOCKS space id 47 page no 25368 n bits 616 index `uniq_customer_id_visit_customer_id` of table `huayan`.`customer_visit` trx id 694322166 lock mode S waiting Record lock, heap no 502 PHYSICAL RECORD: n_fields 3; compact format; info bits 32 0: len 8; hex 800000000462ed68; asc b h;; 1: len 8; hex 8000000003b97a96; asc z ;; 2: len 8; hex 8000000000eb5fb1; asc _ ;; 综上，推测出死锁原因:对于同一个的字段的锁申请是需要排队的，针对uniq_customer_id_visit_customer_id索引,T2 insert申请的S锁之前,T1的delete在申请X锁,而T1的X锁又在等待T2 Delete申请的X锁释放，T2的S锁在等待T1的X锁申请，形成循环等待，导致死锁。\n并发的情况下，发生死锁情况    事务1 事务2      begin    DELETE FROM customer_visit where customer_id = 1001 and visit_customer_id =1002[执行成功，事务2占有1001-1002uniq的X锁，类型为记录锁]   begin    DELETE FROM customer_visit where customer_id = 1001 and visit_customer_id =1002[事务1希望申请1001-1002uniq的X锁,事务2已经申请到了,事务wait，X锁申请进入锁请求队列]    Deadlock insert customer_visit(customer_id,visit_customer_id) values (1001,1002)[事务2需要申请1001-1002uniq的S锁以便检查duplicate key，排在事务1的1001-1002uniq的X之后,形成循环等待 事务1等待事务2commit,事务2等待事务1commit，造成死锁]    解决 消除uniq_customer_id_visit_customer_id的X锁等待，在事务delete之前，先进行select查询是否存在记录，不存在则不进行delete操作，避免事务获取到X锁，避免循环等待。\n另一种死锁情况(非主键索引更新引起的死锁) 业务 数据库表相比情况一缺少了uniq_customer_id_visit_customer_id这个唯一索引(主要是历史问题导致的,一开始就没有唯一索引，后续已经产生了很多脏数据不好添加索引)，业务逻辑是相同的。\n非主键索引行锁相关 关于MySql行锁：行级锁并不是直接锁记录，而是锁索引，如果一条SQL语句用到了主键索引，mysql会锁住主键索引；如果一条语句操作了非主键索引，mysql会先锁住非主键索引，再锁定主键索引。\n依据索引执行DELETE/UPDATE的执行步骤： 由于用到了非主键索引，首先需要获取index_customer_id|index_visit_customer_id上的行级锁(锁非聚簇索引) 根据主键进行更新，所以需要获取主键上的行级锁(锁聚簇索引) 更新完毕，提交并释放所有的锁  语句分析EXPLAIN 发现执行的时候使用的type是index merge。在mysql5.0之前，一个表仅仅能使用一个索引，从5.1开始，引入了 index merge 优化技术，对同一个表可以使用多个索引分别进行条件扫描。 使用index merge的情况下 ，update/delete 需要对多个非主键索引相继获取锁，再获取主键上的锁。\n并发情况下,死锁发生 在并发情况下，可能会出现\n   事务1 事务2      begin    DELETE FROM customer_visit where customer_id = 1003 and visit_customer_id =1002[走主键索引，获取pk锁]   begin    DELETE FROM customer_visit where customer_id = 1001 and visit_customer_id =1002[走非主键索引：获取index锁]    Deadlock 事务2等待获取index锁，事务1等待获取pk锁，形成循环等待，导致死锁   "
            }
    
        ,
            {
                "id": 42,
                "href": "https://chinalhr.github.io/post/vertx-record-2/",
                "title": "Vert.x基础：Web模块",
                "section": "post",
                "date" : "2019.04.06",
                "body": " Vert.x 官方文档阅读\n Web组件 Router Router 接收 HTTP 请求，并查找首个匹配该请求的 Route，然后将请求传递给这个 Route。每一个被路由的请求对应一个唯一的 RoutingContext，这个实例会被传递到所有处理这个请求的处理器上。\nclass MainVerticle : AbstractVerticle() { override fun start(startFuture: Future\u0026lt;Void\u0026gt;) { val server = vertx .createHttpServer() val router = Router.router(vertx) router.route(\u0026#34;/hello\u0026#34;).handler { val response = it.response() response.putHeader(\u0026#34;content-type\u0026#34;, \u0026#34;text/plain\u0026#34;) response.end(\u0026#34;Hello World from Vert.x-Web!\u0026#34;) } server.requestHandler(router).listen(8000) } }  routingContext.next():让其他匹配的 Route 来处理请求 使用阻塞式handler(会使用 Worker Pool 中的线程而不是 Event Loop 线程来处理请求)  router.route().blockingHandler(routingContext -\u0026gt; { // 执行某些同步的耗时操作  service.doSomethingThatBlocks(); });  路由  //精确路径 router.route().path(\u0026#34;/some/path/\u0026#34;) //路径前缀 router.route().path(\u0026#34;/some/path/*\u0026#34;) //捕捉路径参数: Route route = router.route(HttpMethod.POST, \u0026#34;/catalogue/products/:producttype/:productid/\u0026#34;); route.handler(routingContext -\u0026gt; { String productType = routingContext.request().getParam(\u0026#34;producttype\u0026#34;); String productID = routingContext.request().getParam(\u0026#34;productid\u0026#34;); // 执行某些操作... }); //正则表达式路由 router.route().pathRegex(\u0026#34;.*foo\u0026#34;); //HTTP Method路由(不指定路径默认拦截全部) Route route = router.route(HttpMethod.POST, \u0026#34;/some/path/\u0026#34;); //consumes请求媒体类型（MIME types）路由 router.route().consumes(\u0026#34;text/html\u0026#34;).handler(routingContext -\u0026gt; { // 所有 `content-type`消息头的值为的请求会调用这个处理器`text/html`，可以匹配多个，也可以模糊匹配 }); //produces响应MIME类型 router.route().produces(\u0026#34;application/json\u0026#34;)  路由组合  Route route = router.route(HttpMethod.PUT, \u0026#34;myapi/orders\u0026#34;) .consumes(\u0026#34;application/json\u0026#34;) .produces(\u0026#34;application/json\u0026#34;);  路由顺序 默认的路由的匹配顺序与添加到 Router 的顺序一致。order 方法为每一个路由指定一个序号  route2.order(x);\n 子路由 目的：分隔为多个 Router，达到复用handler  实现：通过将一个 Router 挂载到另一个 Router 的挂载点上。挂载的 Router 被称为子路由（Sub Router）。Sub router 上也可以挂载其他的 sub router。\nRouter mainRouter = Router.router(vertx); Router restAPI = Router.router(vertx); mainRouter.mountSubRouter(\u0026#34;/productsAPI\u0026#34;, restAPI); // REST API /productsAPI/products/id restAPI.get(\u0026#34;/products/:productID\u0026#34;).handler(rc -\u0026gt; { //... });  错误处理 failureHandler进行错误处理  RoutingContext 在请求的生命周期中，您可以通过路由上下文 RoutingContext 来维护任何您希望在处理器之间共享的数据。\n参考 Vert.x 官方文档中文翻译\n"
            }
    
        ,
            {
                "id": 43,
                "href": "https://chinalhr.github.io/post/vertx-record-1/",
                "title": "Vert.x基础：Core模块",
                "section": "post",
                "date" : "2019.04.02",
                "body": " Vert.x 官方文档阅读\n 三大优势 Vert.x是一个在JVM上构建响应式应用的工具集，Vert.x是 事件驱动的，同时也是非阻塞的。\n 上下游异步生态 对有状态应用友好的线程模型 内置集群模式的EventBus，高伸缩性  线程和编程模型 有别于基于Servlet容器的线程模型，每个网络客户端在连接时被分配一个线程，并且该线程处理客户端直到它断开连接。 这种\u0026quot;同步I / O\u0026quot;线程模型保持简单理解的优势，但是没有了可扩展性，在重负载下一个操作系统内核花费显著时间在线程调度管理。\nVert.x中的部署单位称为Verticle。Verticle通过事件循环处理传入事件，其中事件可以是接收网络缓冲区，定时事件或其他Verticle发送的消息。\n每个事件循环都附加到一个线程。默认情况下，Vert.x为每个CPU核心线程附加2个事件循环。常规Verticle总是处理同一线程上的事件，因此不需要使用线程协调机制来操纵Verticle状态\n组件 Vert.x Core(Vert.x 的核心API)  功能  - 编写 TCP 客户端和服务端 - 编写支持 WebSocket 的 HTTP 客户端和服务端 - 事件总线 - 共享数据 —— 本地的Map和分布式集群Map - 周期性、延迟性动作 - 部署和撤销 Verticle 实例 - 数据报套接字 - DNS客户端 - 文件系统访问 - 高可用性 - 集群  Vert.x 的 API 大部分都是事件驱动的,所有API都不会阻塞调用线程。  因为Vert.x API不会阻塞线程，所以通过Vert.x您可以只使用少量的线程来处理大量的并发。  CompositeFuture/Future(并发异步操作)  CompositeFuture.all:接受多个 Future,都成功返回成功的 Future，任意一个失败返回失败的Future CompositeFuture.any:任意一个成功返回成功的Future,都失败返回失败的 Future CompositeFuture.join:等待所有的 Future 完成，无论成败 Future.compose:于顺序组合 Future  阻塞/非阻塞访问文件系统(FileSystem) HTTP/TCP/UDP支持  Vertx实例(入口) Vertx实例(instance)可以通过静态工厂方法创建，一个Vertx实例可以用于部署HTTP、TCP Server,client等等操作，可以将Vertx instance看做一个Vert.x Core API 的入口。\nEventloop  Multi-Reactor 模式  每个 Vertx 实例维护的是 多个Event Loop线程。默认情况下根据机器上可用的核数量来设置 Event Loop 的数量。通过Event driven的方式，将Events分发到handlers中进行处理。 任何event都不会阻塞Eventloop线程，这样Eventloop线程可以一直保持高速分发Events的速度。如果QPS为100/s的服务，需要在1s内分发到每个event，Eventloop数量为1，Eventloop必须能在10ms内分发完每个event，一旦Eventloop被阻塞，整个服务器吞吐量都会受到极大影响。所以Eventloop绝对不应该被阻塞住，所有交给Eventloop去处理的event都应该是non-blocking(I/O)的方式或者CPU执行时间较短的，才能确保每个event得到及时的分发。 默认情况下，Vert.x为每个CPU核心线程附加2个事件循环对应Verticle总是处理同一线程上的事件，因此不需要使用线程协调机制来操纵Verticle状态  在Event Loop运行阻塞代码  executeBlocking: 异步执行指定阻塞式代码 Worker Verticle：使用 Worker Pool 中的某个线程来执行阻塞代码 Vertx vertx = Vertx.vertx(); Context Context对应的是一次Handler执行过程的执行context。\nVert.x 传递一个事件给处理器或者调用 Verticle 的 start 或 stop 方法时，它会关联一个 Context 对象(绑定到了一个特定的 Event Loop 线程上),此 Verticle 上所有的普通代码都会在此 Context 上执行。\n 用处  用来管理控制着handlers或者handlers创建的任务执行时的scope和order的。当使用Vert.x API把event分发到handler时，例如设置一个HttpServer的request handler时，会将该handler的callback和一个context绑定起来，这个Context就会被用来调度callbacks。如果执行任务当前所在的线程是Vert.x线程的话，那么该任务将复用已经和线程所绑定的context；如果当前线程不是Vert.x线程，那么就创建一个新的context。\n context种类  Eventloop Context Worker Context Multi-Thread worker Context    Eventloop Context 会和一个Eventloop线程绑定在一起，保证该Context下的executions一直在相同的Eventloop线程中。一个context只会在一个线程中执行，而一个线程可能会被多个contexts所使用。\nVerticle   Verticle使用Actor模式保管代码组件，一个应用程序通常是由在同一个 Vert.x 实例中同时运行的许多 Verticle 实例组合而成。不同的 Verticle 实例通过向 Event Bus 上发送消息来相互通信。\n  当部署一个新的Verticle时，会为这个Verticle创建一个新的Context, 每个Verticle会一直关联一个context，任何在这个Verticle上面注册的handler都会通过verticle的context关联。\n  Verticle 种类\n  Stardand Verticle： 这种Verticle会被指派到创建和启动时的Eventloop线程上,Vert.x会保证你在这个Verticle实例上调用任何的handler操作将在同样的eventloop线程上面执行。 Worker Verticle： 目标是为了执行阻塞的代码，会运行在 Worker Pool 中的线程上。Worker verticle在同一个时间片内只会被Vert.x执行在一个线程内，它不会被并发执行，但是不同时间段有可能被不同线程执行。 Multi-Threaded Worker Verticle： 这类 Verticle 也会运行在 Worker Pool 中的线程上。一个实例可以由多个线程同时执行,因此需要开发者自己确保线程安全。  部署，设置Verticle  //部署 vertx.deployVerticle(\u0026#34;com.mycompany.MyOrderProcessorVerticle\u0026#34;,options); //撤销 vertx.undeploy(deploymentID, res -\u0026gt; { if (res.succeeded()) { System.out.println(\u0026#34;Undeployed ok\u0026#34;); } else { System.out.println(\u0026#34;Undeploy failed!\u0026#34;); } }); //配置 Verticle相关参数(部署多个实例来利用所有的CPU) DeploymentOptions options = new DeploymentOptions().setInstances(16); //设置JSON参数， DeploymentOptions options = new DeploymentOptions().setConfig(config);  简单总结  Verticle是个类，通常我们继承AbstractVerticle来实现自己的业务。 一个Vert.x程序由一个或多个Verticle组成，且必有一个Verticle是程序的入口。 Verticle有生命周期，通常我们关心start与stop方法。 在一个Verticle可以部署其他任意的Verticle，也可以让已部署的下架。这些操作都会命中上面说的生命周期方法。 Verticle除了可以在部署的时候传递参数。也可以通过EventBus在任意Verticle间用收发消息的方式传递数据。 一个Verticle中的代码，永远只会被一个线程访问，这里有三个例外:  自己在代码中手动操作Thread 部署Verticle时追加设置setMultiThreaded(true)（但是这种用法已经被标记为@deprecated） 通过vertx.executeBlocking执行阻塞式代码时ordered参数传false   所以，如果按照Verticle的使用规范编写代码，大部分情况下不会出现线程不安全的情况。 Verticle根据应用场景不同，分为两种，一种是普通版，一种是work版。简单的说就是比较耗时的阻塞式代码应该放在work Verticle中执行。 普通版Verticle用的线程池是eventloop-thread，而work版的线程池叫worker-thread。前者是绑定式实用，部署Verticle的时候，分配一个线程供它使用，就不会再变了；但是后者不是。    AbstractVerticle AbstractVerticle是Vert.x实现了Verticle接口提供给用户使用的一个抽象类，将自己的类继承自AbstractVerticle，在start()方法或者stop()方法中自定义一些操作，然后通过vertx实例来部署verticle。\n综上,Vert.x应用运行时如下：\nEvent Bus 每一个 Vert.x 实例都有一个单独的 Event Bus 实例。通过 Vertx 实例的 eventBus 方法来获得对应的 EventBus 实例。应用中的不同部分通过 Event Bus 相互通信。\n 基本  //获取Event Bus EventBus eb = vertx.eventBus(); //注册handler MessageConsumer\u0026lt;String\u0026gt; consumer = eb.consumer(\u0026#34;news.uk.sport\u0026#34;); consumer.handler(message -\u0026gt; { System.out.println(\u0026#34;I have received a message: \u0026#34; + message.body()); }); //注销handler consumer.unregister(res -\u0026gt; { if (res.succeeded()) { System.out.println(\u0026#34;The handler un-registration has reached all nodes\u0026#34;); } else { System.out.println(\u0026#34;Un-registration failed!\u0026#34;); } }); //一对多发布消息(发送给所有注册handler) eb.publish(\u0026#34;news.uk.sport\u0026#34;, \u0026#34;Yay! Someone kicked a ball\u0026#34;); //一对一发布消息(发送给一个注册handler) eventBus.send(\u0026#34;news.uk.sport\u0026#34;, \u0026#34;Yay! Someone kicked a ball\u0026#34;); //设置参数 DeliveryOptions options = new DeliveryOptions(); options.addHeader(\u0026#34;some-header\u0026#34;, \u0026#34;some-value\u0026#34;); eventBus.send(\u0026#34;news.uk.sport\u0026#34;, \u0026#34;Yay! Someone kicked a ball\u0026#34;, options); //应答消息(消费者可以通过调用 reply 方法来应答这个消息) MessageConsumer\u0026lt;String\u0026gt; consumer = eventBus.consumer(\u0026#34;news.uk.sport\u0026#34;); consumer.handler(message -\u0026gt; { System.out.println(\u0026#34;I have received a message: \u0026#34; + message.body()); message.reply(\u0026#34;how interesting!\u0026#34;); }); //接收消息 eventBus.send(\u0026#34;news.uk.sport\u0026#34;, \u0026#34;Yay! Someone kicked a ball across a patch of grass\u0026#34;, ar -\u0026gt; { if (ar.succeeded()) { System.out.println(\u0026#34;Received reply: \u0026#34; + ar.result().body()); } });  编解码  //为Event Bus注册编解码器 MessageCodec，需要在DeliveryOptions指定名称，也可以为某个类指定特定的编解码 eventBus.registerCodec(myCodec); DeliveryOptions options = new DeliveryOptions().setCodecName(myCodec.name()); eventBus.send(\u0026#34;orders\u0026#34;, new MyPOJO(), options);  EventBusOptions(Event Bus相关配置)  Vert.x 共享数据 共享数据（Shared Data）包含的功能允许您可以安全地在应用程序的不同部分之间、同一 Vert.x 实例中的不同应用程序之间或集群中的不同 Vert.x 实例之间安全地共享数据。\n 本地共享Map(LocalMap)  在同一个 Vert.x 实例中的不同 Event Loop（如不同的 Verticle 中）之间安全共享数据。 SharedData sd = vertx.sharedData(); LocalMap\u0026lt;String, String\u0026gt; map1 = sd.getLocalMap(\u0026#34;mymap1\u0026#34;); // String是不可变的，所以不需要复制 map1.put(\u0026#34;foo\u0026#34;, \u0026#34;bar\u0026#34;); LocalMap\u0026lt;String, Buffer\u0026gt; map2 = sd.getLocalMap(\u0026#34;mymap2\u0026#34;); // Buffer将会在添加到Map之前拷贝 map2.put(\u0026#34;eek\u0026#34;, Buffer.buffer().appendInt(123));  分布式异步Map(允许从集群的任何节点将数据放到 Map 中，并从任何其他节点读取)  SharedData sd = vertx.sharedData(); sd.\u0026lt;String, String\u0026gt;getClusterWideMap(\u0026#34;mymap\u0026#34;, res -\u0026gt; { if (res.succeeded()) { AsyncMap\u0026lt;String, String\u0026gt; map = res.result(); } else { //...  } });  分布式锁(在集群中获取独占锁，以达到只在集群一个节点上执行某些操作或访问资源)  sd.getLockWithTimeout  分布式计数器  sd.getCounter 数据访问库  JDBC client  基于JDBC Drive使用线程简单封装，用法变成异步的，但本质还是原来JDBC的阻塞模型。\n MySQL / PostgreSQL client  基于Mauricio Linhares的异步驱动模型实现的(已废弃)，后续使用jasync异步驱动库实现，不会有线程切换等高负荷操作存在。\n参考 Vert.x 官方文档中文翻译\n"
            }
    
        ,
            {
                "id": 44,
                "href": "https://chinalhr.github.io/post/kotlin-learn03/",
                "title": "Kotlin基础：泛型、注解、DSL",
                "section": "post",
                "date" : "2019.03.14",
                "body": " Kotlin学习记录\n 泛型  对比Java  和 Java 不同， Kotlin 始终要求类型实参要么被显式地说明，要么能被编译器推导出来。(因为Java有历史包袱,1.5才引入泛型)\n 泛型类型约束  约束上界 : \u0026lt; T : Number\u0026gt; T sum(List \u0026lt; T\u0026gt; list)\n 擦除和实化类型参数  JVM 上的泛型一般是通过类型擦除实现的，泛型类实例的类型实参在运行时是不保留的。\nvar items = listOf(\u0026#34;lhr\u0026#34;,\u0026#34;lnx\u0026#34;,2) /** * 运行时泛型被擦除了，无法判断List的类型 */ fun main(args: Array\u0026lt;String\u0026gt;) { if (items is List\u0026lt;String\u0026gt;){ //...  } } 声明带实化类型参数的函数(基于inline内联函数)\nKotlin 有特殊的语法结构可以允许你在函数体中使用具体的类型泛型实参，但只有 inline 函数可以。\n/** * 内联函数 reified声明了类型参数T不会在运行时被擦除 */ inline fun \u0026lt;reified T\u0026gt; Iterable\u0026lt;*\u0026gt;.filterIsInstance():List\u0026lt;T\u0026gt;{ val destination= mutableListOf\u0026lt;T\u0026gt;() /* 对指定实参进行检测存储返回。 因为生成的字节码引用了具体类，而不是类型参数，它不会被运行时发生的类型参数擦除影响 。 */ for(element in this){ if (element is T){ destination.add(element) } } return destination; } fun main(args: Array\u0026lt;String\u0026gt;) { val list = listOf(\u0026#34;123\u0026#34;, \u0026#34;456\u0026#34;, 999) println(list.filterIsInstance\u0026lt;String\u0026gt;()) } 泛型协变、逆变和不变 概念 假设Orange类是Fruit类的子类，Crate是一个泛型类\ntype variance（型变）:指我们是否允许对参数类型进行子类型转换 invariance（不型变）：也就是说，Crate\u0026lt;Orange\u0026gt; 和 Crate\u0026lt;Fruit\u0026gt; 之间没有关系 covariance（协变）：也就是说，Crate\u0026lt;Orange\u0026gt; 是 Crate\u0026lt;Fruit\u0026gt; 的子类型[Java Crate\u0026lt;Orange\u0026gt; 是 Crate\u0026lt;? extends Fruit\u0026gt; 的子类型] contravariance（逆变）：也就是说，Crate\u0026lt;Fruit\u0026gt; 是 Crate\u0026lt;Orange\u0026gt; 的子类型。[Crate\u0026lt;Fruit\u0026gt; 是 Crate\u0026lt;? super Orange\u0026gt; 的子类型] Java Java处理型变的做法概括起来是：Java中的泛型类在正常使用时是不型变的，要想型变必须在使用处通过通配符进行（称为使用处型变）。\n协变\nList\u0026lt;? extends Fruit\u0026gt; fruits = new ArrayList\u0026lt;\u0026gt;(); //编译错误:不能添加任何类型的对象 //fruits.add(new Orange()); //fruits.add(new Fruit()); //fruits.add(new Object()); //我们知道，返回值肯定是Fruit Fruit f = fruits.get(0); fruits的类型是List\u0026lt;? extends Fruit\u0026gt;，代表Fruit类型或者从Fruit继承的类型的List，fruits可以引用诸如Fruit或Orange这样类型的List，然后向上转型为了List\u0026lt;? extends Fruit\u0026gt;。我们并不关心fruits具体引用的是ArrayList\u0026lt;Orange\u0026gt;()，还是ArrayList\u0026lt;Fruit\u0026gt;()，对于类型 List\u0026lt;? extends Fruit\u0026gt; 我们所能知道的就是：调用一个返回Fruit的方法是安全的，因为你知道，这个List中的任何对象至少具有Fruit类型。\n我们之所以可以安全地将 ArrayList向上转型为 List\u0026lt;? extends Fruit\u0026gt;，是因为编译器限制了我们对于 List\u0026lt;? extends Fruit\u0026gt; 类型部分方法的调用。例如void add(T t)方法，以及一切参数中含有 T 的方法（称为消费者方法）。因为这些方法可能会破坏类型安全，只要限制这些方法的调用，就可以安全地将 ArrayList转型为 List\u0026lt;? extends Fruit\u0026gt;。\n协变：通过限制对于消费者方法的调用，使得像 List\u0026lt;? extends Fruit\u0026gt; 这样的类型成为单纯的“生产者”，以保证运行时的类型安全\n逆变\nList\u0026lt;Object\u0026gt; objs = new ArrayList\u0026lt;\u0026gt;(); objs.add(new Object()); List\u0026lt;? super Fruit\u0026gt; canContainFruits = objs; //没有问题，可以写入Fruit类及其子类 canContainFruits.add(new Orange()); canContainFruits.add(new Banana()); canContainFruits.add(new Fruit()); //无法安全地读取,canContainFruits完全可能包含Fruit基类的对象，比如这里的Object //Fruit f = canContainFruits.get(0);  //总是可以读取为Object，然而这并没有太多意义 Object o = canContainFruits.get(1); canContainFruits的类型是List\u0026lt;? super Fruit\u0026gt;，代表Fruit类型或者Fruit基类型的List，canContainFruits可以引用诸如Fruit或Object这样类型的List，然后向上转型为了List\u0026lt;? super Fruit\u0026gt;。对于List中的 T get(int pos) 方法，当指定类型是 “？ super Fruit” 时，get方法的返回类型就变成了 “？ super Fruit”，也就是说，返回类型可能是Fruit或者任意Fruit的基类型，我们不能确定，因此编译器拒绝调用任何返回类型为 T 的方法（除非我们只是读取为Object类）\n逆变：编译器限制了我们对于 List\u0026lt;? super Fruit\u0026gt; 类型部分方法的调用。例如T get(int pos)方法，以及一切返回类型为 T 的方法（称为生产者方法），因为我们不能确定这些方法的返回类型，只要限制这些方法的调用，就可以安全地将 ArrayList转型为 List\u0026lt;? super Fruit\u0026gt;。这就是所谓的逆变，通过限制对于生产者方法的调用，使得像 List\u0026lt;? super Fruit\u0026gt; 这样的类型成为单纯的“消费者”。\n总结\nextends限定了通配符类型的上界，所以我们可以安全地从其中读取；而super限定了通配符类型的下界，所以我们可以安全地向其中写入。\nKotlin Kotlin中的泛型类在定义时即可标明型变类型（协变或逆变，当然也可以不标明，那就是不型变的），在使用处可以直接型变（称为声明处型变）。\n协变\n使用out修饰符，表明类型参数 T 在泛型类中仅作为方法的返回值，不作为方法的参数，因此，这个泛型类是个协变的。回报是，使用时Source可以作为Source的子类型。\nabstract class Source\u0026lt;out T\u0026gt; { abstract fun nextT(): T } fun demo(oranges: Source\u0026lt;Orange\u0026gt;) { val fruits: Source\u0026lt;Fruit\u0026gt; = oranges // 没问题，因为 T 是一个 out-参数，Source\u0026lt;T\u0026gt;是协变的  val oneFruit: Fruit = fruits.nextT() //可以安全读取 } 逆变\n使用in修饰符，表明类型参数 T 在泛型类中仅作为方法的参数，不作为方法的返回值，因此，这个泛型类是个逆变的。回报是，使用时Comparable可以作为Comparable的子类型。\n//kotlin abstract class Comparable\u0026lt;in T\u0026gt; { abstract fun compareTo(other: T): Int } fun demo(x: Comparable\u0026lt;Number\u0026gt;) { val y: Comparable\u0026lt;Double\u0026gt; = x // OK！逆变，Comparable\u0026lt;Number\u0026gt;可以作为Comparable\u0026lt;Double\u0026gt;的子类型  y.compareTo(1.0) //1.0 拥有类型 Double } 总结\nout和in修饰符是自解释的。out代表泛型类中，类型参数 T 只能存在于方法的返回值中，即是作为输出，因此，泛型类是生产者/协变的；in代表泛型类中，类型参数T只能存在于方法的参数中，即是作为输入，因此，泛型类是消费者/逆变的。\n注解   注解实参需要在编译期就是己知的，所以你不能引用任意的属性作为实参。要把属性当作注解实参使用，你需要用const修饰符标记它，来告知编译器这个属性是编译期常量。\n  Kotlin 允许你对任意的表达式应用注解，而不仅仅是类和函数的声明及类型。\n  用注解控制JavaAPI\n  //volatile 多线程可见性 @volatile //Strictfp 多平台浮点数可预测性 @Strictfp //改变由 Kotlin 生成的 Java 方法或字段的名称 @JvmName //能被用在对象声明或者伴生对象的方法上，把它们暴露成 Java 的静态方法 @JvmStatic //指导 Kotlin 编译器为带默认参数值的函数生成多个重载 @JvmOverloads //应用于一个属性，把这个属性暴露成一个没有访问器的 公有 Java 字段。 @JvmField  注解使用  //声明注解 //@Target 元注解 @Target(AnnotationTarget.PROPERTY) annotation class xxx DSL 关于DSL   DSL DSL 也就是 Domain Specific Language 的简称，是指为某些特定领域（domain）设计的专用语言。最常见的 DSL 就是 SQL和正则表达式。和通用编程语言相反，DSL大部分是声明式的(声明式语言描述了想要的结果并将执行细节留给了解释它的引擎)。\n  内部DSL 与有着自己独立语法的外部 DSL 不同，内部 DSL是用通用编程语言编写的程序的一部分，使用了和通用编程语言完全一致的语法。实际上，内部 DSL 不是完全独立的语言，而是使用主要语言的特定方式，同时保留具有独立语法的 DSL 的主要优点。 [使用Kotlin 语言开发的，解决特定领域问题，具备独特代码结构的 API]\n  DSL结构 DSL 通常会依赖在其他上下文中也会广泛使用的语言特性，比如说中缀调用和运算符重载。DSL与API不同的显著特征是结构/文法。 与API的命令查询不同, DSL 的方法调用存在于由 DSL 文法定义的 更大的结构中。\n  Kotlin DSL Kotiin 的 DSL 是完全静态类型的[编译时错误的检测，以及更好的 IDE 支持]。\n  Kotlin DSL使用 利用带接收者的lambda和扩展函数实现HTML Table Sample DSL\nopen class Tag(val name: String) { /** * 保存所有嵌套标签 */ private val children = mutableListOf\u0026lt;Tag\u0026gt;() protected fun \u0026lt;T : Tag\u0026gt; doInit(child: T, init: T. () -\u0026gt; Unit) { //调用作为参数的lambda,初始化并保存子标签  child.init() children.add(child) } override fun toString() = \u0026#34;\u0026lt;$name\u0026gt;${children.joinToString(\u0026#34;\u0026#34;)} \u0026lt;/$name\u0026gt;\u0026#34; } fun table(init: TABLE.() -\u0026gt; Unit) = TABLE().apply(init) class TABLE : Tag(\u0026#34;table\u0026#34;) { /** * 创建并初始化 TR 标签的示例并添加到TABLE 的子标签中 */ fun tr(init: TR.() -\u0026gt; Unit) = doInit(TR(), init) } class TR : Tag(\u0026#34;tr\u0026#34;) { /** * 添加TD标签的一个实例到 TR 的子标签中 */ fun td(init: TD.() -\u0026gt; Unit) = doInit(TD(), init) } class TD : Tag(\u0026#34;td\u0026#34;) fun main(args: Array\u0026lt;String\u0026gt;) { fun createTable() = table { tr { td { } td { } } } //\u0026lt;table\u0026gt;\u0026lt;tr\u0026gt;\u0026lt;td\u0026gt; \u0026lt;/td\u0026gt;\u0026lt;td\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt;  println(createTable()) } 关于带接收者的lambda与扩展函数\n让我们使用一个结构来构建 API。当函数被调用的时候需要提供这个对象，它在函数体内是可用的。实际上 ，一个扩展函数类型描述了一个可以被当作扩展函数来调用的代码块。\n调用扩展函数类型lamdba时，可以像调用一个扩展函数那样调用 lambda，而不是将对象作为参数传递给 lambda。\n/** * action: StringBuilder.() -\u0026gt; Unit 定义接收者为StringBuilder的lambda * StringBuilder.() -\u0026gt; Unit 扩展函数 */ fun buildString(action: StringBuilder.() -\u0026gt; Unit):String{ val sb = StringBuilder() //传递一个 StringBuilder 实例作为lambda的接收者  sb.action() return sb.toString() } var s = buildString { append(\u0026#34;are you ok\u0026#34;) } invoke约定\ninvoke约定允许把自定义类型的对象当作函数一样调用。(类似于运算符重载)\nclass Person(val name: String){ /** * 重载invoke */ operator fun invoke(no: String){ println(\u0026#34;$name - $no \u0026#34;) } } fun main(args: Array\u0026lt;String\u0026gt;) { val person = Person(\u0026#34;lhr\u0026#34;) person(\u0026#34;cp272727\u0026#34;)//实际上调用的是person.invoke  } 中缀调用 DSL清晰的语法\nobject start infix fun String.should(x: start): StartWrapper = StartWrapper(this) class StartWrapper(val value: String) { infix fun with(prefix: String): Unit = if (value.startsWith(prefix)) { } else { throw AssertionError(\u0026#34;String does not start with $prefix: $value\u0026#34;) } } fun main(args: Array\u0026lt;String\u0026gt;) { \u0026#34;kotlin\u0026#34; should start with \u0026#34;ko\u0026#34; } 基本数据类型扩展\nval Int.days: Period //this 引用数字常量值  get() = Period . ofDays (this) val Period.ago:LocalDate get() = LocalDate.now() - this fun main(args: Array\u0026lt;String\u0026gt;) { //获取一天前日期  println(1.days.ago) } 协程相关 参考: http://johnnyshieh.me/categories/kotlin/\n"
            }
    
        ,
            {
                "id": 45,
                "href": "https://chinalhr.github.io/post/kotlin-learn02/",
                "title": "Kotlin基础：类型系统、语言特性",
                "section": "post",
                "date" : "2019.02.27",
                "body": " Kotlin学习记录\n 可Null性 java类型系统缺陷：\n例如String类型的变量可以持有两种值，分别是String的实例和 null，但两种值完全不一样(instanceof 运算符null不是String)。类型系统的缺陷导致即使变量拥有声明的类型 String你依然无法知道能对该变量的值做些什么，除非做额外的检查。\nKotlin解决NullPointerException之道：把运行时的错误转变成编译期的错误\n  编译器强制检测变量实参,所有常见类型默认都是非空的,可以在任何类型的后面加?来表示这个类型的变量可以存储 null 引用。但是对可null参数进行的操作也会受到限制，需要对可null参数进行null处理(判断，安全符\u0026hellip;)才可以编译通过。\n  Java 中的类型在Kotlin中被解释成平台类型，允许开发者把它们当作可空或非空来对待。\n  fun strLen(s: String?) = s?.length  安全运算符 ?. 和 ?: 安全Null运算符  /** * ?.把一次 null 检查和一次方法调用合并成一个操作 * 下面两个函数是等效的 */ fun strLen(s: String?) = s?.length fun strLenOld(s: String?) = { if (s != null) s.length else null } /** * Elvis 运算符 代替 null 的默认值，也可配合throw食用 */ val t: String = s ?: \u0026#34;\u0026#34; val t: String = s ?: throw IllegalArgumentException(\u0026#34;String is null\u0026#34;) /** * 链式安全调用 */ class Address(val streetAddress: String, val zipCode: Int, val city: String, val country: String) class Company(val name: String, val address: Address ? ) class Person(val name: String, val company: Company?) fun Person.countryName() :String{ val companyStr = this.company?.address?.country return companyStr?:\u0026#34;UnKnow\u0026#34; } as? 安全类型转换运算符\n/** * as? 尝试把值转换成指定的类型， 如果值不是合适的类型就返回 null * 配合Elvis 运算符 */ val otherPerson = o as? Person?: return false 非Null断言!!\n/** * !!可以把任何值转换成非空类型。如果对 null 值做非空断言，则会抛出异常。 */ fun ignoreNulls(s: String?) { val sNotNullVal = s!! //如果s为null,此处抛出KotlinNullPointerException异常  println(sNotNullVal) } let\n/** * let:允许你对表达式求值，检查求值结果是否为 null ,并把结果保存为一个变量 * 只在表达式不为 null 时执行 lambda,相当于 * if (email != null) sendEmailTo(email) */ email?.let { email -\u0026gt; sendEmailTo(email) } lateinit 延迟初始化\n/** * 不需要在构造方法中初始化它。如果在属性被初始化之前就访问了它，会抛出异常 */ private lateinit var myService: MyService  Kotlin与Java Null操作 Java 的类型系统是不支持可空性的  Kotlin注解转换：Java 中的@ Nullable String 被 Kotlin 当作 String?,而@NotNull String 就是String\nKotlin编译器无法对Java的可Null类型检测，开发者负责正确处理来自 Java 的值。\n数据类型  对比Java  整数类型 Byte Short Int Long 浮点数类型 Float Double 字符类型 Char 布尔类型 Boolean\n对比Java，Kotlin不区分基本数据类型和它们的包装类。对于变量、属性、参数和返回类型Kotlin编译器会尽可能编译为基本类型，除了泛型。\nKotiin不会自动地把数字从一种类型转换成另外一种，即便是转换成范围更大的类型。可以使用支持双向转换的函数,toByte(),toShort()\u0026hellip; Kotlin 要求转换必须是显式的，可以避免转换数值的问题。\n  根类型\u0026rsquo;Any\u0026rsquo; \u0026lsquo;Any?\u0026rsquo; Any类型是Kotiin所有非空类型的超类型，包括基础类型,区别于Java Object 只是所有引用类型的超类型，非基础类型\n  Unit 类型 和 Nothing 类型 Unit是Kotlin 的 \u0026lsquo;void\u0026rsquo; ，区别是Unit是一个完备的类型，可以作为类型参数。只存在一个值是 Unit 类型，这个值也叫作 Unit，并且（在函数中）会被隐式地返回。\n  //下面两个函数等效 fun f () : Unit { . . . } fun f () { .. } interface Processor\u0026lt;T\u0026gt; { fun process(): T } class NoResultProcessor : Processor\u0026lt;Unit\u0026gt;{ override fun process() { //不需要显示return 会隐式返回Unit  } } Nothing 类型没有任何值,只有被当作函数返回值使用,表示函数永不返回\n 集合数组  集合\nList\u0026lt;Int?\u0026gt; : 列表中的单个值是可Null的 List? : 集合本身是可Null的,但列表中的元素保证是非空的 List\u0026lt;Int?\u0026gt;? :集合本身是可Null的,列表中的单个值是可Null的\n可以使用filterNotNull()函数过滤集合\n只读集合，可变集合\n对比Java,Kotlin把访问集合数据的接口和修改集合数据的接口分开了。Collection接口(读)，MutableCollection接口(写)\n优势:让程序 中的数据发生的事情更容易理解，根据入参类型是可读或者可写的，很容易实现防御性编程，\nfun\u0026lt;T\u0026gt; copyElements(source:Collection\u0026lt;T\u0026gt;,target:MutableCollection\u0026lt;T\u0026gt;){ for (item in source){ target.add(item) } } 数组\nKotiin 中的一个数组是一个带有类型参数的类Array，其元素类型被指定为相应的类型参数。基本数据类型的数组使用像 IntArray 这样的特殊类来表示 。\n语言特性-约定 重载  二元运算符重载     表达式 函数名     a * b times   a / b div   a % b mod   a + b plus   a - b minus    /** * 定义一个名为plus的方法，operator重载+号运算符(扩展函数) */ operator fun Point.plus(other: Point): Point { return Point(x + other.x, y + other.y) } data class Point(val x: Int, val y: Int) fun main(args: Array\u0026lt;String\u0026gt;) { val p1 = Point(10, 20) val p2 = Point(30, 40) println(p1 + p2) }  一元运算符重载  用于重载一元运算符的函数，没有任何参数。\n   表达式 函数名     +a unaryPlus   -a unaryMinus   !a not   ++a,a++ inc   \u0026ndash;a,a\u0026ndash; dec     比较运算符重载 如果在 Kotiin 中使用＝＝(!=)运算符，它将被转换成 equals(!equals) 方法的调用。===恒等于等效于Java的==。  data class Point(val x: Int, val y: Int){ override fun equals(obj: Any?): Boolean { if(obj === this) return true //引用检测  if (obj !is Point) return false//类型检测  return obj.x == x \u0026amp;\u0026amp; obj.y == y } }  重载(下标访问get/set),in,rangeTo,iterator  operator fun Point.get(index: Int):Int{ return when(index){ 0 -\u0026gt; x 1 -\u0026gt; y else -\u0026gt; throw IndexOutOfBoundsException(\u0026#34;Invalid coordinate $index\u0026#34;) } } fun main(args: Array\u0026lt;String\u0026gt;) { val p1 = Point(10, 20) println(\u0026#34;p1 x-\u0026gt; ${p1[0]}y-\u0026gt; ${p1[1]}\u0026#34;) } 解构声明和组件函数 解构声明允许你展开单个复合值，并使用它来初始化多个单独的变量。\n/** * 数据类持有类型 */ data class NamesComponents(val name: String, val ext: String) fun splitFilename(fullName: String): NamesComponents { val reslut = fullName.split(\u0026#34;.\u0026#34;, limit = 2) return NamesComponents(reslut[0], reslut[1]) } fun main(args: Array\u0026lt;String\u0026gt;) { //解构声明展开类  val (name, ext) = splitFilename(\u0026#34;ext.so\u0026#34;) println(\u0026#34;name:$name ext $ext\u0026#34;) } 解构声明可用于集合循环\nfor (entry in map.entries) { val key = entry. cornponentl() val value= entry.cornponent2() } 委托属性 委托模式，这个模式应用于一个属性时，它也可以将访问器的逻辑委托给一个辅助对象。Kotlin中关键宇by可以用于任何符合属性委托约定规则的对象。\n用处：Observable，Listener(编译器自动实现代码)Delegates.observable 函数可以用来添加属性更改的观察者\n 委托属性  class Foo { // 对p的get/set操作调用对应Delegate的get/setValue var p: Type by Delegate () } //委托属性应用:存储到MySql/Map  class Person { private val _attributes = hashMapOf\u0026lt;String, String\u0026gt; () fun setAttribute(attrName: String, value: String) { -attributes[attrName] =value } //将map(_attributes) 作为委托属性 set/get name自动存储到Map  val name: String by _attributes }  惰性初始化 相比java的if(obj==null){//初始化并返回}else{//直接返回}，Kotlin可以更简单做到  class Person(val name:String){ val email by lazy { loadEmail(this) } } "
            }
    
        ,
            {
                "id": 46,
                "href": "https://chinalhr.github.io/post/algorithm-bitmap/",
                "title": "Bit-map算法及应用",
                "section": "post",
                "date" : "2019.01.27",
                "body": " Bit-map算法学习\n little-endian和big-endian Big endian 认为第一个字节是最高位字节（按照从低地址到高地址的顺序存放数据的高位字节到低位字节）；而 Little endian 则相反，它认为第一个字节是最低位字节（按照从低地址到高地址的顺序存放据的低位字节到高位字节）。\n关于Bit-map 用一个bit位来标记某个元素对应的Value， key0|1表示该元素是否存在(类似于桶排序算法)，由于采用了Bit为单位来存储数据，因此在存储空间方面，可以大大节省。\n假设我们要对0-7内的6个不重复元素排序，开辟一个8个Bit(1Bytes)的内存空间，此时所有的bit位为0，遍历6个元素，将对应下标的bit位置为1 (采用big-endian)。处理完所有元素后，对Bytes区域进行遍历，将该位是一的位的编号输出，达到排序的目的。对于更多的元素排序，可以扩展为Bytes数组进行存储排序。\n思想 使用bit数组来表示某些元素是否存在，达到节省空间的目的。\n算法实现[Java] public class BitmapTest { //保存数据的  private byte[] bitArray; //能够存储多少bit数据  private int capacity; public BitmapTest(int capacity) { this.capacity = capacity; this.bitArray = new byte[(capacity \u0026gt;\u0026gt;3 )+1]; } /** * byte[index] ——\u0026gt; Index(N) = N/8 = N \u0026gt;\u0026gt; 3; * bit位Position ——\u0026gt; Position(N) = N%8 = N \u0026amp; 0x07; */ public void add(int num){ // byte[]的index  int arrayIndex = num \u0026gt;\u0026gt; 3; // byte[index]的Position位置  int position = num \u0026amp; 0x07; //将1左移position后，Position位置置为1，然后和byte里的数据做|操作，byte数据里的position为1。  bitArray[arrayIndex] |= 1 \u0026lt;\u0026lt; position; } public void clear(int num){ // byte[]的index  int arrayIndex = num \u0026gt;\u0026gt; 3; // byte[index]的Position位置  int position = num \u0026amp; 0x07; //将1左移position后，Position位置置为1，然后对取反，再与byte里的数据做\u0026amp;操作，即可清除当前的位置了.  bitArray[arrayIndex] \u0026amp;= ~(1 \u0026lt;\u0026lt; position); } public boolean contain(int num){ // num/8得到byte[]的index  int arrayIndex = num \u0026gt;\u0026gt; 3; // num%8得到在byte[index]的Position位置  int position = num \u0026amp; 0x07; //将1左移position后，Position位置置为1，然后与byte里的数据做\u0026amp;，判断是否为0即可  return (bitArray[arrayIndex] \u0026amp; (1 \u0026lt;\u0026lt; position)) !=0; } } 常见实例😮   给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？\n Bitmap的方法，申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。    在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数\n 采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存。扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。   "
            }
    
        ,
            {
                "id": 47,
                "href": "https://chinalhr.github.io/post/mysql-bigdata-optimization/",
                "title": "MySql大数据表优化",
                "section": "post",
                "date" : "2019.01.23",
                "body": " 公司数据库账单单表存储过亿,急需优化\n 优化现有MySql 表设计   表字段避免null值出现，null值很难查询优化，占用额外的索引空\n  如果非负则加上UNSIGNED\n  尽量使用TIMESTAMP而非DATETIME\n TIMESTAMP:它把客户端插入的时间从当前时区转化为UTC（世界标准时间）进行存储。查询时，将其又转化为客户端当前时区进行返回。 DATETIME:不做任何改变，基本上是原样输入和输出。    索引建立\n 在WHERE和ORDER BY命令上涉及的列建立索引,利用最左前缀原则： 值分布很稀少的字段不适合建索引 字符字段只建前缀索引 多个单列索引在多条件查询时只会生效第一个索引,所以多条件联合查询时最好建联合索引 离散度大（不同的值多）的列，放在联合索引前面。 ...    外层优化  通过Redis进行热点缓存 通过Elasticsearch进行api对外查询的读写分离  分区[mysql支持的功能，业务代码无需改动]   关于分区\n 分区表底层是由多个物理子表组成，对应用是透明的，对分区表的请求会通过句柄对象转化为对存储引擎的接口调用。 MySql的分区表索引按照分区的子表定义，没有全局索引。 查询优化：优化器根据分区函数过滤分区，让查询扫描更少的数据。用户的SQL语句是需要针对分区表做优化，SQL条件中要带上分区条件的列，从而使查询定位到少量的分区上，否则就会扫描全部分区 避免分区表存在NULL值(NULL值会使分区过滤无效) 对于原生的RANGE分区，LIST分区，HASH分区，分区对象返回的只能是整数值 explain partitions 查看分区执行计划    分区类型\n  范围分区（RANGE）\n 允许将数据划分不同范围(从属于一个连续区间值的集合)[非null列] PARTITION BY RANGE(YEAR(separated))( PARTITION p0 VALUES LESS THAN(1995), PARTITION p1 VALUES LESS THAN(2000), PARTITION p2 VALUES LESS THAN(2005) );    列表分区（LIST）\n 允许将数据划分不同范围(从属于一个枚举列表值的集合)[非null列] LIST分区只支持整形，非整形字段需要通过函数转换成整形. PARTITION BY LIST(category)( PARTITION P0 VALUES IN (3,5), PARTITION P1 VALUES IN (1,10), PARTITION P2 VALUES IN (4,9), PARTITION P3 VALUES IN (2), PARTITION P4 VALUES IN (6) );    哈希分区（HASH）\n 基于给定的分区个数，将数据散列到不同的分区(HASH分区的底层实现基于MOD取余函数) 只能针对整数进行HASH，对于非整形的字段只能通过表达式将其转换成整数 PARTITION BY HASH(id) PARTITIONS 4; -- 分区数    哈希分区（LINEAR HASH）\n HASH分区的特殊类型，基于Power-of-Two算法...    KEY分区\n 基于给定的分区个数，将数据散列到不同的分区(KEY分区的底层实现基于列的MD5算法) KEY分区对象必须为列， PARTITION BY KEY(id) PARTITIONS 2;      分区的缺陷\n 分区表，分区键设计不太灵活，如果不走分区键，很容易出现全表锁 一旦数据量并发量上来，如果在分区表实施关联，很容易发生灾难 对比分库分表:分库分表，自己掌控业务场景与访问模式，可控。分区表，基于mysql底层机制，不太可控    分区的优势\n 冷热分离：表非常大且只在表的最后部分有热点数据，冷数据根据分区规则自动归档 定期淘汰历史数据：按时间写入，历史数据可淘汰，可快速删除，空间可快速回收 优化查询：在where字句中包含分区列时，分区可以大大提高查询效率，减少缓存开销、减少IO开销 统计性能提升：在涉及sum()和count()这类聚合函数的查询时，可以在每个分区上面并行处理，最终只需要汇总所有分区得到的结果。    分库分表[业务层改动实施]   垂直拆分表 垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表，建议在数据表设计之初就执行垂直拆分\n 把不常用的字段单独放在一张表; 把text，blob等大字段拆分出来放在附表中; 经常组合查询的列放在一张表中;      水平拆分表 水平拆分是指数据表行的拆分，把一张的表的数据拆成多张表来存放。\n 通常情况下，我们使用取模的方式来进行表的拆分，在insert时还需要一张临时表uid_temp来提供自增的ID,该表的唯一用处就是提供自增的ID;     client模式和proxy模式   client模式   proxy模式     无论是client模式，还是proxy模式，几个核心的步骤是一样的：SQL解析，重写，路由，执行，结果归并。 采用client模式，架构简单，性能损耗也比较小，运维成本低。如果在项目中引入mycat或者cobar，他们的单机模式无法保证可靠性，一旦宕机则服务就变得不可用，你又不得不引入其他中间件(HAProxy)来实现它的高可用集群部署方案\n存储升级 升级MySql  阿里云POLARDB 关系型分布式云原生数据库，100%兼容MySQL，存储容量最高可达 100T，性能最高提升至 MySQL 的 6 倍。 参考：https://yq.aliyun.com/articles/173291  参考  \u0026lt;高性能MySql\u0026gt; https://zhuanlan.zhihu.com/p/54594681 "
            }
    
        ,
            {
                "id": 48,
                "href": "https://chinalhr.github.io/post/kotlin-learn01/",
                "title": "Kotlin基础：函数、对象、Lamdba",
                "section": "post",
                "date" : "2019.01.13",
                "body": " Kotlin学习记录\n Kotlin的生态  Android开发 服务端开发 前端开发 原生环境开发  设计哲学   静态类型与类型推导 所有表达式的类型在编译期已经确定了，而编译器就能验证对象是否包含了你想访问的方法或者宇段。 与Java相比(JDK8↓)，拥有型推导，不需要你在源代码中显式地声明每个变量，变量类型可 以根据上下文来自动判断的类型。\n  null安全 Kotlin 的类型系统跟踪那些可以或不可以为 null 的 值，并且禁止那些运行时可能导致 NullPointerException 的操作。(把类型标记为可空的只要在类型尾部增加一个字符?)\n  互操作性 与Java，现成库拥有极高互操作性，Kotlin的类和方法可以像常规的Java 类和方法一样被调用。(Kotlin 没有自己的集合库，它完全依赖 Java 标准库中的类)\n  没有checked exception 要求checked exception可以提高开发人员的工作效率，又可以提高代码质量，但是大型软件项目的经验表明了不同的结果-checked exception降低了生产力，代码质量很少或根本没有增加。\n  参考：https://kotlinlang.org/docs/reference/exceptions.html\n 面向表达式编程 Kotlin中的流程控制不仅是普通语句，它们可以返回值，如if表达式、when表达式、try表达式等，而且还有范围表达式(1..100),中缀表达式(a to b)。一切皆表达式的设计让开发者在设计业务时，促进了避免创造副作用的逻辑设计，从而让程序变得更加安全;而且表达式通常也具有更好的表达能力，典型的一个例子就是表达式更容易进行组合。  Kotlin与Java  关键宇 fun 用来声明一个函数，参数的类型写在它 的名称后面 数组就是类,和 Java 不同， Kotlin 没有声明数组类型的特殊语法 在 Kotlin 中，除了循环（ for, do 和 do/while）以外大多数控制结构( if,When )都是表达式  表达式与语句:表达式有值，并且能作为另一个表达式的一部分使用；而语句总是包围着它的代码块中的顶层元素，并且没有自己的值。 fun max(a : Int , b: Int) = if (a \u0026gt; b) a else b  以关键字开始，然后是变量名称。(var age:Int = 20) val-不可变引用,var-可变引用 值对象简单声明：class Person(val name: String) 头等属性，省略getter/setter 智能转换：使用 is 检查来判断一个变量是否是某种类型,Kotlin编译器检查过一个变量是某种类型，后面就不再需要转换它，可以就把它当作你检查过的类型使用。(对比Java instanceOf 之后需要显示转换) when:when对比switch：switch只能使用枚举常量，字符串或者数字字面值;when 允许使用任何对象并且不需要在每个分支都写上break 语句。when是表达式且可以与if连用\u0026hellip;.  //when fun evalWhen(e: Expr): Int = when (e) { is Num -\u0026gt; e.value //智能转换  is Sum -\u0026gt; evalWhen(e.right) + evalWhen(e.left) else -\u0026gt; throw RuntimeException(\u0026#34;类型错误\u0026#34;) }   for:Kotlin没有Java的for-i概念，多出来区间与数列\n 区间  for (i in 1..100) { println(\u0026#34;current：$i\u0026#34;) }  downTo step until  //从100 到 1 打印偶数 步长2 向下到1 for (i in 100 downTo 1 step 2) { println(\u0026#34;current：$i\u0026#34;) }   异常\nKotlin 中 throw 结构是一个表达式，不必使用 new 关键字来创建异常实例。(try也是) Kotlin 并不区分受检异常和未受检异常。不用指定函数抛出的异常 ， 而且可以处理也可以不处理异常。\n  函数  命名参数 当调用一个 Kotlin定义的函数时，可以显式地标明一些参数的名称,避免混淆  joinToString(collectio, separator =\u0026#34;\u0026#34;, prefix =\u0026#34;\u0026#34;, postfix =\u0026#34;·\u0026#34;)  默认参数值 对比Java一些类的重载函数很多， Kotlin中可以在声明函数的时候，指定参数的默认值，这样就可以避免创建重载的函数。  fun \u0026lt;T\u0026gt; joinToString( collection: Collection\u0026lt;T\u0026gt;, separator: String =\u0026#34;,\u0026#34;,//默认参数值  prefix: String =\u0026#34;\u0026#34;, postfix: String = \u0026#34;\u0026#34; ) : String{ }  顶层函数|参数 在Java中，类处于顶层，类包含属性和方法，在Kotlin中，函数处于顶层，我们可以直接把函数放在代码文件的顶层，让它不从属于任何类，任何类都可以调用。 目的，消除了Java中常见的静态工具类，使我们的代码更加整洁。  @file:JvmName(\u0026#34;StrUtil\u0026#34;) package util fun joinToStr(collection: Collection\u0026lt;String\u0026gt;): String{ //....  return \u0026#34;\u0026#34;; } import util.joinToStr fun main(args: Array\u0026lt;String\u0026gt;){ joinToStr(collection = listOf(\u0026#34;123\u0026#34;, \u0026#34;456\u0026#34;)) } 因为在Java中，类还是必须要存在的，所以编译器将Str.kt文件里的代码放在了一个自动生成的类中，然后把我们定义的Kotlin的函数作为静态方法放在其中，在Java中是先通过import导入这个类，然后通过类名.方法名来调用。可以通过@file:JvmName注解来自定义类名。\n 扩展函数|参数  /** * 扩展函数 * * String-接收者类型 get(this.length -1) 对接收者对象(字符串)进行操作并返回 */ fun String.lastChar() : Char = get(this.length -1) fun main(args: Array\u0026lt;String\u0026gt;) { System.out.println(\u0026#34;Kotlin\u0026#34;.lastChar()) } 导入扩展函数(Kotiin 允许用和导入类一样的语法来导入)\nimport strings.lastChar  to 中缀调用 infix 修饰符标识可以使用中缀调用，to 函数会返回一个 Pair 类型的对象用来表示一对元素(解构声明)  fun \u0026lt;K, V\u0026gt; mapOf(vararg values: Pair\u0026lt;K, V\u0026gt;): Map\u0026lt;K , V\u0026gt; //使用 mapOf(1 to \u0026#34;One\u0026#34;,2 to \u0026#34;Two\u0026#34;) 类-对象 Kotlin 在类名后面使用冒号来代替了 Java 中的 extends 和 implements 关键字。\n Java的类和方法默认是open的，而Kotlin中默认都是final的。  目的：避免脆弱的基类问题(类被重写方法的风险)。Kotlin采用了open，final 和 abstract 修饰符\n编译期强制使用override修饰符\n   修饰符 相关成员 使用     final 不能被重写 类中成员默认使用   open 可以被重写 需要明确地表明   abstract 必须被重写 只能在抽象类中使用：抽象成员不能有实现   override 重写父类或接口中的成员 如果没有使用 final 表明，亟写的成员默认是开放的     可见性修饰符：默认为 public ,模块修饰符internal  internal：只在模块内部可见，一个模块就是一组一起编译的 Kotiin 文件(一个Maven或Gradle项目,IDEA模块\u0026hellip;)\n Kotlin编译器生成:数据类，委托类  //重写toString,equals,hashCode override fun toString() = \u0026#34;...\u0026#34; /** * 数据类，编译器重写toString 、equals 和 hashCode 。 * equals 用来比较实例 * hashCode 用来作为例如 HashMap 这种基于哈希容器的键 * toString 用来为类生成按声明顺序排列的所有字段的字符串表达形式 * copy 方法： copy对象的同时修改某些属性的值 */ data class Client(val name: String, val postalCode: Int) /** * 类委托：可以使用 by 关键字将接口的实现委托到另一个对象(避免装饰器模式带来的重复模板代码) * *类相当于innerList的包装类，拥有innerList的实现，并可以覆盖原有方法提供不同实现 */ class DelegatingCollection\u0026lt;T\u0026gt;(innerList: Collection\u0026lt;T\u0026gt; = ArrayList\u0026lt;T\u0026gt;()) : Collection\u0026lt;T\u0026gt; by innerList {}  object关键字与伴生对象  “伴生”是相较于一个类而言的，意为伴随某个类的对象，它属于这个类所有，因此伴生对象跟Java中static修饰效果性质一样，全局只有一个单例。它需要声明在类的内部，在类被装载时会被初始化。\n/** * object关键字 对象声明：创建单例类 */ object Payroll { ... } /** * 伴生对象companion：工厂方法 */ //构造方法私有化 class User private constructor(val nickname: String) { //声明伴生对象:工厂方法创建对象  companion object { fun newSubscribingUser(email: String) = User(email.substringBefore(\u0026#34;@\u0026#34;)) fun newFacebookUser(accountld: Int) = User(accountld.toString()) } } /** * 普通类的伴生对象：伴生对象是一个声明在类中的普通对象，有名字(默认Companion )，可以实现一个接口或者有扩展函数或属性 。 伴生对象（与包级别函数和属性一起）替代了 Java 静态方法和字段定义 */ class Prize(val name: String, val count: Int, val type: Int) { companion object { val TYPE_REDPACK = 0 val TYPE_COUPON = 1 fun isRedpack(prize: Prize): Boolean { return prize.type == TYPE_REDPACK } } } fun main(args: Array\u0026lt;String\u0026gt;) { val prize = Prize(\u0026#34;红包\u0026#34;, 10, Prize.TYPE_REDPACK) print(Prize.isRedpack(prize)) } /** * 对象表达式：(匿名内部类) */ val Listener = object : MouseAdapter() { override fun mouseClicked(e: MouseEvent) { .. . } override fun mouseEntered(e: MouseEvent) { ... } }  延迟初始化：by lazy与lateinit  /** * by lazy : val变量的初始化 * lazy的背后是接受一个lambda并返回一个Lazy \u0026lt;T\u0026gt;实例的函数，第一次访问该属性时，会执行lazy对应的Lambda表达式并记录* 结果。 * lazy属性默认LazyThreadSafetyMode.SYNCHRON IZED（同步锁） */ class Bird(val weight: Double, val age: Int, val color: String) { val sex: String by lazy { if (color == \u0026#34;yellow\u0026#34;) \u0026#34;male\u0026#34; else \u0026#34;female\u0026#34; } } /** * lateinit : var变量的初始化 */ lateinit var sex: String fun printSex() { this.sex = if (this.color == \u0026#34;yellow\u0026#34;) \u0026#34;male\u0026#34; else \u0026#34;female\u0026#34; } /** * Delegates.notNull\u0026lt;T\u0026gt; ：基本数据类型变量初始化 */ var test by Delegates.notNull\u0026lt;Int\u0026gt;() fun doSomething() { test = 1 } Lamdba  Kotlin与Java Lamdba区别 1.在Kotiin中Lamdba不会仅限于访问 final 变量，在 lambda 内部也可以修改这些变量(lambda捕捉)  部分操作符增强：[kotlin]count = [java]filter + count   惰性集合操作Sequence 例如 map 和 filter操作符，每一步的中间结果都被存储在一个临时列表，Sequence可以避免创建这些临时中间对象(序列中的元素求值是惰性的)。 注意： Sequence 只提供了 一个方法， iterator 关于惰性求值：1. 惰性求值是逐个处理元素 2. 延期操作，调用末端操作(count，sum\u0026hellip;)的时候才会求值  iterator 与 Sequence 区别:\n/** * 处理序列的中间操作函数是不进行任何计算的。相反，它们会返回上一个中间操作处理后产生的新序列。 * 所有这些一系列中间计算都将在终端操作执行中被确定，例如常见的终端操作toList或count.在另一方面，处理Iterable的每个中间操作函数都是会返回一个新的集合。 * */ interface Iterable\u0026lt;out T\u0026gt; { operator fun iterator(): Iterator\u0026lt;T\u0026gt; } interface Sequence\u0026lt;out T\u0026gt; { operator fun iterator(): Iterator\u0026lt;T\u0026gt; } //惰性求值，避免创建临时中间对象  val list = persons.asSequence() .map(Person::name) .filter { it.startsWith(\u0026#34;l\u0026#34;) } .toList() //惰性求值：给定序列中 的前一个元素，这个函数会计算出下一个元素。 val naturalNumbers = generateSequence(0) { it + 1 } println(\u0026#34;求100 以内的和：${naturalNumbers.takeWhile { it \u0026lt;= 100 }.sum()}\u0026#34;)  with 和 apply  /** * 使用给定的[receiver]作为接收器调用指定的函数[block]并返回其结果。 */ @kotlin.internal.InlineOnly public inline fun \u0026lt;T, R\u0026gt; with(receiver: T, block: T.() -\u0026gt; R): R { contract { callsInPlace(block, InvocationKind.EXACTLY_ONCE) } return receiver.block() } val returnAToZ = with(StringBuilder()) { for (letter in \u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;) { append(letter) } toString() } /** * 使用`this`值作为接收器调用指定的函数[block]并返回`this`值。 */ @kotlin.internal.InlineOnly public inline fun \u0026lt;T\u0026gt; T.apply(block: T.() -\u0026gt; Unit): T { contract { callsInPlace(block, InvocationKind.EXACTLY_ONCE) } block() return this } val returnAToZ = StringBuilder().apply { for (letter in \u0026#39;A\u0026#39;..\u0026#39;Z\u0026#39;) { append(letter) } } 高阶Lamdba  高阶函数  一个函数返回另一个函数作为结果\n fun foo(x: Int): (Int) -\u0026gt; Int { return { y: Int -\u0026gt; x + y } } 执行foo函数之后，会返回另一个类型为(Int)-\u0026gt; Int的函数\n 柯里化(Currying)  柯里化指的是把接收多个参数的函数变换成一系列仅接收单一参数函数的过程，在返回最终结果值之前，前面的函数依次接收单个参数，然后返回下一个新的函数;柯里化是为了简化Lambda演算理论中函数接收多参数而出现的，它简化了理论，将多元函数变成了一元;\n fun sum(x: Int) = { y: Int -\u0026gt; { z: Int -\u0026gt; x + y + z } } sum(1)(2)(3) 在kotlin中，如果参数不止一个，且最后一个参数为函数类型时，就可以采用类似柯里化风格的调用\n fun curryingLike(content: String, block: (String) -\u0026gt; Unit) { block(content) } curryingLike(\u0026quot;looks like currying style\u0026quot;) { content -\u0026gt; println(content) } // 运行结果 looks like currying style //也可以等值为 curryingLike(\u0026quot;looks like currying style\u0026quot;,{ content -\u0026gt; println(content) })  内联函数(消除lambda运行时开销)  开销：每调用 一次 lambda 表达式，一个额外的类就会被创建。并且如果 lambda 捕捉了某个变量，那么每次调用的时候都会创建一个新的对象 。 这会带来运行时的额外开销，导致使用 lambda 比使用 一个直接执行相同代码的函数效率更低 。\ninline\n使用 inline 修饰符标记一个函数，在函数被使用的时候编译器并不会生成函数调用的代码，而是使用函数实现的真实代码替换每一次的函数调用。\n@kotlin.internal.InlineOnly public inline fun \u0026lt;T\u0026gt; Lock.withLock(action: () -\u0026gt; T): T { lock() try { return action() } finally { unlock() } } 内联函数的限制：\n作为参数的 lambda表达式的函数体会被直接替换到最终生成的代码中。如果（lambda）参数在某个地方被保存起来，lambda 表达式的代码将不能被内联。如果（lambda）参数被调用，这 样的代码能被容易地内联。\nJVM优化 对于普通的函数调用，JVM己经提供了强大的内联支持。在将宇节码转换成机器代码时自动完成内联。使用inline 关键字只能提高带有 lambda 参数的函数的性能。将带有 lambda 参数的函数内联，节约了为 lambda 创建匿名类，以及创建 lambda 实例对象的开销。\n 函数中的控制流  lambda 中使用 return 关键字，是非局部返回(从调用 lambda 的函数中返回，并不只是从 lambda 中返回 )。只有在以 lambda 作为 参数的函数是内联函数的时候才能从更外层的函数返回。\n标签返回\n/** * tag@标签：return 会跳转到引用的标签lamdba 而不会跳出整个函数 */ fun lookForAlice(s:List\u0026lt;String\u0026gt;){ s.forEach tag@{ if (it == \u0026#34;Alice\u0026#34;) return@tag } println(\u0026#34;I find Alice\u0026#34;) } 匿名函数：默认使用局部返回\n/** * 使用匿名函数lambda */ fun lookForAlice2(s:List\u0026lt;String\u0026gt;){ s.forEach(fun (s){ //return指向最近的匿名函数  if (s == \u0026#34;Alice\u0026#34;) return }) println(\u0026#34;I find Alice\u0026#34;) } 字符串  原生字符串  Kotlin中支持原生字符串的语法，用这种3个引号定义的字符串，最终的打印格式与在代码中所呈现的格式一致，而不会解释转化转义字符\nval html = \u0026#34;\u0026#34;\u0026#34;\u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello World.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34;  字符串模板  fun message(name: String, lang: String) = \u0026#34;Hi ${name}, welcome to ${lang}! \u0026#34; \u0026gt;\u0026gt;\u0026gt; message(\u0026#34;Shaw\u0026#34;, \u0026#34;Kotlin\u0026#34;) Hi Shaw, welcome to Kotlin! "
            }
    
        ,
            {
                "id": 49,
                "href": "https://chinalhr.github.io/post/reactive-programming-reactor/",
                "title": "响应式编程库-Reactor",
                "section": "post",
                "date" : "2019.01.06",
                "body": " 响应式编程学习\n 响应式流规范 相关链接： https://github.com/reactive-streams/reactive-streams-jvm\nReactive Streams是JVM的面向流的库的标准和规范，目的是实现\n 具有处理无限数量的元素的能力； 按序处理； 异步地传递元素； 必须实现非阻塞的回压（backpressure）。  响应式流接口   Publisher(发布者,发出元素)\npublic interface Publisher\u0026lt;T\u0026gt; { public void subscribe(Subscriber\u0026lt;? super T\u0026gt; s); }   Subscriber(订阅者,接收元素并做出响应的)\npublic interface Subscriber\u0026lt;T\u0026gt; { public void onSubscribe(Subscription s); public void onNext(T t); public void onError(Throwable t); public void onComplete(); }   Subscription\npublic interface Subscription { public void request(long n); public void cancel(); }   Processor(集合Publisher和Subscriber)\npublic interface Processor\u0026lt;T, R\u0026gt; extends Subscriber\u0026lt;T\u0026gt;, Publisher\u0026lt;R\u0026gt; { }   当执行subscribe方法时，发布者会回调订阅者的onSubscribe方法，订阅者借助传入的Subscription的request方法向发布者请求n个数据。发布者通过不断调用订阅者的onNext方法向订阅者发出最多n个数据。如果数据全部发完，则会调用onComplete告知订阅者流已经发完；如果有错误发生，则通过onError发出错误数据，同样也会终止流。\n简单实现：https://github.com/ChinaLHR/reactor-stream\nReactor Reactor是完全基于Reactive Streams设计和实现的库，也是 Spring 5 中反应式编程的基础。\nFlux与Mono 发布者（Publisher）,一个Flux对象代表一个包含0..N个元素的响应式序列，而一个Mono对象代表一个包含零/一个（0..1）元素的结果。\n  创建数据流\nFlux.fromArray(array); Flux.fromIterable(list); Flux.fromStream(stream); Flux.just(...);   订阅\n// 订阅并触发数据流 subscribe(); // 订阅并指定对正常数据元素如何处理 subscribe(Consumer\u0026lt;? super T\u0026gt; consumer); // 订阅并定义对正常数据元素和错误信号的处理 subscribe(Consumer\u0026lt;? super T\u0026gt; consumer, Consumer\u0026lt;? super Throwable\u0026gt; errorConsumer); // 订阅并定义对正常数据元素、错误信号和完成信号的处理 subscribe(Consumer\u0026lt;? super T\u0026gt; consumer, Consumer\u0026lt;? super Throwable\u0026gt; errorConsumer, Runnable completeConsumer); // 订阅并定义对正常数据元素、错误信号和完成信号的处理，以及订阅发生时的处理逻辑 subscribe(Consumer\u0026lt;? super T\u0026gt; consumer, Consumer\u0026lt;? super Throwable\u0026gt; errorConsumer, Runnable completeConsumer, Consumer\u0026lt;? super Subscription\u0026gt; subscriptionConsumer);   自定义数据流-generate\n  generate是一种同步地，逐个地发出数据的方法。因为它提供的sink是一个SynchronousSink， 而且其next()方法在每次回调的时候最多只能被调用一次。\npublic static \u0026lt;T\u0026gt; Flux\u0026lt;T\u0026gt; generate(Consumer\u0026lt;SynchronousSink\u0026lt;T\u0026gt;\u0026gt; generator) public static \u0026lt;T, S\u0026gt; Flux\u0026lt;T\u0026gt; generate(Callable\u0026lt;S\u0026gt; stateSupplier, BiFunction\u0026lt;S, SynchronousSink\u0026lt;T\u0026gt;, S\u0026gt; generator) public static \u0026lt;T, S\u0026gt; Flux\u0026lt;T\u0026gt; generate(Callable\u0026lt;S\u0026gt; stateSupplier, BiFunction\u0026lt;S, SynchronousSink\u0026lt;T\u0026gt;, S\u0026gt; generator, Consumer\u0026lt;? super S\u0026gt; stateConsumer)  自定义数据流-create  高级的创建Flux的方法，其生成数据流的方式既可以是同步的，也可以是异步的，并且还可以每次发出多个元素。(create 常用的场景就是将现有的 API 转为响应式，比如监听器的异步方法)\nMyEventSource eventSource = new MyEventSource(); Flux.create(sink -\u0026gt; { //向事件源注册用匿名内部类创建的监听器  eventSource.register(new MyEventListener() { @Override public void onNewEvent(MyEventSource.MyEvent event) { //监听器在收到事件回调的时候通过sink将事件再发出  sink.next(event); } @Override public void onEventStopped() { //监听器在收到事件源停止的回调的时候通过sink发出完成信号  sink.complete(); } }); } ).subscribe(System.out::println); 调试  单元测试工具StepVerifier  expectNext用于测试下一个期望的数据元素，expectErrorMessage用于校验下一个元素是否为错误信号，expectComplete用于测试下一个元素是否为完成信号。\nStepVerifier.create(...) .expectNext(1, 2, 3, 4, 5, 6) .expectComplete() .verify(); ​\nOperator(操作符) 操作符选择器：https://htmlpreview.github.io/?https://github.com/get-set/reactor-core/blob/master-zh/src/docs/index.html#which.create\n  map - 元素映射为新元素\n  flatMap - 元素映射为流\n  filter - 过滤\n  zip - 一对一合并:对两个Flux/Mono流每次各取一个元素，合并为一个二元组（Tuple2）\npublic static \u0026lt;T1,T2\u0026gt; Flux\u0026lt;Tuple2\u0026lt;T1,T2\u0026gt;\u0026gt; zip(Publisher\u0026lt;? extends T1\u0026gt; source1,Publisher\u0026lt;? extends T2\u0026gt; source2) public static \u0026lt;T1, T2\u0026gt; Mono\u0026lt;Tuple2\u0026lt;T1, T2\u0026gt;\u0026gt; zip(Mono\u0026lt;? extends T1\u0026gt; p1, Mono\u0026lt;? extends T2\u0026gt; p2)   transform - 可以将一段操作链打包为一个函数式 //将filter和map操作符进行了打包 Function\u0026lt;Flux, Flux\u0026gt; filterAndMap = f -\u0026gt; f.filter(color -\u0026gt; !color.equals(\u0026ldquo;orange\u0026rdquo;)) .map(String::toUpperCase);\nFlux.fromIterable(Arrays.asList(\u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;purple\u0026#34;)) .doOnNext(System.out::println) //将包装的函数拼装到操作链  .transform(filterAndMap) .subscribe(d -\u0026gt; System.out.println(\u0026#34;Subscriber to Transformed MapAndFilter: \u0026#34;+d));   compose - 与 transform类似，将几个操作符封装到一个函数式中。区别是，这个函数式是针对每一个订阅者起作用的。对每一个subscription可以生成不同的操作链。\nFlux\u0026lt;String\u0026gt; composedFlux = Flux.fromIterable(Arrays.asList(\u0026#34;blue\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;purple\u0026#34;)) .doOnNext(System.out::println) .compose(filterAndMap);   调度器\u0026amp;线程模型   Schedulers工具类\n当前线程（Schedulers.immediate()）； 可重用的单线程（Schedulers.single()）。注意，这个方法对所有调用者都提供同一个线程来使用，直到该调度器被废弃。Schedulers.newSingle()使用独占的线程； 弹性线程池（Schedulers.elastic()）。它根据需要创建一个线程池，重用空闲线程。线程池如果空闲时间过长 （默认为 60s）就会被废弃。对于 I/O 阻塞的场景比较适用。Schedulers.elastic()能够方便地给一个阻塞 的任务分配它自己的线程，从而不会妨碍其他任务和资源； 固定大小线程池（Schedulers.parallel()），所创建线程池的大小与CPU个数等同； 自定义线程池（Schedulers.fromExecutorService(ExecutorService)）基于自定义的ExecutorService创建 Scheduler\n  示例：\n// 基于Callable的Mono Mono.fromCallable(() -\u0026gt; getStringSync()) // 使用subscribeOn将任务调度到Schedulers内置的弹性线程池执行  .subscribeOn(Schedulers.elastic()) .subscribe(System.out::println);   publishOn与subscribeOn：\npublishOn会影响链中其后的操作符 subscribeOn无论出现在什么位置，都只影响源头的执行环境\n  数据流配置Context\n  类似于 ThreadLocal ,区别是ontext作用于一个 Flux 或一个 Mono 上，而不是应用于一个线程（Thread）,其生命周期伴随整个数据流。\n错误处理\u0026amp;重试   错误处理\n 捕获并返回一个静态的缺省值： onErrorReturn方法能够在收到错误信号的时候提供一个缺省值 捕获并执行一个异常处理方法或计算一个候补值来顶替: onErrorResume方法能够在收到错误信号的时候提供一个新的数据流 捕获，并再包装为某一个业务相关的异常，然后再抛出业务异常: onErrorMap 捕获，记录错误日志...，然后继续抛出 doOnError(只读，不会对数据流造成影响) 使用 finally 来清理资源 doFinally    重试\nretry：retry对于上游Flux是采取的重订阅（re-subscribing）的方式，出错了retry会从新订阅了原始的数据流执行x次。   Backpressure背压(回压)   背压的前提\n 发布者与订阅者不在同一个线程中，因为在同一个线程中的话，通常使用传统的线性逻辑，不需要进行回压处理； 发布者发出数据的速度高于订阅者处理数据的速度，也就是处于“PUSH”状态下。    背压策略(OverflowStrategy )\n IGNORE: 完全忽略下游背压请求 ERROR： 当下游跟不上节奏的时候发出一个错误信号。 DROP：当下游没有准备好接收新的元素的时候抛弃这个元素。 LATEST：让下游只得到上游最新的元素。 BUFFER：缓存下游没有来得及处理的元素（如果缓存不限大小的可能导致OutOfMemoryError）。    create声明回压策略\npublic static \u0026lt;T\u0026gt; Flux\u0026lt;T\u0026gt; create(Consumer\u0026lt;? super FluxSink\u0026lt;T\u0026gt;\u0026gt; emitter, OverflowStrategy backpressure)   操作符调整背压策略\n  onBackpressureBuffer(对于来自其下游的request采取“缓存”策略)   onBackpressureDrop(元素就绪时，根据下游是否有未满足的request来判断是否发出当前元素)   onBackpressureLatest(当有新的request到来的时候，将最新的元素发出)   onBackpressureError(当有多余元素就绪时，发出错误信号)     通过BaseSubscriber定义自己的Subscriber进行上游流量控制\nsubscribe(new BaseSubscriber\u0026lt;Integer\u0026gt;() { @Override protected void hookOnSubscribe(Subscription subscription) { //hookOnSubscribe定义在订阅的时候执行的操作，订阅时首先向上游请求1个元素  request(1); } @Override protected void hookOnNext(Integer value) { //hookOnNext定义每次在收到一个元素的时候的操作，每次处理完1个元素后再请求1个。  request(1); }  "
            }
    
        ,
            {
                "id": 50,
                "href": "https://chinalhr.github.io/post/reactive-programming/",
                "title": "响应式编程",
                "section": "post",
                "date" : "2018.12.28",
                "body": " 响应式编程学习\n 关于响应式编程 响应式编程是一种关注于数据流（datastreams）和变化传递（propagation of change）的异步编程方式。可以用既有的编程语言表达静态（如数组）或动态（如事件源）的数据流。\n响应式编程通常作为面向对象编程中的“观察者模式”（Observer design pattern）的一种扩展。\nreactive streams 与 Iterator design pattern 关系   Iterator:\n 基于Pull模式(拉取)，Iterable角色 提供访问元素的方法，但是什么时候执行next()获取元素取决于开发者。 命令式编程，详细的命令机器怎么（How）去处理数据以达到你想要的结果（What）。    Reactive:\n 基于Push模式(推送)，Publisher(发布者)-Subscriber(订阅者)角色提供获取元素的方法。当有新的值到来的时候，Publisher通知Subscriber。 声明式编程，开发者通过描述“控制流程”来定义对数据流的处理逻辑。    特性  变化传递（propagation of change） 类似于Excel的公式计算，一个单元格变化之后，导致直接和间接引用它的其他单元格均发生相应变化。 数据流（data stream） 数据/事件在响应式编程里会以数据流的形式发出。 声明式（declarative） 声明好了对于数据流“将会”进行什么样的处理，当有数据流过来时，就会按照声明好的处理流程逐个进行处理。(形成了一种计算逻辑对数据的“绑定”。) 响应式流（Reactive Stream） 异步非阻塞(Java Stream是同步的),可以通过背压进行流量控制。  阻塞与异步   阻塞式（blocking）编码造成浪费资源，当一个程序面临延迟（通常是I/O方面， 比如数据库读写请求或网络调用），所在线程需要进入 idle 状态等待数据，从而浪费资源。\n  异步非阻塞编码， （任务发起异步调用后）执行过程会切换到另一个 使用同样底层资源 的活跃任务，然后等异步调用返回结果再去处理，避免资源浪费。\n  Java的异步\n  Callback：类似于观察者模式，异步方法没有返回值，而是采用一个 callback 作为参数，当结果出来后回调这个 callback。\n  Futures:异步方法 立即 返回一个Future，该异步方法要返回结果的是 T 类型,线程实际处理结束时可以拿到结果。\n  缺陷：容易写出回调地狱（Callback Hell）\n    响应式库目标  可编排性（Composability）以及可读性（Readability）使用丰富的操作符 来处理形如流的数据 在订阅（subscribe）之前什么都不会发生 背压（backpressure）具体来说即消费者能够反向告知生产者生产内容的速度的能力 高层次 （同时也是有高价值的）的抽象，从而达到 并发无关 的效果 JVM 异步方式缺陷  关于响应式宣言 响应式宣言是一组架构与设计原则，即时响应性（Responsive）、回弹性（Resilient）、弹性（Elastic）以及消息驱动（Message Driven）。 对于这样的系统，我们称之为响应式系统（Reactive System）。\n 回弹性  系统在出现失败时依然保持即时响应性。 失败的扩散被遏制在了每个组件内部，与其他组件相互隔离， 从而确保系统某部分的失败不会危及整个系统，并能独立恢复。每个组件的恢复都被委托给了另一个（外部的）组件。[如熔断器组件]   弹性  系统在不断变化的工作负载之下依然保持即时响应性。响应式系统可以对输入（负载）的速率变化做出反应，比如通过增加或者减少被分配用于服务这些输入（负载）的资源。[如DevOps,云原生\u0026hellip;]   消息驱动  响应式系统依赖异步的消息传递，从而确保了松耦合、隔离、位置透明的组件之间有着明确边界。    响应式编程与响应式系统 响应式宣言是一组设计原则，一种关于分布式环境下系统架构与设计的思考方式，关注于通过分布式系统的通信和协作所得到的弹性和可靠性(消息驱动)，响应式系统是符合这一架构风格的系统。\n响应式编程是异步编程下的一个子集，是一种范式，有具体的开发库，侧重于由信息/数据流而不是命令式的控制流来推动逻辑的前进，专注于短时间的数据流链条上的计算(事件驱动)。\n一个拥有长期存活的可寻址long-lived addressable组件的消息驱动系统跟一个事件驱动的数据流驱动模型的不同在于，消息具有固定的导向，而事件则没有。消息会有明确的（一个）去向，而事件则只是一段等着被观察observe的信息。\n异步且非阻塞执行下的数据流管理——通常只在单个结点或服务中。当有多个结点时，就需要开始认真地考虑像数据一致性data consistency、跨结点沟通cross-node communication、协调coordination、版本控制versioning、编制orchestration、错误管理failure management、关注与责任concerns and responsibilities分离等等的东西——也即是：系统架构。\n参考 Reactor中文文档 响应式宣言 响应式编程与响应式系统\n"
            }
    
        ,
            {
                "id": 51,
                "href": "https://chinalhr.github.io/post/os-heart-multicpu/",
                "title": "计算机的心智-多核心",
                "section": "post",
                "date" : "2018.12.20",
                "body": " 读《计算机的心智》,另一个角度看操作系统\n 多核演变  多处理结构(SMP Architecture)  一条总线上挂载多个CPU。CPU角色功能平等，没有主从，称为对称结构。\n 超线程结构(HyperThreading)  英特尔提出的技术，让一个CPU同时执行多重线程，从而提高CPU效率。目的是让一个CPU上同时执行多个程序而共享这个CPU内的资源。(让同一时间不同应用程序使用CPU的不同资源,由CPU上的逻辑处理单元指针控制)\n如下，4 个物理 CPU ，而每个CPU又因超线程技术分解为两个逻辑 CPU 。每个逻辑 CPU 可以执行一个线程序列。这样一个物理 CPU 可以同时执行两个线程。\n 多核结构(Multi-core Architecture)  多核结构就是在一个CPU里面布两个执行核，即两套执行单元，如 ALU,FPU和L2缓存等。而其他部分则两个核共享。这样，由于使用的是一个 CPU ，其功耗和单CPU一样。由于布置了多个核，其指令级并行将是真正的并行，而不是超线程结构的半并行。\n 多核超线程结构(Multi-core HyperThreading Architecture)  在多核结构下使用超线程技术。\n 区别 主要体现在同时执行的两个线程之 间共享物理资源的多少。多处理器的共享物理资源最少,每个线程有自己单独的处理器;超线程共享最多,ALU、FPU、MSR、缓存等均为共享物理资源;而多核则介于两者之间,共享处理器,但不共享ALU、FPU等。   多核原理 多核内存结构(协调多核心对内存的访问)  UMA  将内存作为与执行核独立的单元构建在核之外，所有的核通过同一总线对内存进行访问。由于每个核使用相同的方式访问内存，其到内存的延迟也相同，这种访问模式称为均匀内存访问(Uniform Memory Access , UMA)。\n优点是设计简单，实现容易。缺点是难以针对个体的程序进行访问优化，以及扩展困难。随着执行核数最的增加会加剧对共享内存的竞争，造成系统效率急剧下降。\n NUMA  使用多个分开的独立共享内存。每个执行核或 CPU 到达不同共享内存的距离不同，访间延迟也不一样。这种访问延迟不一致的内存共享模式称为非均匀内存访问（ Non-Uniform Memo y Access , NUMA）。在这种模式下，最重要的特点是执行核在不同的内存单元面前地位并不平等：到近的内存具有优势地位，到远的内存则处于劣势。\n优点是易扩展，但对调度的要求很高(调度时将程序就近执行)。\n COMA  在每个执行核里面配置缓存,其执行需要的数据均缓存在该缓存里面。所有访问由缓存得到满足。避免数据存储的内存单元(远/近),其对效率的影响。这种完全由缓存满足数据访问的模式称为全缓存内存访问( Cache Only Memor Access,COMA)。在这种模式下,每个执行核配备的缓存共同组成全局地址空间。\n多处理器之间通信 使用CPU之间中断的机制就是高级可编程中断控制器(APIC)。在此种规范下,每个CPU内部必须内置APIC单元(成为那个CPU的本地APIC)。CPU通过彼此发送中断(IPI,即处理器间中断)来完成它们之间的通信。通过给中断附加动作,不同的CPU可以在某种程度上彼此进行控制。除了每个CPU自己本地的APC外,所有CPU通常还共享一个IO APIC来处理由LO设备引起的中断,这个IO APIC是安装在主板上的。\n对称多处理器(SMP)的缓存一致性 在对称多处理器结构下,每个处理器都有自己的缓存,因此在一个系统里面存在多个缓存的情况下就有可能出现两个缓存的数据不一致的情况。即两个CPU缓存同样的数据,其中一个或两个CPU对数据进行了修改从而造成两个CPU缓存数据的不同。\nMESI模型：CPU中每个缓存行（cacehline)使用4种状态进行标记（使用额外的两位(bit)表示)\n M: 被修改（Modified)  该缓存行只被缓存在该CPU的缓存中，并且是被修改过的（dirty),即与主存中的数据不一致，该缓存行中的内存需要在未来的某个时间点（允许其它CPU读取请主存中相应内存之前）写回（write back）主存。当被写回主存之后，该缓存行的状态会变成独享（exclusive)状态。\n E: 独享的（Exclusive)  该缓存行只被缓存在该CPU的缓存中，它是未被修改过的（clean)，与主存中数据一致。该状态可以在任何时刻当有其它CPU读取该内存时变成共享状态（shared)。同样地，当CPU修改该缓存行中内容时，该状态可以变成Modified状态。\n S: 共享的（Shared)  该状态意味着该缓存行可能被多个CPU缓存，并且各个缓存中的数据与主存数据一致（clean)，当有一个CPU修改该缓存行中，其它CPU中该缓存行可以被作废（变成无效状态（Invalid））。\n I: 无效的（Invalid）  该缓存是无效的（可能有其它CPU修改了该缓存行）。\n 状态迁移  触发事件：\n本地读取（Local read）\t本地cache读取本地cache数据 本地写入（Local write）\t本地cache写入本地cache数据 远端读取（Remote read）\t其他cache读取本地cache数据 远端写入（Remote write）其他cache写入本地cache数据   多核环境下的进程同步与调度 同步  加载与存入，测试与设置(硬件同步原语)  \u0026hellip;\n 总线锁(硬件同步原语)  总线锁就是将总线锁住,只有持有该锁的CPU才能使用总线。这样,由于所有CPU均需要使用共享总线来访问共享内存,而总线的锁住将使得其他CPU没有办法执行任何与共享内存有关的指令,从而使得数据的访问是排他的。\n 软件同步原语  Linux内核提供的原子操作:\n 总线锁:置换、比较与置换、原子递增操作。 原子算术操作:原子读、设置、加、减、递增、递减、递减与测试。 原子位操作:位设置、位清除;位测试与设置、位测试与清除、位测试与改变。  Windows内核提供的原子:\n 互锁操作( interlocked operation) 执行体互锁操作( executive interlocked operations   旋锁  多核操作系统提供的CPU互斥机制。旋锁通常用于保护某个全局的数据结构,如 Windows里面的DPC(延迟过程调用)队列。这里的互斥指的是多个处理器或执行核之间的互斥,即两个处理器或核不能(物理上)同时访问同一个数据结构。\n旋锁通过获取和释放两个操作来保证任何时候只有一个拥有者。旋锁的状态有两种:要么是闲置的,要么被某个CPU所拥有。如果一个CPU获得一个旋锁,那么运行在该CPU上的所有的线程都可以访问该旋锁所保护的寄存器和数据结构。\n使用测试与设置实现\n旋锁是一个特定的内存单元，位于整个系统的共享内存里面。如果一个处理器要使用旋锁,就必须检査这个特定内存单元的值。如果为0,则将其设置为1,表示获得该旋锁。如果为1,则表示该旋锁被其他处理器所占有,则在该旋锁上进行繁忙等待,即不停地循环。\n避免内存总线竞争\n每个CPU在检查旋锁的状态时均需要使用系统总线来访间旋锁所在的共享全局内存单元。测试并设置这个全局内存单元,就需要不停地发信号到总线上,造成对内存总线的竞争。\n队列旋锁优化：需要旋锁的CPU不要到全局内存去SPIN,而是到自己的局部内存去SPN。这样就可以排除对总线的竞争。需要旋锁的CPU使用队列数据结构(全局内存)，旋锁释放的时候就去检查这个队列进行操作。\n调度  调度策略  每个CPU有着自己的就绪队列( runqueue)。该队列里面又 可以按照不同的优先级分解为多个子队列,就像单核环境下的情况一样。一个进程只可以排在任何一个CPU的队列。\nLinux就绪队列：\n 调度域(多个CPU负载平衡) 负载平衡的目标是将进程均匀分配到每个CPU的就绪队列里面。Linux CPU负载均衡策略基于多级调度域。  主动负载平衡是队列里面进程数多的CPU将某些进程推出去(push)。被动负载平衡则是队列为空的CPU从别的CPU队列里面将进程拉出来(pu)。\n"
            }
    
        ,
            {
                "id": 52,
                "href": "https://chinalhr.github.io/post/os-heart-file/",
                "title": "计算机的心智-文件",
                "section": "post",
                "date" : "2018.12.16",
                "body": " 读《计算机的心智》,另一个角度看操作系统\n 文件系统 关于文件系统\n文件系统时操作系统提供的抽象，将用户从数据存放的细节中解放出来。\n目标\n文件系统目的是达到地址独立(目录)与地址保护(权限)。\n文件的基本信息\n文件命名：操作系统负责根据文件名找到文件存放磁盘的数据块。（磁盘扇面）\n扩展名：文件类型\n内容寻址：通过内容寻址存储系统查找\n文件内容组织：关系导向型组织与非关系导向型组织\n文件类型：目录|一般文件|块文件(关于输入输出的文件)\n文件格式：例如Unix下的可执行文件，归档文件 文件访问：随机访问(任意顺序读取数据记录)\n文件属性：创建者，拥有者，只读标志，隐藏标志，文件大小尺寸,修改时间\u0026hellip;\n文件操作：创建，删除，打开，关闭，重命名，读写\u0026hellip;\n文件夹(目录)Folder(地址独立实现机制) 文件夹用来保存的是文件及文件系统的信息，文件夹本身也是文件，存放文件到文件在磁盘的地址映射(文件名——\u0026gt;文件在磁盘的地址)。\n 文件夹结构  文件夹是层级结构，顶端是根文件夹，称为根目录。根目录是文件系统的起点，操作系统启动的时候就加载到内存。 Shell的ls与Window的dir：读取文件夹的内容(数组)展示。\n 根据文件名找到所在磁盘地址  /book/dart/file.pdf /代表根目录(根目录存放book文件夹的磁盘地址)——\u0026gt;book文件夹存放dart文件夹的磁盘地址—\u0026gt;\u0026hellip;\n 共享与链接  链接：操作系统的Link调用(为访问不同目录的文件建立链接，直接访问不同目录下的文件)\n内存映射的文件访问 内存映射：解决系统调用读取文件效率(每次需要陷如内核)低下核心思想是把磁盘访问变成内存访问。\n原理：把需要访问的文件映射到一个进程的虚拟地址，访问虚拟地址就相当于访问文件。把磁盘访问变成内存访问，不需要系统调用来访问。映射到多个进程上可以实现文件共享。\n 文件的物理结构 顺序文件 文件中的物理记录按其在文件中的逻辑记录顺序依次存入存储介质而建立的。顺序文件中物理记录的顺序和逻辑记录的顺序是一致的。\n 连续结构文件 把逻辑上连续的文件信息依次存放在连续编号的物理块中。即次序相继的两个物理记录在存储介质上的位置是相邻的。  结构简单，顺序访问速度快，但不能动态增长，外存容易存在碎片。\n 链式结构文件  链结构将逻辑上连续的文件信息分散存放在若干不连续的物理块中，其中每个物理块设有一个指针，指向其后续连接的另一个物理块。\n提高了磁盘空间利用率，解决了磁盘碎片问题，便于文件的插入和删除操作与动态增长,但不适宜随机存取访问\n  索引文件\n 索引文件 建立一张逻辑记录和物理记录之间对应关系的索引表。 索引顺序文件和索引非顺序文件  索引顺序文件(Indexed Sequential File)：主文件按主关键字有序的文件称索引顺序文件。 在索引顺序文件中，可对一组记录建立一个索引项。这种索引表称为稀疏索引。 索引非顺序文件(Indexed NonSequentail File)：主文件按主关键字无序得文件称索引非顺序文件。 在索引非顺序文件中，必须为每个记录建立一个索引项，这样建立的索引表称为稠密索引。      多级索引\n  对索引表建立的索引，称为查找表。查找表的建立可以为占据多个页块的索引表的查阅减少外存访问次数。当查找表中项目仍很多，可建立更高一级的索引。通常最高可达四级索引。 多级索引是一种静态索引，各级索引均为顺序表，结构简单，修改很不方便(需要重组索引)\n  动态索引 当数据文件在使用过程中记录变动较多时，利用二叉排序树(或AVL树)、B-树(或其变型)等树表结构建立的索引，为动态索引。特点是插入、删除方便，无需多级索引，建立索引=树排序。\n  ISAM文件(索引顺序存取方法)\n  是一种专为磁盘存取设计的文件组织方法，采用静态索引结构。 由于磁盘是以盘组、柱面和磁道三级地址存取的设备，ISAM对磁盘上的数据文件建立盘组、柱面和磁道三级索引。\n  检索过程\n1.从主索引出发，找到相应的柱面索引； 2.从柱面索引找到记录所在柱面的磁道索引； 3.从磁道索引找到记录所在磁道的起始地址，由此出发在该磁道上进行顺序查找，直到找到为止。    VSAM文件 基于B+树,文件是利用操作系统中提供的虚拟存储器的功能组织的文件，免除了用户为读/写记录时直接对外存进行的操作，对用户而言，文件只有控制区间和控制区域等逻辑存储单位。\n  VSAM文件的结构由三部分组成:索引集，顺序集，数据集\nVSAM的优势是能保持较高的查找效率，查找一个后插入记录和查找一个原有记录具有相同的速度；动态地分配和释放存储空间，可以保持平均75％的存储利用率；而且永远不必对文件进行再组织。\n 文件系统实现 文件系统  操作系统的启动  磁盘包括一个个扇面，第0个扇面存放MBR(Master Boot Record)主引导记录，用于启动计算机与记录磁盘分区表(给出磁盘所有分区的开启终结地址)。计算机启动时，主板ROM里面的BIOS程序首先运行，随后将磁盘0扇面进行读操作，将MBR程序加载到内存运行，MBR程序根据磁盘分区表找到主分区，将主分区的Boot Record程序加载，Boot Record负责找到操作系统映像加载到内存并启动。\n文件实现   数据在磁盘的存放方式\n  连续空间存放 例如数据库\n  非连续空间存放 链表存放实现(每个数据块留出一个指针空间存放下一个数据块地址，使用文件分配表FAT存储数据库指针)\n    FAT文件系统\n  FAT12,FAT16,FAT32等(表示用12,16,28位表示磁盘地址)。 索引数据块(I-NODE):将每个文件的所有数据块的磁盘地址收集起来，每次打开文件就把目标文件的数据块磁盘地址从I-NODE加载到内存，依据索引获取物理磁盘地址。I-NODE还存有文件创建时间，修改时间，类型等信息。\nI-NODE是非连续组织方式，基于链表增长灵活\n非对称多级索引组织(解决I-NODE范围增长)\n多级I-NODE，有顶级次级I-NODE用于索引大文件 单级I-NODE用于索引小文件(因为小文件使用的数据块不会超过I-NODE指针数，避免浪费I-NODE空间，降低访问磁盘次数)\n文件夹(目录)实现 如果使用的是 I-NODE 组织形式，我们只需要知道文件对应的顶级 I-NODE 地址即可。文件的数据块地址可以从I-NODE 里面获得。这种情况下文件夹里面存放的映射是到I-NODE 编号。\n 文件共享 操作系统的Link调用(为访问不同目录的文件建立链接，直接访问不同目录下的文件)  硬链接：在某个文件夹Link调用创建目标文件链接时，该文件夹会增加目标文件到文件地址的映射。删除的时候(逻辑或者物理删除)，如果链接数不为0只是将文件对应I-NODE里面的链接计数减一，如果链接数为0则删除。会存在文件创建者删除文件，但因为被链接而没有被物理删除的问题。\n软链接：在某个文件夹Link调用创建目标文件链接时，该文件夹会增加目标文件的路径名。从链接访问文件需要从原始路径访问。原始文件删除后链接断开无法访问文件。(WINDOW快捷方式)\n 文件系统挂载 挂载是将一个文件系统并入另一个文件系统(修改被挂载文件系统的根目录和挂载点 U盘，光盘) "
            }
    
        ,
            {
                "id": 53,
                "href": "https://chinalhr.github.io/post/os-heart-memory/",
                "title": "计算机的心智-内存",
                "section": "post",
                "date" : "2018.12.11",
                "body": " 读《计算机的心智》,另一个角度看操作系统\n 基本内存管理 内存架构：缓存——主存——磁盘\n内存管理机制负责对内存架构进行管理抽象，使程序在内存架构的任何一个层次上存放对于用户都是无感知的。\n内存管理目标\n地址保护：一个程序不能访问另一个程序的地址空间 地址独立：程序发出的地址与物理主存地址无关\n虚拟内存\n将物理主存扩大到大容量的磁盘上(将磁盘空间看作主存空间的一部分)\n多道编程的内存管理 固定分区策略\n思想：将内存固定分为几个区域，每个区域大小固定。每个分区对应一个队列，程序按照大小排在相应的队列里进行加载。\n非固定分区策略\n思想：除了划分给操作系统的空间外，其余的空间作为一个整体存在，一个程序加载划分出一块内存。操作系统使用基址和极限管理。\n交换(swap)[解决程序加载到内存后继续扩大，超过预留空间]：\n交换就是将一个进程从内存倒到磁盘，等待再将其从磁盘加载到内存的过程。目的是为程序找到一片更大的空间，防止程序因为空间不够而崩溃。\n重叠(overlay)[解决程序占用空间超过物理内存]:\n重叠就是将程序按照功能分成一段一段完整的单元，一个单元执行完再执行下一个单元(后续单元可以覆盖之前的单元)\n闲置空间管理\n 给每个分配的单元赋予一个字位用于记录是否空闲 链表表示法   离散分配内存管理 基本内存管理为进程分配的空间是连续的，使用的地址都是物理地址。如果允许将一个进程分散到许多不连续的空间，就可以避免内存紧缩，减少碎片。基于这一思想，通过引入进程的逻辑地址，把进程地址空间与实际存储空间分离，增加存储管理的灵活性。\n地址空间：将源程序经过编译后得到的目标程序，存在于它所限定的地址范围内，这个范围称为地址空间。地址空间是逻辑地址的集合。\n存储空间：指主存中一系列存储信息的物理单元的集合，这些单元的编号称为物理地址存储空间是物理地址的集合。\n根据分配时所采用的基本单位不同，可将离散分配的管理方式分为三种： 页式存储管理、段式存储管理和段页式存储管理。其中段页式存储管理是前两种结合的产物。\n 页式内存管理 核心 页式内存管理核心：将虚拟内存空间和物理内存空间皆划分为大小相同的页面(4k,8k\u0026hellip;),以页面作为内存空间最小分配单位。\n分页系统下，程序发出的虚拟内存由页面号和页面偏移值组成(32位寻址系统，页面大小4k，页面号20位，偏移值12位)\n页表\n在页式系统中进程建立时，操作系统为进程中所有的页分配页框。当进程撤销时收回所有分配给它的页框。在程序的运行期间，如果允许进程动态地申请空间，操作系统还要为进程申请的空间分配物理页框。操作系统为了完成这些功能，必须记录系统内存中实际的页框使用情况。操作系统还要在进程切换时，正确地切换两个不同的进程地址空间到物理内存空间的映射。这就要求操作系统要记录每个进程页表的相关信息。\n物理页面表：整个系统有一个物理页面表，描述物理内存空间的分配使用状况(通过比特位01表示是否空闲)\n请求表：整个系统有一个请求表，描述系统内各个进程页表的位置和大小，用于地址转换也可以结合到各进程的PCB(进程控制块)里。\n页表记录内容：\n地址翻译(虚拟地址转换为物理地址)\n通过产生系统中断(缺页中断)，将虚页从磁盘上转移到内存，然后将分配给他的物理页面号返回。通过内存管理单元MMU完成(MMU接受CPU发出的虚拟地址，翻译为物理地址发送给内存，内存按照物理地址进行数据的读写)。\nMMU翻译方式： 每个程序，内存管理单元都为其保存一个页表，通过查询进程表获得页表页表存放虚拟页面到物理页面的映射。通过虚拟地址的页号获取到页表的页号，页号映射的物理地址+页内偏移地址=物理地址\n多级页表 顶级页表存放一级页表信息，一级页表存放二级\u0026hellip;(顶级页表常驻内存，次级页表存放物理内存)Linux采用三级页表。\n目的减少空间占用,大部分次级页表存放到磁盘。缺点是增加内存访问，磁盘访问。(缓存优化提高翻译速度)\n页面更换算法 关于页面更换：如果发生缺页中断，就需要从磁盘上将需要的页面调入内存。如果内存没有多余的空间，就需要从页面中选择一个页面进行替换。\n页面更换算法目标：如果更换的页面很快又被访问，系统会很快再次产生缺页中断(缺页中断代价很大)。页面更换算法的目标是降低随后发生缺页中断的次数。\n公平算法👇 随机更换算法\n略\u0026hellip;\nFIFO算法 更换最早进入内存的页面。使用链表将所在内存的页面按照进入时间的早晚链接起来，每次置换链表头的页面即可。\n缺陷：如果最先加载进来的页面经常被访问，会降低效率。\n第二次机会算法 改进FIFO算法，在更换链表头的页面时，查看是否在最近被访问过，没访问过则替换，访问过则挂载到链表尾。\n缺陷：移至链表尾部消耗时间,时间分辨粒度低，影响页面替换效果。\n时钟算法(第二次机会算法改进) 把页面排成一个时钟的形状。该时钟有一个针臂。每次需要更换页面时，我们从针臂所指的页面开始检查。如果当前页面的访问位为 0 , 即从上次检查到这次，该页面没有被访问过，将该页面替换。如果当前页面被访问过，那就将其访问位清零，并顺时针移动指针到下一个页面。我们重复这些步辍直到找到一个访问位为 0 的页面。\n缺陷：时间分辨粒度低，影响页面替换效果，可能无限循环。\n非公平算法👇 NRU算法(最近未使用算法) 选择最近一段时间没被访问过的页面进行替换，利用页面的访问和修改位。 当对页面进行读写操作时，访问位设置为1,进程对页面进行写操作时，修改位设置为1。\nLRU算法(最近最少使用算法) 对NRU进行改进，不仅考虑是否使用过，还需要考虑频率。\n使用位移寄存器实现：\u0026hellip;\n缺陷：保留了每个页面每次的访问记录，导致空间成本高。\n工作集算法 维持少量信息选出最近最少使用。为页表的每个记录增加一项信息用来记录该页面最后一次被访问的时间。\n 如果一个页面的访问位是1，则将该页面的最后一次访问时间设为当前时间，并将访问位清零。 如果页面的访问位为0，则查看其访问时间是否在当前时间减去 T 之前。是则替换  段式内存管理 页式内存管理的缺陷是共享困难，一个进程只能占有一个虚拟地址空间。(一个程序的大小至多只能和虚拟空间一样大)，段式内存管理就是为了解决这些问题。\n关于分段管理 在段式存储管理中，将程序的地址空间划分为若干个段(segment)，这样每个进程有一个二维的地址空间。在前面所介绍的动态分区分配方式中，系统为整个进程分配一个连续的内存空间。而在段式存储管理系统中，则为每个段分配一个连续的分区，而进程中的各个段可以不连续地存放在内存的不同分区中。程序加载时，操作系统为所有段分配其所需内存，这些段不必连续，物理内存的管理采用动态分区的管理方法。(相比基本内存管理，基本内存管理一个程序只有一个段，分段管理有多个段)\n程序通过分段划分为多个模块，如代码段、数据段、共享段：\n–可以分别编写和编译 –可以针对不同类型的段采取不同的保护 –可以按段为单位来进行共享，包括通过动态链接进行代码共享\n这样做的优点是：可以分别编写和编译源程序的一个文件，并且可以针对不同类型的段采取不同的保护，也可以按段为单位来进行共享。\n段式内存管理下，程序发出的虚拟内存由段号和段内偏差组成\n分段实现方式 使用一组基址与极限对，每个基址与极限用于其中一段的管理。逻辑分段，每个虚拟地址只需要对其对应的基址寄存器与极限寄存器值进行调整，就可以加载到物理内存的任何空间。\n地址翻译(虚拟地址转换为物理地址) 段表存放虚拟段号到该段的所在的内存基址的映射。物理地址=内存基址+段内偏差\n缺陷 和基础内存管理一样，存在外部碎片和一个段必须全部加载到内存中，所以就有了段页式内存管理。\n段页式内存管理 段页式管理就是将程序分为多个逻辑段，在每个段里面又进行分页，即将分段和分页组合起来使用。这样做的目的就是想同时获得分段和分页的好处，但又避免了单独分段或单独分页的缺陷。如果我们将每个段看做一个单独的程序，则逻辑分段就相当于同时加载多个程序。\n实现\n在段里面分页（次级页表），一个程序对应多个次级页表，在多个次级页表上增加一层段表（顶级页表）。由段表获取所应该使用的页表，通过页表查找物理地址。\n成长过程 基本内存管理 缺陷：内存空间增长困难，外部碎片，程序不能超过物理内存容量，一个程序必须加载到内存才可以执行。\n👇\n页式内存管理 缺陷：一个程序只能在一个虚拟地址空间增长。\n👇\n段式内存管理 缺陷：解决了：一个程序只能在一个虚拟地址空间增长，但存在外部碎片和一个段必须全部加载到内存中(因为是基本内存管理的升级)\n👇\n段页式内存管理\n"
            }
    
        ,
            {
                "id": 54,
                "href": "https://chinalhr.github.io/post/os-heart-thread/",
                "title": "计算机的心智-线程",
                "section": "post",
                "date" : "2018.11.29",
                "body": " 读《计算机的心智》,另一个角度看操作系统\n 关于线程 线程模式: 一个进程至少包含一个线程，也可以包含多个线程。\n线程提高多核CPU利用率： 让不同的线程运行在不同的处理器上，提高进程的执行速度。\n线程管理: 线程控制表维护线程的各种信息。同一进程内的线程共享一块内存空间。\n线程共享与独享资源划分\n   共享 独享     地址空间，全局变量，打开的文件，子进程，信号\u0026hellip; 程序计数器，寄存器，栈，状态字     线程的三种实现方式 内核级线程实现方式(1:1线程模型)：\n内核线程建立和销毁都是由操作系统负责、通过系统调用完成的。线程管理的所有工作由内核完成，应用程序没有进行线程管理的代码，只有一个到内核级线程的编程接口.内核为进程及其内部的每个线程维护上下文信息(线程表)，调度也是在内核基于线程架构的基础上完成。\n一对一线程映射:内核线程驻留在内核空间，它们是内核对象。有了内核线程，每个用户线程被映射或绑定到一个内核线程。用户线程在其生命期内都会绑定到该内核线程。一旦用户线程终止，两个线程都将离开系统。\n特点：当某个线程希望创建一个新线程或撤销一个已有线程时，它进行一个系统调用\n优势：\n 多处理器系统中，内核能够并行执行同一进程内的多个线程 如果进程中的一个线程被阻塞，能够切换同一进程内的其他线程继续执行（用户级线程的一个缺点） 所有能够阻塞线程的调用都以系统调用的形式实现，代价很小 信号是发给进程而不是线程的，线程可以“注册”它们感兴趣的信号  缺陷：\n 效率低，每次线程切换都需要陷如内核 占用系统内核资源多，操作系统需要维护线程表  用户级线程实现方式(1:n线程模型)：\n有关线程管理的所有工作都由应用程序完成，内核意识不到线程的存在. 应用程序可以通过使用线程库设计成多线程程序.\n用户级线程仅存在于用户空间中，此类线程的创建、撤销、线程之间的同步与通信功能，都无须利用系统调用来实现。用户进程利用线程库来控制用户线程。由于线程在进程内切换的规则远比进程调度和切换的规则简单，不需要用户态/核心态切换，所以切换速度快。多见于一些历史悠久的操作系统。\n用户级线程对于操作系统是不可见的，因此无法被调度到处理器内核。任意给定时刻每个进程只能够有一个线程在运行，而且只有一个处理器内核会被分配给该进程。\n库调度器从进程的多个线程中选择一个线程，然后该线程和该进程允许的一个内核线程关联起来。内核线程将被操作系统调度器指派到处理器内核。用户级线程是一种”多对一”的线程映射。\n特点：内核对线程包一无所知。从内核角度考虑，就是按正常的方式管理，即单线程进程（存在运行时系统）\n优点：\n 可以在不支持线程的操作系统中实现。 创建和销毁线程、线程切换代价等线程管理的代价比内核线程少得多, 因为保存线程状态的过程和调用程序都只是本地过程 线程能够利用的表空间和堆栈空间比内核级线程多 不需要内陷，不需要上下文切换，也不需要对内存高速缓存进行刷新，使得线程调用非常快捷  缺陷：\n 线程发生I/O或页面故障引起的阻塞时，会阻塞整个进程从而阻塞所有线程 一个单独的进程内部，没有时钟中断，所以不可能用轮转调度的方式调度线程 资源调度按照进程进行，多个处理机下，同一个进程中的线程只能在同一个处理机下分时复用(无法利用计算机多核)  混合型线程(n:m线程模型)：\n用户态的执行系统负责进程内部的线程在非阻塞时的切换；内核态的操作系统负责阻塞线程的切换，同时实现内核态和用户态线程管理。(用户态线程被多路复用到内核态线程上)\nJava线程模型 Java线程的实现\nJava中最常用的JVM是Oracle/Sun研发的HotSpot VM。在这个JVM的较新版本所支持的所有平台上，它都是使用内核级线程模型。\n这种方式实现的线程，是直接由操作系统内核支持的——由内核完成线程切换，内核通过操纵调度器（Thread Scheduler）实现线程调度，并将线程任务反映到各个处理器上。内核线程是内核的一个分身。程序一般不直接使用该内核线程，而是使用其高级接口，即轻量级进程（LWP），也即线程。\n线程调度(Java使用的线程调度方式是抢占式调度)\n程调度是指系统为线程分配处理器使用权的过程，主要调度方式有两种，分别是协同式线程调度（Cooperative Threads-Scheduling）和抢占式线程调度（Preemptive Threads-Scheduling）。\n如果使用协同式调度的多线程系统，线程的执行时间由线程本身来控制，线程把自己的工作执行完了之后，要主动通知系统切换到另外一个线程上。协同式多线程的最大好处是实现简单，而且由于线程要把自己的事情干完后才会进行线程切换，切换操作对线程自己是可知的，所以没有什么线程同步的问题。缺陷是会出现阻塞问题。\n如果使用抢占式调度的多线程系统，那么每个线程将由系统来分配执行时间，线程的切换不由线程本身来决定（在Java中，Thread.yield（）可以让出执行时间，但是要获取执行时间的话，线程本身是没有什么办法的）。在这种实现线程调度的方式下，线程的执行时间是系统可控的，也不会有一个线程导致整个进程阻塞的问题。\nJava的线程优先级\nJava语言一共设置了10个级别的线程优先级（Thread.MIN_PRIORITY至Thread.MAX_PRIORITY），在两个线程同时处于Ready状态时，优先级越高的线程越容易被系统选择执行。Java的线程是通过映射到系统的原生线程上来实现的，所以线程调度最终还是取决于操作系统，操作系统的 优先级不一定和Java的会一一对应(Linux 1~99,Window 7种)，导致Java线程优先级不一定可靠。\nJava线程状态转换\n  新建（New）：创建后尚未启动的线程处于这种状态。\n  运行（Runable）：Runable包括了操作系统线程状态中的Running和Ready，也就是处于此状态的线程有可能正在执行，也有可能正在等待着CPU为它分配执行时间。\n  无限期等待（Waiting）：处于这种状态的线程不会被分配CPU执行时间，它们要等待被其他线程显式地唤醒。以下方法会让线程陷入无限期的等待状态：\n 没有设置Timeout参数的Object.wait（）方法。 没有设置Timeout参数的Thread.join（）方法。 LockSupport.park（）方法。    限期等待（Timed Waiting）：处于这种状态的线程也不会被分配CPU执行时间，不过无须等待被其他线程显式地唤醒，在一定时间之后它们会由系统自动唤 醒。以下方法会让线程进入限期等待状态：\n Thread.sleep（）方法。 设置了Timeout参数的Object.wait（）方法。 设置了Timeout参数的Thread.join（）方法。 LockSupport.parkNanos（）方法。 LockSupport.parkUntil（）方法。    5.阻塞（Blocked）：线程被阻塞了，“阻塞状态”与“等待状态”的区别是：“阻塞状态”在等待着获取到一个排他锁，这个事件将在另外一个线程放弃这个锁的时候发生；而“等待状态”则是在等待一段时间，或者唤醒动作的发生。在程序等待进入同步区域的时候，线程将进入这种状态。\n6.结束（Terminated）：已终止线程的线程状态，线程已经结束执行。\n 线程同步 线程同步的目的：不管多线程之间的执行如何穿插，运行结果都是正确的。(保证多线程执行下结果的确定性)\n多个线程争相执行同一段代码或访问同一资源的现象称为竞争，造成竞争的代码段或者资源称为临界区，通过协调限制任何时刻都只有一个线程在临界区里，称为互斥\n线程同步方案 锁\n锁的特征：\n 锁的初始状态是打开的 进入临界区必须获得锁 出临界区必须打开锁 如果别人持有锁，必须等待  缺陷:循环等待\nWait-Notify\nsleep原语：进入睡眠，释放CPU wakeup原语:发送信号给指定进程\n伪代码：\ndefine N 100 int count = 0; void producer(void){ while(true){ if(count==N) sleep(); //生产商品 count = count + 1; if(count == 1)wakeup(consumer); } } void consumer(void){ while(true){ if(count==0) sleep(); //获取商品 count = count - 1; if(count == N -1 ) wakeup(producer); } }  缺陷：\n 如果消费者先获取商品，获取不到sleep，但是在sleep语句之前CPU切换，生产者生产商品，发送wakeup给消费者(此时消费者还没进入sleep）,当生产者生产完成，进入sleep之后，CPU切换为消费者，此时消费者继续进程Sleep，导致死锁。  2.count变量没有保护，会发生竞争\n信号量-semphore\n信号量是一个计数器，其取值为当前累积的信号数量，支持两个操作，加法操作up，减法操作down\n  up(一组原子操作):\n 将信号量的值加1,唤醒一个在该信号量上面等待的线程 程序继续执行    down(一组原子操作):\n 判断信号量的取值是否大于等于1 true 将信号量的值减去1，继续执行 false 在该信号量上等待    如果将信号量的取值限制为0和1，等效于锁。因为二元信号量是原子的，等效于锁+Wait-Notify\n例子(进程B执行完再执行进程A)：\n - 进程A 进程B - down() doSomeThing - doSomeThing up()  伪代码：\ndefine N 100 typedef int semphore;//定义信号量类型 semphore mutex = 1;//互斥信号量 semphore empty = N;//缓存区计数信号量,空位数量 semphore full = 0;//缓存区计数信号量,商品数量 void producer(void){ while(true){ down(empty); down(mutex); //生产商品 up(mutex); up(full); } } void consumer(void){ while(true){ down(full); down(mutex); //获取商品 up(mutex); up(empty); } }  缺陷：程序效率低下\n管程monitor(监视器)\n管程是一个程序语言级别的构造，它的正确运行由编译器保证。（Java Synchronized）\n把需要同步的代码使用管程的构造框起来，即将需要保护的代码置于begin monitor 和 end monitor之间。编译器在识别到monitor包裹的代码块时，在翻译成低级代码时就会将需要的操作系统原语添加上去， 使两个线程不能同时活跃在同一个管程内。\n管程的同步机制：锁(互斥) | 条件变量(控制执行的顺序)\n缺陷：依赖编译器，只能在单台计算机上生效\n 锁的实现 线程上下文切换(切换线程)方式\n中断启用和禁止来实现锁(繁忙等待)  线程自愿放弃CPU(yield之类的操作系统调用) 线程被强制放弃CPU而失去控制权(中断实现)  操作系统主要通过周期性时钟中断来获得CPU控制权。要想让一组操作变成原子操作，可以通过禁止中断，并且不自动调用yield之类让出CPU，防止进行切换，将一组操作变为原子操作。\nunlock实现\nunlock(){ disable interrupts;//启用中断 value = FREE;//value设置为FREE enable interrupts;//禁止中断 }  测试与设置指令来实现锁(繁忙等待) 测试与设置(test\u0026amp;set)指令 \u0026ldquo;读——修改——写\u0026rdquo;\n 设置操作：将x写入指定内存单元 读取操作：返回指定内存单元原来的值(写入x之前的值)  lock/unlock实现\n//value原始值=0 lock(){ while(test_and_set(value) == 1){ } } unlock(){ value = 0 } "
            }
    
        ,
            {
                "id": 55,
                "href": "https://chinalhr.github.io/post/java-cache-strategy/",
                "title": "缓存更新策略",
                "section": "post",
                "date" : "2018.11.21",
                "body": " 缓存实践\n 更新缓存的四种设计模式 Cache Aside Pattern  失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。 命中：应用程序从cache中取数据，取到后返回。 更新：先把数据存到数据库中，成功后，再让缓存失效。  缺陷：并发查询(没命中缓存在SQL中查到老数据)与更新会导致缓存存在脏数据\nRead/Write Through Pattern  Read Through：在查询操作中更新缓存 Write Through：在更新操作的时候更新缓存,再去更新数据  缺陷：如上\nWrite Behind Caching Pattern 在更新数据的时候，只更新缓存，不更新数据库，而我们的缓存会异步地批量更新数据库。\n缺陷：数据不是强一致性,系统不可控性\nJava系统中缓存与事务带来的问题  Spring Transaction与Spring Cache使用时，当外层事务回滚，这时候缓存中的数据可能已经更新，是回滚之前的数据。 可能外层事务还没有提交， 其他线程缓存就把刷新了，缓存中会存在脏数据  两种分布式缓存更新的方案 基于阿里云DTS进行缓存刷新  缓存更新：对目标数据查询的时候，没有命中缓存则对数据进行缓存，存储到Redis中 缓存删除：利用阿里云数据传输DTS服务数据库订阅监控，监控数据库目标数据表，对进行UPDATE操作的数据进行缓存删除。 规避风险：利用DTS服务，隔离事务与缓存，确保数据入库后再对数据进行缓存删除，确保缓存更新的时候拿到的是最新的数据。 避免DTS单点故障：dts消费实例只能是一个进程，不能像mq一样分布式消费. 这样在重启dts消费进程的时候，dts消息的处理逻辑就有短暂的时间不可用。 优化：新建立dts消费者进程， 处理dts消息， 并将消息转化为mq消息。  基于Spring Transaction后置处理机制  关于TransactionSynchronizationAdapter  public abstract class TransactionSynchronizationAdapter implements TransactionSynchronization, Ordered { @Override public int getOrder() { return Ordered.LOWEST_PRECEDENCE; } @Override public void suspend() { } @Override public void resume() { } @Override public void flush() { } @Override public void beforeCommit(boolean readOnly) { } @Override public void beforeCompletion() { } @Override public void afterCommit() { } //完成事务后进行回调操作  @Override public void afterCompletion(int status) { } } 缓存更新：对目标数据查询的时候，没有命中缓存则对数据进行缓存，存储到Redis中 缓存删除：创建AfterTransactionService实现TransactionSynchronizationAdapter,对完成后的事务进行缓存清除操作  参考 https://coolshell.cn/articles/17416.html\nhttps://zhuanlan.zhihu.com/p/37643608\n"
            }
    
        ,
            {
                "id": 56,
                "href": "https://chinalhr.github.io/post/os-heart-process/",
                "title": "计算机的心智-进程",
                "section": "post",
                "date" : "2018.11.15",
                "body": " 读《计算机的心智》,另一个角度看操作系统\n 关于进程 进展中的程序，一个程序被加载到内存后就变成进程。对于操作系统，进程是其提供的一种抽象，目的是通过并发提高系统CPU利用率，缩短响应时间。\n  进程的三种典型状态   进程的创建过程\n 分配进程控制块 初始化机器寄存器 初始化页表 将程序代码从磁盘读取到内存 将处理器状态设置为用户态 跳转到程序的起始地址(设置程序计数器)    UNIX 创建进程:1.fork[创建一个与自己完全一样的新进程] 2.exec[新进程的地址空间用另一个程序覆盖，跳转到新进程的起始地址完成启动]\nWindow CreateProcess函数:CreateProcess[传递进来新的程序名称，创建新的页表]\n 进程调度   程序使用CPU的三种模式\n 计算密集型(科学计算,矩阵乘法\u0026hellip;) IO密集型(游戏,人机交互\u0026hellip;) 平衡程序(网页浏览，下载，网络视频\u0026hellip;)    进程调度的目标\n  极小化平均响应时间，极大化系统吞吐，保持各个系统功能部件处于繁忙和维持公平机制。\n 进程调度算法  First Come First Server(先来先服务算法)  顾名思义\u0026hellip;\n 时间片轮转算法  周期性地进行进程切换\u0026hellip;\n Shorted Time to Completion First(短任务优先算法)  非抢占式:CPU上的程序运行到阻塞或者结束，从候选程序中选择执行时间最短的进行执行。 抢占式：新增一个进程时，就从所有进程中选择执行时间最短的运行。\n 优先级调度算法  对每个进行赋予优先级，按照优先级调度\n 保证调度算法  众生平等策略，如果系统有n个进程，保证每个进程的CPU占用时间为1/n。\n  实时调度算法\n 动态优先调度(EDF) 动态计算每个任务的截止时间进行动态调度优先级。由STCF算法改进而来，EDF就是最早截至的任务先运行，如果新进程比正在运行的进程的截止时间更靠前，就抢占当前任务进行调度。  缺陷：需要动态计算任务截止时间，消耗CPU资源，所以就有了静态调度算法\n静态优先调度(RMS) 在进行调度之前，先计算出所有任务的优先级，按照计算的优先级进行调度，任务执行过程中不接受新进程也不进行优先级调整与CPU抢占。  缺陷：没有动态优先调度灵活\n   进程通信(IPC)  管道通信(类UNIX下) 通过Shell的 | :sort \u0026lt; file |grep li [在排序sort与查找grep之间建立管道。数据从sort流向grep，作用是对file进行排序，排序结果作为grep程序的输入，在结果中找出包括li的文本行]  通过pipe函数\n管道：用于管道的读与写。半双工的，具有固定的读端和写端(他只能用于具有亲缘关系的进程之间的通信)。可以看成是一种特殊的文件，对于它的读写也可以使用普通的read、write 等函数。但是它不是普通的文件，并不属于其他任何文件系统，并且只存在于内存中。\n FIFO(命名管道，是一种文件类型) FIFO可以在无关的进程之间交换数据，与无名管道不同。  FIFO有路径名与之相关联，它以一种特殊设备文件形式存在于文件系统中。\n  Socket套接字 分为本地(UNIX域)套接字，网域套接字\n  信号 利用CPU内中断进行通信\n  在计算机里，信号就是一个内核对象(数据结构)。发送方将该数据结构的内容填好，并指明该信号的目标进程后，发出特定的软件中断。操作系统接收到特定的中断请求后，知道是有进程要发送信号，于是到特定的内核数据结构里查找信号接收方，并进行通知。接到通知的进程则对信号进行相应处理。\n  共享内存 \u0026hellip;\n  消息队列 无固定读写进程，支持多进程，多对多而并非管道之类的点对点。消息队列是消息的链接表，存放在内核中。一个消息队列由一个标识符（即队列ID）来标识。\n   参考：  https://songlee24.github.io/2015/04/21/linux-IPC/\n权衡  管道通信,Socket套接字消耗系统资源大 信号:小数据量通信 共享内存，消息队列:大数据量通信 "
            }
    
        ,
            {
                "id": 57,
                "href": "https://chinalhr.github.io/post/mybatis-shard-plugin/",
                "title": "Mybatis自定义分表Plug",
                "section": "post",
                "date" : "2018.11.12",
                "body": " 工作中需要用到MyBatis进行分表操作，简单记录实现过程\n MyBatis Interceptor MyBatis 允许你在已映射语句执行过程中的某一点使用插件来拦截,包括： Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed)[拦截执行器的方法] ParameterHandler (getParameterObject, setParameters)[拦截参数的处理] ResultSetHandler (handleResultSets, handleOutputParameters)[拦截结果集的处理] StatementHandler (prepare, parameterize, batch, update, query)[拦截Sql语法构建的处理]  Interceptor接口 public interface Interceptor { //进行拦截的时候要执行的方法 \tObject intercept(Invocation invocation) throws Throwable; //决定是否要进行拦截进而决定要返回一个什么样的目标对象 \tObject plugin(Object target); //Mybatis配置文件中指定一些属性 \tvoid setProperties(Properties properties); } @Intercepts @Intercepts用于表明当前的对象是一个Interceptor，而@Signature则表明要拦截的接口、方法以及对应的参数类型。 @Intercepts( { @Signature(method = \u0026#34;query\u0026#34;, type = Executor.class, args = { MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class }), @Signature(method = \u0026#34;prepare\u0026#34;, type = StatementHandler.class, args = { Connection.class }) }) 利用Interceptor实现自定义分表原理 利用JDBC对数据库进行操作就必须要有一个对应的Statement对象，Mybatis在执行Sql语句前也会产生一个包含Sql语句的Statement对象，Sql语句生成的时机是在Statement之前的，利用Interceptor可以在Sql语句成为Statement之前对Sql语句进行修改。\nMybatis中Statement语句是通过RoutingStatementHandler对象的 prepare方法生成的。使用Interceptor对StatementHandler的prepare方法进行拦截即可获得原始Sql。\n获取原始Sql后，可以解析出根表名，再通过一系列的策略获取到实际的分表表名，对其原始Sql进行表名替换。\n示例: @Intercepts({@Signature(type = StatementHandler.class, method = \u0026#34;prepare\u0026#34;, args = {Connection.class, Integer.class})}) public class CustomerIdShardInterceptor implements Interceptor { private static final ObjectFactory DEFAULT_OBJECT_FACTORY = new DefaultObjectFactory(); private static final ObjectWrapperFactory DEFAULT_OBJECT_WRAPPER_FACTORY = new DefaultObjectWrapperFactory(); private static final ReflectorFactory DEFAULT_REFLECTOR_FACTORY = new DefaultReflectorFactory(); @Override public Object intercept(Invocation invocation) throws Throwable { StatementHandler statementHandler = (StatementHandler) invocation.getTarget(); // 保存会话信息 \tMetaObject metaStatementHandler = MetaObject.forObject(statementHandler, DEFAULT_OBJECT_FACTORY, DEFAULT_OBJECT_WRAPPER_FACTORY, DEFAULT_REFLECTOR_FACTORY); // 获取原sql \tBoundSql boundSql = (BoundSql) metaStatementHandler.getValue(\u0026#34;delegate.boundSql\u0026#34;); //修改原有Sql数据\t\tString modifiedSql = modifySql(boundSql); //将修改后的Sql设置到metaStatementHandler中 \tmetaStatementHandler.setValue(\u0026#34;delegate.boundSql.sql\u0026#34;, modifiedSql); return invocation.proceed(); } @Override public Object plugin(Object o) { //对应类型进行封装 \tif (o instanceof StatementHandler) { return Plugin.wrap(o, this); } else { return o; } } @Override public void setProperties(Properties properties) { } private String modifySql(BoundSql boundSql) { String targetSql = boundSql.getSql().trim().toLowerCase(); String tableName = getTableName(targetSql); // 对targetSql进行表名修改 \t... return targetSql; } /** * 根据sql获取表名 * * @param sql * @return */ private String getTableName(String sql) { String[] sqls = sql.split(\u0026#34;\\\\s+\u0026#34;); switch (sqls[0]) { case \u0026#34;select\u0026#34;: { // select tableName \tfor (int i = 0; i \u0026lt; sqls.length; i++) { if (sqls[i].equals(\u0026#34;from\u0026#34;)) { return sqls[i + 1]; } } break; } case \u0026#34;update\u0026#34;: { // update tableName \treturn sqls[1]; } case \u0026#34;insert\u0026#34;: { // insert into tableName \treturn sqls[2]; } case \u0026#34;delete\u0026#34;: { // delete tableName \treturn sqls[1]; } } return StringUtils.EMPTY; } } 关于分表策略 取模 1.可以维护一个根symbols表用来存储插入表的自增的id代表固定业务id，使用id取模进行分表(存储分表table_suffix = table_(id%i),i的大小取决于数据量的大小),由此策略可分table_0~table_i-1个分表进行数据存储。 2.插入时根据symbols表的自增业务id进行取模对对应表进行插入。 3.查询时从symbols对查询业务id进行取模,决定去哪个分表查询  参考 http://elim.iteye.com/blog/1851081\nhttps://zhuanlan.zhihu.com/p/35483580\n"
            }
    
        ,
            {
                "id": 58,
                "href": "https://chinalhr.github.io/post/mybatis-sql-precompile/",
                "title": "Mybatis动态表名：# 与 $ 区别",
                "section": "post",
                "date" : "2018.11.12",
                "body": " 学习Mybatis# 与 $ 机制\n MyBatis中#与$区别 动态SQL：mybatis在对sql语句进行预编译之前，会对sql进行动态解析，解析为一个BoundSql对象，也是在此处对动态sql进行处理。\nselect * from customer where id = #{id}; ↓ select * from customer where id = ?; select * from customer where id = ${id}; ↓——传入参数为12589时 select * from customer where id = 12589;    $与#的区别\n ${ } 的变量的替换阶段是在动态 SQL 解析阶段，而 #{ }的变量的替换是在 DBMS 中。 ${ } 在预编译之前已经被变量替换了，存在 sql 注入问题 #{}将传入的参数当成一个字符串（会给传入的参数加一个单引号) ${}将传入的参数直接显示生成在sql中，不会添加引号    MyBatis动态表名 MyBatis使用中，表名作为变量时，必须使用 ${ }。因为使用#{}会带上单引号 \u0026lsquo;'，这会导致 sql 语法错误。\n示例(添加statementType=\u0026quot;STATEMENT\u0026quot;禁止预编译)：\n \u0026lt;select id=\u0026quot;selectByCustomerId\u0026quot; resultMap=\u0026quot;BaseResultMap\u0026quot; statementType=\u0026quot;STATEMENT\u0026quot;\u0026gt; select \u0026lt;include refid=\u0026quot;Base_Column_List\u0026quot;/\u0026gt; from ${tableName} where customer_id = ${customerId} "
            }
    
        ,
            {
                "id": 59,
                "href": "https://chinalhr.github.io/post/code-optimize/",
                "title": "编码技巧：防御性编程|表驱动法",
                "section": "post",
                "date" : "2018.11.03",
                "body": " 读\u0026lt;代码大全\u0026gt;\n 防御性编程 主要思想：子程序不因传入错误数据而被破坏  避免非法输入数据的破坏(校验输入的数据) 断言(建立断言机制如Guava的Preconditions)  表驱动法 主要思想：表驱动法是一种编程模式，从表里面查找信息而不使用逻辑语句(if\u0026amp;case)，作用是使逻辑语句更直白健壮。 两个问题：\n①如何从表中查询条目\n②表里面存储什么\n1.直接访问表 示例1：获取月份的天数\nInteger[] daysPerMonth = new Integer[]{31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 30}; //获取三月的天数,避免过多的if case Integer marDays = daysPerMonth[3 - 1]; 示例2：营业类型对应提成\n/** * 营业类型 - 提成 */ HashMap\u0026lt;BusinessType, Integer\u0026gt; businessTable = new HashMap\u0026lt;\u0026gt;(); businessTable.put(BusinessType.MALECONSUME,11); ... /** * 营业类型 */ private enum BusinessType{ MALECONSUME,MALEINCOME,FEMALECONSUME,FEMALEINCOME } //获取男生消费提成 Integer maleConsumePush = businessTable.get(BusinessType.MALECONSUME); 2.索引表访问 索引表访问先用一个基本数据类型从索引表中查出一个键值，再根据键值去主查询表中查询数据。 3.阶梯访问表 相对于索引表访问，阶梯访问对于不同数据范围有效，而不是不同数据点有效。\n示例：成绩类型-分数\nprivate static HashMap\u0026lt;LadderType, Integer\u0026gt; ladderTable; static { /** * 成绩类型 - 分数上限 */ ladderTable = new LinkedHashMap\u0026lt;\u0026gt;(); ladderTable.put(LadderType.E, 50); ladderTable.put(LadderType.D, 65); ladderTable.put(LadderType.C, 75); ladderTable.put(LadderType.B, 90); ladderTable.put(LadderType.A, 100); } /** * 根据分数获取成绩类型 * @param socre * @return */ private static LadderType getStudentLadder(Integer socre) { for (LadderType type : ladderTable.keySet()) { if (socre\u0026lt;ladderTable.get(type)){ return type; } } throw new RuntimeException(\u0026#34;Error cannot find type\u0026#34;); } /** * 成绩类型(A\u0026gt;=90%,B\u0026lt;90%,C\u0026lt;75%,D\u0026lt;65%,F\u0026lt;50%) */ private enum LadderType { A, B, C, D, E, F } "
            }
    
        ,
            {
                "id": 60,
                "href": "https://chinalhr.github.io/post/highavailability-httpclient/",
                "title": "高可用的HttpClient",
                "section": "post",
                "date" : "2018.10.17",
                "body": " 对HttpClient的优化，基于HttpClient4.4+的连接池(PoolingHttpClientConnectionManager) 使其在高QPS，并发请求下提高效率。\n HttpClient优化点  httpclient是一个线程安全的类，全局维护一个可避免httpclient反复创建带来的开销。 使用PoolingHttpClientConnectionManager连接池避免反复创建tcp连接 定时监控清理关闭服务端已CLOSE的连接 合理配置连接池的总连接数与并发数  HttpClient与RestTemplate实现  配置PoolingHttpClientConnectionManager连接池  public static CloseableHttpClient getCloseableHttpClient(boolean isRetry) { // 长连接保持30秒 \tPoolingHttpClientConnectionManager pollingConnectionManager = new PoolingHttpClientConnectionManager(30, TimeUnit.SECONDS); // 总连接数 \tpollingConnectionManager.setMaxTotal(200); // 默认同路由的并发数 \tpollingConnectionManager.setDefaultMaxPerRoute(100); HttpClientBuilder httpClientBuilder = HttpClients.custom(); httpClientBuilder.setConnectionManager(pollingConnectionManager); // 重试次数，默认是3次，设置为2次，没有开启 \tif (isRetry) { httpClientBuilder.setRetryHandler(new DefaultHttpRequestRetryHandler(2, true)); } else { httpClientBuilder.setRetryHandler(new DefaultHttpRequestRetryHandler(0, false)); } // 保持长连接配置，需要在头添加Keep-Alive \thttpClientBuilder.setKeepAliveStrategy(DefaultConnectionKeepAliveStrategy.INSTANCE); CloseableHttpClient httpClient = httpClientBuilder.build(); runIdleConnectionMonitor(pollingConnectionManager); return httpClient; }\t自定义keepAliveStrategy  private static ConnectionKeepAliveStrategy getConnectionKeepAliveStrategy(){ return (response, context) -\u0026gt; { HeaderElementIterator it = new BasicHeaderElementIterator (response.headerIterator(HTTP.CONN_KEEP_ALIVE)); while (it.hasNext()) { HeaderElement he = it.nextElement(); String param = he.getName(); String value = he.getValue(); if (value != null \u0026amp;\u0026amp; param.equalsIgnoreCase (\u0026#34;timeout\u0026#34;)) { return Long.parseLong(value) * 1000; } } return 60 * 1000;//如果没有约定，则默认定义时长为60s \t}; } 定时监控清理实现连接关闭  private static void runIdleConnectionMonitor(HttpClientConnectionManager clientConnectionManager) { FixedRateSchedule schedule = new FixedRateScheduleImpl(); schedule.setPoolTag(\u0026#34;IDLE_CONNECTION_MONITOR_POOL\u0026#34;); schedule.init(); schedule.schedule(() -\u0026gt; { //关闭过期的链接  clientConnectionManager.closeExpiredConnections(); //关闭闲置超过30s的链接  clientConnectionManager.closeIdleConnections(30, TimeUnit.SECONDS); }, IDLE_INITIALDELAY, IDLE_PERIOD, TimeUnit.MILLISECONDS); } 代码  https://github.com/ChinaLHR/HttpClientOptimization\n参考 https://www.cnblogs.com/bethunebtj/p/8493379.html\nhttps://hc.apache.org/httpcomponents-client-ga/tutorial/html/connmgmt.html\nhttps://www.oschina.net/question/2011290_2199294\n"
            }
    
        ,
            {
                "id": 61,
                "href": "https://chinalhr.github.io/post/joda-datetime-record/",
                "title": "Joda DateTime记录",
                "section": "post",
                "date" : "2018.10.04",
                "body": " 用来记录joda DateTime工具类使用\n //获取X时间前 minusDays/minusHours/minusMillis... \tDateTime.now().minusDays(15).toDate(); //获取X时间后 plusDays/plusHours/plusMillis \tDateTime.now().plusDays(15).toDate(); /** * 获取DateTime今日零点 * * @return */ public static Date getStartOfToday(DateTime dateTime) { return dateTime.withMillisOfDay(0).toDate(); } /** * 获取DateTime昨日零点 * * @return */ public static Date getStartOfYesterday(DateTime dateTime) { return dateTime.plusDays(-1).withMillisOfDay(0).toDate(); } /** * 获取上周一零点 * * @param dateTime * @return */ public static Date getLastMonday(DateTime dateTime) { return dateTime.minusWeeks(1) .withDayOfWeek(1) .withTimeAtStartOfDay() .toDate(); } /** * 获取上周日零点 * * @param dateTime * @return */ public static Date getLastSunday(DateTime dateTime) { return dateTime .minusDays(dateTime.getDayOfWeek()) .withTime(23, 59, 59, 0) .toDate(); } /** * 获取上月一号零点 * @param dateTime * @return */ public static Date getLastMonthStratTime(DateTime dateTime) { return dateTime.minusMonths(1) .withDayOfMonth(1) .withTimeAtStartOfDay().toDate(); } /** * 获取上月底零点 * @param dateTime * @return */ public static Date getLastMonthEndTime(DateTime dateTime) { return dateTime.minusDays(dateTime.getDayOfMonth()) .withTime(23, 59, 59, 0).toDate(); } //获取两个DateTime相隔的时间 \tint between = new Period(start, end, PeriodType.days()).getDays(); //时间解析 \tDateTimeFormatter format = DateTimeFormat .forPattern(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); DateTime d = DateTime.parse(\u0026#34;2017-10-17 20:06:05\u0026#34;,format) ; //得到某一日期的年与日时分秒 \tDateTime time = new DateTime() ; time.getYear(); time.getMonthOfYear(); time.getDayOfMonth();... "
            }
    
        ,
            {
                "id": 62,
                "href": "https://chinalhr.github.io/post/spring-aop/",
                "title": "Spring Aop分析",
                "section": "post",
                "date" : "2018.09.24",
                "body": " Spring Aop源码分析\n 示例 编程式aop  ProxyFactory proxyFactory = new ProxyFactory();//创建代理工厂 proxyFactory.setTarget(new LoginServiceImpl());//设置目标对象 proxyFactory.addAdvice(new LogAroundLogin());//环绕增强 LoginService loginService = (LoginService) proxyFactory.getProxy();//从代理工厂中获取代理 loginService.login(); //执行方法 代理工厂ProxyFactory ProxyFactory proxyFactory = new ProxyFactory();代理工厂的作用就是使用编程的方式创建AOP代理。ProxyFactory继承自AdvisedSupport， AdvicedSupport是AOP代理的配置管理器。\nTargetSource TargetSource用来获取当前的Target，也就是TargetSource中会保存着我们的的实现类。\npublic interface TargetSource { //返回目标类的类型 Class getTargetClass(); //查看TargetSource是否是static的 //静态的TargetSource每次都返回同一个Target boolean isStatic(); //获取目标类的实例 Object getTarget() throws Exception; //释放目标类 void releaseTarget(Object target) throws Exception; } 在AdvisedSupport中，setTarget等于设置一个SingletonTargetSource。\naddAdvice public void addAdvice(Advice advice) throws AopConfigException { //advisors是Advice列表，是一个LinkedList //如果被添加进来的是一个Interceptor，会先被包装成一个Advice //添加之前现获取advisor的大小，当做添加的Advice的位置 int pos = (this.advisors != null) ? this.advisors.size() : 0; //添加Advice addAdvice(pos, advice); } public void addAdvice(int pos, Advice advice) throws AopConfigException { //只能处理实现了AOP联盟的接口的拦截器 if (advice instanceof Interceptor \u0026amp;\u0026amp; !(advice instanceof MethodInterceptor)) { throw new AopConfigException(getClass().getName() + \u0026quot; only handles AOP Alliance MethodInterceptors\u0026quot;); } //IntroductionInfo接口类型，表示引介信息 if (advice instanceof IntroductionInfo) { //不需要IntroductionAdvisor addAdvisor(pos, new DefaultIntroductionAdvisor(advice, (IntroductionInfo) advice)); } //动态引介增强的处理 else if (advice instanceof DynamicIntroductionAdvice) { //需要IntroductionAdvisor throw new AopConfigException(\u0026quot;DynamicIntroductionAdvice may only be added as part of IntroductionAdvisor\u0026quot;); } else { //添加增强器，需要先把我们的增强包装成增强器，然后添加 addAdvisor(pos, new DefaultPointcutAdvisor(advice)); } } 到添加增强的时候，实际调用添加增强器这个方法，首先需要把我们的Advice包装成一个PointCutAdvisor，然后在添加增强器。方法new DefaultPointcutAdvisor(advice)，将Advice包装成一个DefaultPointcutAdvisor。其实就是将advice和默认的Pointcut包装进DefaultPointcutAdvisor。\nDefaultPointcutAdvisor是Advisor的最常用的一个实现，可以使用任意类型的Pointcut和Advice，但是不能使用Introduction。\n获取代理  ProxyFactory的getProxy方法  public Object getProxy() { //创建一个AOP代理 AopProxy proxy = createAopProxy(); //返回代理 return proxy.getProxy(); } protected synchronized AopProxy createAopProxy() { if (!this.isActive) { activate(); } //获取AOP代理工厂，然后创建代理 return getAopProxyFactory().createAopProxy(this); } public AopProxy createAopProxy(AdvisedSupport advisedSupport) throws AopConfigException { //对于指定了使用CGLIB方式，或者代理的是类，或者代理的不是接口，就使用CGLIB的方式来创建代理 boolean useCglib = advisedSupport.getOptimize() || advisedSupport.getProxyTargetClass() || advisedSupport.getProxiedInterfaces().length == 0; if (useCglib) { return CglibProxyFactory.createCglibProxy(advisedSupport); } else { //使用JDK动态代理来创建代理 return new JdkDynamicAopProxy(advisedSupport); } } Joinpoint（连接点） 连接点是指那些被拦截到的点。在Spring 中,这些点指的是方法,因为 Spring 只支持方法类型的连接点。\npublic interface Joinpoint { //开始调用拦截器链中的下一个拦截器 Object proceed() throws Throwable; // Object getThis(); // AccessibleObject getStaticPart(); } Pointcut(切入点) 所谓切入点是指我们要对哪些 Joinpoint 进行拦截的定义。 PointCut 依赖了ClassFilter和MethodMatcher,ClassFilter用来指定特定的类，MethodMatcher 指定特定的函数,能实现函数级别的AOP。\npublic interface Pointcut { //类过滤器，可以知道哪些类需要拦截 ClassFilter getClassFilter(); //方法匹配器，可以知道哪些方法需要拦截 MethodMatcher getMethodMatcher(); // could add getFieldMatcher() without breaking most existing code Pointcut TRUE = TruePointcut.INSTANCE; }  ClassFilter接口  public interface ClassFilter { //判断给定的类是不是要拦截 boolean matches(Class clazz); ClassFilter TRUE = TrueClassFilter.INSTANCE; }  MethodMatcher接口  public interface MethodMatcher { / 静态方法匹配 boolean matches(Method m, Class targetClass); //是否是运行时动态匹配 boolean isRuntime(); //运行是动态匹配 boolean matches(Method m, Class targetClass, Object[] args); MethodMatcher TRUE = TrueMethodMatcher.INSTANCE; }  MethodMatcher 两个实现类  StaticMethodMatcher：不在运行时检测Joinpoint的参数(可以利用框架内缓存，性能高) DynamicMethodMatcher：DynamicMethodMatcher要在运行时实时检测Joinpoint的参数    Advice（通知/增强） Advice不属于Spring，是AOP联盟定义的接口。Advice接口并没有定义任何方法，是一个空的接口，用来做标记，实现了此接口的的类是一个通知类。 通知是指拦截到 Joinpoint 之后所要做的事情就是通知。通知分为【前置通知】, 【后置通知】,【异常通知】,【最终通知】, 【环绕通知】(切面要完成的功能)\nBeforeAdvice，前置增强，意思是在我们的目标类之前调用的增强。这个接口也没有定义任何方法。 AfterReturningAdvice，方法正常返回前的增强，该增强可以看到方法的返回值，但是不能更改返回值，该接口有一个方法afterReturning ThrowsAdvice，抛出异常时候的增强，也是一个标志接口，没有定义任何方法。 Interceptor，拦截器，也没有定义任何方法，表示一个通用的拦截器。不属于Spring，是AOP联盟定义的接口 DynamicIntroductionAdvice，动态引介增强，有一个方法implementsInterface。  per-class类型的Advice（可以在目标对象类的所有实例之间共享，通常只提供方法拦截功能，不会对目标对象保存任何状态或添加新功能）   per-instance类型的Advice（不会在目标类所有实例之间共享，而是会为不同的实例对象保存他们各自的状态以及相关逻辑）Introduction可以在不改变目标类的定义的情况下，为对象添加新的属性与行为  Advisor(切面/增强器) Advisor是切入点和通知（引介）的结合。增强器，它持有一个增强Advice，还持有一个过滤器，来决定Advice可以用在哪里。\nSpring AOP的Pointcut Advisor\nAbstractPointcutAdvisor 实现了Ordered,为多个Advice指定顺序，顺序为Int类型，越小优先级越高, AbstractGenericPointcutAdvisor 指定了Advice，除了Introduction之外的类型\nProxy(代理) 一个类被 AOP 织入增强后，就产生一个结果代理类。  ProxyConfig  \tprivate boolean proxyTargetClass = false;//true,使用CGLIB,false,使用原生 private boolean optimize = false;//是否进行优化 boolean opaque = false;//是否强制转化为advised boolean exposeProxy = false;//AOP生成对象时，绑定到ThreadLocal, 可以通过AopContext获取 private boolean frozen = false;//代理信息一旦设置，是否允许改变  ProxyFactory ProxyFactory是Spring的AOP织入器，接受Pointcut/Advice返回织入了横切逻辑的目标对象代理。 ProxyFactoryBean 本质上是一个用来生产Proxy的FactoryBean，AOP与IOC的融合。 （如果容器中某个对象依赖于ProxyFactoryBean ，他将会使用到ProxyFactoryBean的getObject()方法返回的内容）  aop核心实现(Weaving织入器) 将AOP融入Bean的创建过程,AspectJ方式织入的核心，是一个BeanPostProcess（会扫描所有的Pointcut与遍历所有Bean,并对需要的Bean进行织入-自动代理，当对象实例化的时候，为其生成代理对象并返回）\n简单的实现如下:\npublic class AspectJAwareAdvisorAutoProxyCreator implements BeanPostProcessor, BeanFactoryAware { private AbstractBeanFactory beanFactory; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws Exception { return bean; } //在Bean创建之后对Bean进行AOP处理 @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws Exception { if (bean instanceof AspectJExpressionPointcutAdvisor) { return bean; } if (bean instanceof MethodInterceptor) { return bean; } //获取所有Pointcut，进行切面处理，返回完成Aop的Proxy List\u0026lt;AspectJExpressionPointcutAdvisor\u0026gt; advisors = beanFactory .getBeansForType(AspectJExpressionPointcutAdvisor.class); for (AspectJExpressionPointcutAdvisor advisor : advisors) { if (advisor.getPointcut().getClassFilter().matches(bean.getClass())) { // 1. 设置被代理对象(Joinpoint) AdvisedSupport advisedSupport = new AdvisedSupport(); TargetSource targetSource = new TargetSource(bean, bean.getClass().getInterfaces()); advisedSupport.setTargetSource(targetSource); // 2. 设置拦截器(Advice) advisedSupport.setMethodInterceptor((MethodInterceptor) advisor.getAdvice()); advisedSupport.setMethodMatcher(advisor.getPointcut().getMethodMatcher()); // 3. 创建代理(Proxy) return new JdkDynamicAopProxy(advisedSupport).getProxy(); } } return bean; } @Override public void setBeanFactory(BeanFactory beanFactory) throws Exception { this.beanFactory = (AbstractBeanFactory) beanFactory; } } "
            }
    
        ,
            {
                "id": 63,
                "href": "https://chinalhr.github.io/post/spring-cache-shrio-bug/",
                "title": "Shrio导致SpringCache缓存失效原因分析",
                "section": "post",
                "date" : "2018.09.22",
                "body": " 记录Shrio与SpringCache使用过程遇到的问题\n 问题 Spring Cache 和 Apache Shiro 整合时，自定义的shiroRealm中引用了service，会导致service的Cache相关注解作用失效\n分析 关于BeanPostProcessor  BeanPostProcessor:构建Bean的时候调用，会处理所有符合条件的对象实例(扫描所有Bean进行处理,Aop实现就是通过BeanPostProcessor找到匹配的Pointcut进行自动代理)   提供了postProcessBeforeInitialization与postProcessAfterInitialization方法，对所有实现了InitializingBean的Bean的afterPropertiesSet方法前后执行。 BeanPostProcessor本身也是一个Bean，一般而言其实例化时机要早过普通的Bean，但是BeanPostProcessor也会依赖一些Bean，这就导致了一些Bean的实例化早于BeanPostProcessor，由此会导致一些问题。  BeanPostProcessor启动阶段对其依赖的Bean造成的影响  AbstractApplicationContext refresh是Spring IOC容器的核心方法，这个方法的作用是创建加载Spring容器配置(包括.xml配置,property文件和数据库模式等) AbstractApplicationContext refresh()——\u0026gt;registerBeanPostProcessors(beanFactory)方法会注册BeanPostProcessors：  public static void registerBeanPostProcessors( ConfigurableListableBeanFactory beanFactory, AbstractApplicationContext applicationContext) { String[] postProcessorNames = beanFactory.getBeanNamesForType(BeanPostProcessor.class, true, false); // 注册BeanPostProcessorChecker \t// 检查可在当前Bean上起作用的BeanPostProcessor个数与总的BeanPostProcessor个数，如果起作用的个数少于总数打印：//xxx is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for //auto-proxying) \tint beanProcessorTargetCount = beanFactory.getBeanPostProcessorCount() + 1 + postProcessorNames.length; beanFactory.addBeanPostProcessor(new BeanPostProcessorChecker(beanFactory, beanProcessorTargetCount)); // Separate between BeanPostProcessors that implement PriorityOrdered, \t// Ordered, and the rest. \tList\u0026lt;BeanPostProcessor\u0026gt; priorityOrderedPostProcessors = new ArrayList\u0026lt;BeanPostProcessor\u0026gt;(); List\u0026lt;BeanPostProcessor\u0026gt; internalPostProcessors = new ArrayList\u0026lt;BeanPostProcessor\u0026gt;(); List\u0026lt;String\u0026gt; orderedPostProcessorNames = new ArrayList\u0026lt;String\u0026gt;(); List\u0026lt;String\u0026gt; nonOrderedPostProcessorNames = new ArrayList\u0026lt;String\u0026gt;(); for (String ppName : postProcessorNames) { if (beanFactory.isTypeMatch(ppName, PriorityOrdered.class)) { //getBean PriorityOrdered类型的BeanPostProcessor会预初始化 \tBeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); priorityOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } else if (beanFactory.isTypeMatch(ppName, Ordered.class)) { orderedPostProcessorNames.add(ppName); } else { nonOrderedPostProcessorNames.add(ppName); } } // 1，注册实现 PriorityOrdered BeanPostProcessors \tsortPostProcessors(beanFactory, priorityOrderedPostProcessors); registerBeanPostProcessors(beanFactory, priorityOrderedPostProcessors); // 2，注册实现 Ordered BeanPostProcessors \tList\u0026lt;BeanPostProcessor\u0026gt; orderedPostProcessors = new ArrayList\u0026lt;BeanPostProcessor\u0026gt;(); for (String ppName : orderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); orderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } sortPostProcessors(beanFactory, orderedPostProcessors); registerBeanPostProcessors(beanFactory, orderedPostProcessors); // 3，注册所有无序(没有实现Ordered/ PriorityOrdered) BeanPostProcessors. \tList\u0026lt;BeanPostProcessor\u0026gt; nonOrderedPostProcessors = new ArrayList\u0026lt;BeanPostProcessor\u0026gt;(); for (String ppName : nonOrderedPostProcessorNames) { BeanPostProcessor pp = beanFactory.getBean(ppName, BeanPostProcessor.class); nonOrderedPostProcessors.add(pp); if (pp instanceof MergedBeanDefinitionPostProcessor) { internalPostProcessors.add(pp); } } registerBeanPostProcessors(beanFactory, nonOrderedPostProcessors); // 4, 注册所有内部(MergedBeanDefinitionPostProcessor) BeanPostProcessors. \tsortPostProcessors(beanFactory, internalPostProcessors); registerBeanPostProcessors(beanFactory, internalPostProcessors); // Re-register post-processor for detecting inner beans as ApplicationListeners, \t// moving it to the end of the processor chain (for picking up proxies etc). \tbeanFactory.addBeanPostProcessor(new ApplicationListenerDetector(applicationContext)); } BeanPostProcessor执行顺序如下：\n 实现了PriorityOrdered接口的BeanPostProcessor 实现了Ordered接口的BeanPostProcessor 注册无实现任何接口的BeanPostProcessor 实现了MergedBeanDefinitionPostProcessor接口的BeanPostProcessor  关于PriorityOrdered 实现了PriorityOrdered的BeanPostProcessor先于其他BeanPostProcessor，并会影响到其他BeanPostProcessor的autowiring behavior\nSpring的一些BeanPostProcessor  ScheduledAnnotationBeanPostProcessor:实现了Ordered AsyncAnnotationBeanPostProcessor:实现了Ordered AbstractAutoProxyCreator (Transactional|Cache):实现了Ordered  Shiro LifecycleBeanPostProcessor导致的问题 Spring整合shiro的系统中，需要在配置bean的时候加入LifecycleBeanPostProcessor(Shiro提供的一个BeanPostProcessor类),用来管理shiro一些bean的生命周期。\npublic class LifecycleBeanPostProcessor implements DestructionAwareBeanPostProcessor, PriorityOrdered LifecycleBeanPostProcessor实现了BeanPostProcessor与PriorityOrdered，postProcessBeforeInitialization方法,调用了自定义Realm（AuthorizingRealm）中实现了的 init() 方法初始化授权缓存，自定义Realm中依赖的一些Bean被提前初始化了，导致Spring某些BeanPostProcessor（Ordered级别下）不能拦截到这些Bean，造成依赖功能的失效(如Transaction ,Async,Cache\u0026hellip;)\nprotected void onInit() { super.onInit(); //trigger obtaining the authorization cache if possible  getAvailableAuthorizationCache(); } private Cache\u0026lt;Object, AuthorizationInfo\u0026gt; getAvailableAuthorizationCache() { Cache\u0026lt;Object, AuthorizationInfo\u0026gt; cache = getAuthorizationCache(); if (cache == null \u0026amp;\u0026amp; isAuthorizationCachingEnabled()) { cache = getAuthorizationCacheLazy(); } return cache; }  通过Debug查看registerBeanPostProcessors  @Lazy @Lazy修饰Spring Bean类,用于指定该Bean是否取消预初始化。 容器初始化方法在AbstractApplicationContext的refresh()——\u0026gt;finishBeanFactoryInitialization(beanFactory)——\u0026gt;preInstantiateSingletons()\nfor (String beanName : beanNames) { RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); //单例并且非Lazy类型的才会在容器初始化的时候被注入 \tif (!bd.isAbstract() \u0026amp;\u0026amp; bd.isSingleton() \u0026amp;\u0026amp; !bd.isLazyInit()) { if (isFactoryBean(beanName)) { final FactoryBean\u0026lt;?\u0026gt; factory = (FactoryBean\u0026lt;?\u0026gt;) getBean(FACTORY_BEAN_PREFIX + beanName); boolean isEagerInit; if (System.getSecurityManager() != null \u0026amp;\u0026amp; factory instanceof SmartFactoryBean) { isEagerInit = AccessController.doPrivileged(new PrivilegedAction\u0026lt;Boolean\u0026gt;() { @Override public Boolean run() { return ((SmartFactoryBean\u0026lt;?\u0026gt;) factory).isEagerInit(); } }, getAccessControlContext()); } else { isEagerInit = (factory instanceof SmartFactoryBean \u0026amp;\u0026amp; ((SmartFactoryBean\u0026lt;?\u0026gt;) factory).isEagerInit()); } if (isEagerInit) { getBean(beanName); } } else { getBean(beanName); } } } 添加了@Lazy注解的Bean在容器初始化的过程中不会进行依赖注入，只有当第一个getBean的时候才会实例化Bean。\n解决 通过对自定义Realm中依赖的Bean加上@Lazy，使其延迟加载。\n参考  http://stackoverflow.com/questions/21512791/spring-service-with-cacheable-methods-gets-initialized-without-cache-when-autowi 《Spring揭秘》 《深入分析Java Web技术内幕》 "
            }
    
        ,
            {
                "id": 64,
                "href": "https://chinalhr.github.io/post/uid-generate-snowflake/",
                "title": "基于SnowFlake的分布式UID生成服务",
                "section": "post",
                "date" : "2018.09.08",
                "body": " 分析与改造SnowFlake算法\n 背景 公司原本订单ID的生成是基于Java Random与Hash算法的单机UID生成策略，但是后端系统的生成环境是基于SLB，Nginx与Java多进程的分布式环境，导致出现了UID 碰撞,产生线上问题。现准备基于Twitter的SnowFlake，将UID生成独立出来做一个统一的全局ID生成服务。\nSnowFlake概述 SnowFlake算法用来生成64位的ID，刚好可以用long整型存储，能够用于分布式系统中生产唯一的ID， 并且生成的ID有大致的顺序。 生成的64位ID可以分成5个部分：\n0 - 41位时间戳 - 5位数据中心标识 - 5位机器标识 - 12位序列号  5位数据中心标识跟5位机器标识这样的分配仅仅是当前实现中分配的，可以按其它的分配比例分配，如10位机器标识，不需要数据中心标识\u0026hellip;\n 1位标识部分，在java中由于long的最高位是符号位，正数是0，负数是1，一般生成的ID为正数，所以为0 41位时间戳部分，这个是毫秒级的时间，一般实现上不会存储当前的时间戳，而是时间戳的差值（当前时间-固定的开始时间），这样可以使产生的ID从更小值开始；41位的时间戳可以使用69年，41位可以表示241−1个毫秒的值，转化成单位年则是(241−1)/(1000∗60∗60∗24∗365)=69年 10位节点部分，Twitter实现中使用前5位作为数据中心标识，后5位作为机器标识，可以部署1024个节点(5位可支持2^5 = 0~31整型,32*32=1024节点)； 12位序列号部分，支持同一毫秒内同一个节点可以生成4096个ID(2^12=4096)；  简单实现 结构 0 - 41位时间戳 - 10位机器标识 - 12位序列号  集群环境下机器标识获取   IP后三位\n启动系统的时候获取IP地址(或者截取IP后三位)作为机器标识(0~255)。缺陷：会浪费256~1024位的机器标识\n  Redis Set\n①在服务启动时通过Redis setnx \u0026lt;key:机器标识 value:ip地址\u0026gt;,for 1024对机器标识进行获取，如果Key不存在则设置当前IP地址，并使用key作为当前服务的机器标识。\n②在服务运行过程中使用ScheduleThreadPool或者DeamonThread为当前服务的Redis Key续时,避免其他服务获取到相同的机器标识\n  其他 1. 服务鉴权|IP白名单 2. 服务限流  代码  Java 实现  /** * @Author : lhr * @Date : 12:03 2018/9/14 * \u0026lt;p\u0026gt; * 基于Twitter SnowFlake算法 UID生成类 * 0 - 41位时间戳 - 10位机器标识 - 12位序列号 * 41位时间戳：时间戳的差值（当前时间-固定的开始时间START_STMP） * 10位机器标识:固定值,取IP地址后八位 * 12位序列号：同一毫秒内同一个节点可以生成4096个ID * \u0026lt;p\u0026gt; * 0 - 000000000 0000000000 0000000000 0000000000 0 - 000000000 - 0000000000 00 */ public class SnowFlakeUidGenerate { /** * 起始时间戳 2018-09-10 00:00:00 */ private final static long START_STMP = 1536508800000L; private static ReentrantLock reentrantLock = new ReentrantLock(); /** * 每一部分占用的位数 * SEQUENCE_BIT:序列号占用的位数 * MACHINE_BIT:机器标识占用的位数(0-1024) */ private final static long SEQUENCE_BIT = 12; private final static long MACHINE_BIT = 10; /** * 每一部分占用的最大值 * MAX_SEQUENCE_NUM:序列号最大值 * MAX_MACHINE_NUM:机器标识最大值 */ private final static long MAX_SEQUENCE_NUM = -1L ^ (-1L \u0026lt;\u0026lt; SEQUENCE_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L \u0026lt;\u0026lt; MACHINE_BIT); /** * 每一部分向左移动 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long TIMESTMP_LEFT = SEQUENCE_BIT + MACHINE_BIT; private static long machineId = 0L; private static long sequence = 0L; private static long lastStmp = -1L; /** * 产生下一个Id * * @return */ public static long nextId() { reentrantLock.lock(); try { long currStmp = getNewstmp(); if (currStmp \u0026lt; lastStmp) { String msg = String.format(\u0026#34;SnowFlakeUidGenerate: 机器时钟发生错误,无法生成UID \u0026#34; + \u0026#34;machineId:{} ; currStmp:{} ; lastStmp:{}\u0026#34;, machineId, currStmp, lastStmp); throw new RuntimeException(msg); } if (currStmp == lastStmp) { //相同毫秒内，序列号自增  sequence = (sequence + 1) \u0026amp; MAX_SEQUENCE_NUM; //同一毫秒的序列数已经达到最大  if (sequence == 0L) { currStmp = getNextMill(); } } else { //不同毫秒内，序列号置为0  sequence = 0L; } lastStmp = currStmp; return ((currStmp - START_STMP) \u0026lt;\u0026lt; TIMESTMP_LEFT) | machineId \u0026lt;\u0026lt; MACHINE_LEFT | sequence; } finally { reentrantLock.unlock(); } } private static long getNextMill() { long mill = getNewstmp(); while (mill \u0026lt;= lastStmp) { mill = getNewstmp(); } return mill; } private static long getNewstmp() { return System.currentTimeMillis(); } public static void initMachineId(long currentMachineId) { if (currentMachineId \u0026gt; MAX_MACHINE_NUM || currentMachineId \u0026lt; 0) { String msg = String.format(\u0026#34;SnowFlakeUidGenerate 机器标识不正确 machineId:{}\u0026#34;, machineId); throw new RuntimeException(msg); } machineId = currentMachineId; } public static long getMachineId() { return machineId; } }  核心操作  ((currStmp - START_STMP) \u0026lt;\u0026lt; TIMESTMP_LEFT) 使用当前时间戳减去开始时间戳得到时间戳数值,时间戳数值左移22位进行bit位对齐 0 [000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00]00 0000 0000 0000 0000 0000 | machineId \u0026lt;\u0026lt; MACHINE_LEFT 10位的机器ID左移12位进行bit位对其,并于上述二进制数值进行|或操作,进行bit位对齐 0 000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00 [00 0000 0000] 0000 0000 0000 | 0 [000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00]00 0000 0000 0000 0000 0000 = 0 [000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00][00 0000 0000] 0000 0000 0000 | sequence 12位的序列号与上述二进制数值进行|或操作,进行bit位对齐 得到 0 [000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00][00 0000 0000][0000 0000 0000] SnowFlake改造 固定长度UID改造 Snowflake生成的ID长度不是固定的，和设置的起始时间有关系。多数订单号需要一个统一长度的订单ID。可以在64位数前面使用01达到确定位数，对机器ID与序列号进行取舍。(对于机房小的小型公司可以采用缩减机器ID的方式，对于并发性不高的业务可以采用缩减序列号的方式，减小毫秒级别UID的生成数量)\n/** * @Author : lhr * @Date : 14:06 2019/7/30 * \u0026lt;p\u0026gt; * 基于Twitter SnowFlake算法改造 UserId生成类 生成固定长度为19的ID * 01 - 41位时间戳 - 10位机器标识 - 11位序列号 * 41位时间戳：时间戳的差值（当前时间-固定的开始时间START_STMP） * 10位机器标识:固定值,取IP地址后八位 * 11位序列号：同一毫秒内同一个节点可以生成2048个ID * \u0026lt;p\u0026gt; * 01 - 000000000 0000000000 0000000000 0000000000 0 - 000000000 - 0000000000 0 */ public class SnowFlakeUserIdGenerate { /** * 起始时间戳 2018-09-10 00:00:00 */ private final static long START_STMP = 1536508800000L; private static ReentrantLock reentrantLock = new ReentrantLock(); /** * 有效bit位 */ private static final long bitNum = 62; /** * 每一部分占用的位数 * SEQUENCE_BIT:序列号占用的位数 * MACHINE_BIT:机器标识占用的位数(0-1024) */ private final static long SEQUENCE_BIT = 11; private final static long MACHINE_BIT = 10; /** * 每一部分占用的最大值 * MAX_SEQUENCE_NUM:序列号最大值 * MAX_MACHINE_NUM:机器标识最大值 */ private final static long MAX_SEQUENCE_NUM = -1L ^ (-1L \u0026lt;\u0026lt; SEQUENCE_BIT); private final static long MAX_MACHINE_NUM = -1L ^ (-1L \u0026lt;\u0026lt; MACHINE_BIT); /** * 每一部分向左移动 */ private final static long MACHINE_LEFT = SEQUENCE_BIT; private final static long TIMESTMP_LEFT = SEQUENCE_BIT + MACHINE_BIT; private static long machineId = 0L; private static long sequence = 0L; private static long lastStmp = -1L; /** * 产生下一个Id * * @return */ public static long nextId() { reentrantLock.lock(); try { long currStmp = getNewstmp(); if (currStmp \u0026lt; lastStmp) { String msg = String.format(\u0026#34;SnowFlakeUserIdGenerate: 机器时钟发生错误,无法生成UID \u0026#34; + \u0026#34;machineId:{} ; currStmp:{} ; lastStmp:{}\u0026#34;, machineId, currStmp, lastStmp); throw new RuntimeException(msg); } if (currStmp == lastStmp) { //相同毫秒内，序列号自增  sequence = (sequence + 1) \u0026amp; MAX_SEQUENCE_NUM; //同一毫秒的序列数已经达到最大  if (sequence == 0L) { currStmp = getNextMill(); } } else { //不同毫秒内，序列号置为0  sequence = 0L; } lastStmp = currStmp; return ((currStmp - START_STMP) \u0026lt;\u0026lt; TIMESTMP_LEFT) | machineId \u0026lt;\u0026lt; MACHINE_LEFT | sequence | 1L \u0026lt;\u0026lt; bitNum; } finally { reentrantLock.unlock(); } } private static long getNextMill() { long mill = getNewstmp(); while (mill \u0026lt;= lastStmp) { mill = getNewstmp(); } return mill; } private static long getNewstmp() { return System.currentTimeMillis(); } public static void initMachineId(long currentMachineId) { if (currentMachineId \u0026gt; MAX_MACHINE_NUM || currentMachineId \u0026lt; 0) { String msg = String.format(\u0026#34;SnowFlakeUserIdGenerate 机器标识不正确 machineId:{}\u0026#34;, machineId); throw new RuntimeException(msg); } machineId = currentMachineId; } public static long getMachineId() { return machineId; } }  核心操作  | 1L \u0026lt;\u0026lt; bitNum; 1左移62个有效bit位，并与原来生成的uid进行|或操作,使得生成的UID bit位1和2为01，达到确定生成十进制数为19位的目的 00 [000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00][00 0000 0000][0000 0000 000] | 01 000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00 00 0000 0000 0000 0000 000 = 01 [000 0000 0000 0000 0000 0000 0000 0000 0000 0000 00][00 0000 0000][0000 0000 000] 参考  http://www.wolfbe.com/detail/201701/386.html http://www.wolfbe.com/detail/201611/381.html https://segmentfault.com/a/1190000011282426 "
            }
    
        ,
            {
                "id": 65,
                "href": "https://chinalhr.github.io/post/mysql-paging/",
                "title": "MySql大数据量表分页",
                "section": "post",
                "date" : "2018.07.19",
                "body": " MySql分页相关技巧整理\n Limit不适用于大数量表的分页查询 select * from table limit 3000000,10; //mysql会读取300w+10条数据，再取最后的10条数据  大数据量表的分页方式 方式一  不允许查看靠后的数据,例如百度  方式二   使用id进行分页操作，在查询下一页时把上一页的最后一个id传给服务器(客户端无页码分页查询)\nselect * from table where id\u0026gt;lastid limit 10;\n  针对自增的id并且中间没有删除和断点\nselect * from table where id\u0026gt;420*10 limit 10; //查询第420页的数据\n  方式三   延迟关联，利用对id进行limit查询有索引的优势，先查询对应分页的id值再对查询的id进行关联查询\nSELECT d.* FROM dynamic d INNER JOIN (SELECT id FROM dynamic LIMIT 100000,10) tem on d.id = tem.id\n "
            }
    
        ,
            {
                "id": 66,
                "href": "https://chinalhr.github.io/post/java-supplier-predicate-consumer/",
                "title": "函数式接口-Supplier|Predicate|Consumer|Function",
                "section": "post",
                "date" : "2018.07.11",
                "body": " Java函数式接口相关记录\n Supplier  Supplier接口没有入参，通过调用T get()返回一个T类型的对象 开发中通过supplier实时获取配置(配置使用json写在数据库中)  //设置Supplier获取函数为从数据库获取 \tzSetRedisConsumer.setBatchProcessSize(() -\u0026gt; hkConfigService.findRedisBuffConfig().getBatchProcessSize()); private Supplier\u0026lt;Long\u0026gt; fixDelayMilliseconds; @Override public RedisConsumer setFixDelayMilliseconds(Supplier\u0026lt;Long\u0026gt; fixDelayMilliseconds) { this.fixDelayMilliseconds = fixDelayMilliseconds; return this; } //通过调用batchProcessSize.get()实时获取配置  Predicate  Predicate函数式接口的主要作用就是提供一个boolean test(T t)方法(断言)，接受一个参数返回一个布尔类型 开发中使用Predicate进行StreamFilter  public class PredicateDemo { public static void main(String[] args) { List\u0026lt;Integer\u0026gt; list = Arrays.asList(...); PredicateTest predicateTest = new PredicateTest(); //输出大于5的数字  List\u0026lt;Integer\u0026gt; result = predicateTest.conditionFilter(list, integer -\u0026gt; integer \u0026gt; 5); result.forEach(System.out::println); } //抽象方法  public List\u0026lt;Integer\u0026gt; conditionFilter(List\u0026lt;Integer\u0026gt; list, Predicate\u0026lt;Integer\u0026gt; predicate){ return list.stream().filter(predicate).collect(Collectors.toList()); } } Consumer  Consumer函数式接口的主要作用就是提供一个void accept(T t)方法(进行消费)，执行带有副作用的操作。forEach源码使用的Consumer接口。  Consumer\u0026lt;String\u0026gt; printString = s -\u0026gt; System.out.println(s); printString.accept(\u0026#34;helloWorld!\u0026#34;); //控制台输出 helloWorld! Function  Function函数式接口主要作用是提供一个R apply(T t)方法(进行计算并返回结果)  System.out.println(validInput(name, inputStr -\u0026gt; inputStr.isEmpty() ? \u0026#34;名字不能为空\u0026#34;:inputStr)); System.out.println(validInput(name1, inputStr -\u0026gt; inputStr.length() \u0026gt; 3 ? \u0026#34;名字过长\u0026#34;:inputStr)); //抽象方法 public static String validInput(String name,Function\u0026lt;String,String\u0026gt; function) { return function.apply(name); } "
            }
    
        ,
            {
                "id": 67,
                "href": "https://chinalhr.github.io/post/mysql-alter/",
                "title": "Mysql对存有大数据表进行结构修改需要注意的问题",
                "section": "post",
                "date" : "2018.07.08",
                "body": " 记录生产中遇到的问题\n MySql对大数据表进行alter操作导致的问题 Mysql执行DDL直接修改表结构的过程中可能会锁表，导致无法写数据，出现生产事故。\nMySql各版本执行DDL方式  Copy Table(5.5之前)：通过临时表拷贝的方式实现的:新建一个带有新结构的临时表，将原表数据全部拷贝到临时表，然后Rename。【过程原表可读不可写】 Inplace(5.5)：直接在原表上执行DDL，但仅支持添加、删除索引两种方式。【过程原表可读不可写】 Online(5.6)：通过全量+增量的方式实现，直接在原表上执行DDL。  【如添加普通列|不存在全文索引时可读可写】 【修改列类型DDL|添加auto_increment列|修改字符集|存在全文索引时可读不可写】 【存在慢SQL或者较大的结果集的SQL在运行|存在一个事务在操作表可读不可写】    详细参见：https://www.cnblogs.com/mysql-dba/p/6192897.htmlhttp://www.cnblogs.com/cchust/p/4639397.html\n方案1：创建新表进行alter并复制数据 [选择在凌晨3-4时更新]\n 首先创建新的临时表，表结构通过命令ALTAR TABLE新定义的结构,索引 然后把原表中数据导入到临时表  记录最后一条更新数据的索引,统计更新数量 使用脚本对之前的数据进行小数据批量分批复制到临时表（走task或者脚本，记得复制id） 更新完成,对之前更新数据索引后增加的数据进行复制(transaction)   删除原表 最后把临时表重命名为原来的表名  实践代码： -- 1. 要修改结构的大数据表 DROP TABLE IF EXISTS USER; CREATE TABLE `user` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `username` VARCHAR (32) NOT NULL DEFAULT '', `age` INT (11) NOT NULL DEFAULT '-1', PRIMARY KEY (`id`) ) ENGINE = INNODB DEFAULT CHARSET = utf8mb4 COMMENT = '视频记录表'; -- 2. 新表结构 DROP TABLE IF EXISTS user_tmp; CREATE TABLE `user_tmp` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `username` VARCHAR (32) NOT NULL DEFAULT '', `age` INT (11) NOT NULL DEFAULT '-1', PRIMARY KEY (`id`), INDEX `index_username_age` (`username`, `age`) ) ENGINE = INNODB DEFAULT CHARSET = utf8mb4 COMMENT = '视频记录表'; -- 3.创建存储过程 max_id为 去最近更新的一条数据的id 11265265 DELIMITER $$ DROP PROCEDURE IF EXISTS copy_data ; CREATE PROCEDURE copy_data () BEGIN DECLARE limitSize BIGINT ; DECLARE beginId BIGINT ; DECLARE endId BIGINT ; DECLARE maxId BIGINT ; SET limitSize = 5000 ; SET beginId = 0 ; SET endId = limitSize ; SET maxId = 11265265 ; WHILE beginId \u0026lt; maxId DO IF endId \u0026gt; maxId THEN SET endId = maxId ; END IF ; INSERT INTO user_tmp (`id`, `username`, `age`) SELECT `id`, `username`, `age` FROM USER WHERE id \u0026gt;= beginId AND id \u0026lt; endId ; SET beginId = endId ; SET endId = endId + limitSize ; END WHILE ; END ;$$ DELIMITER ; -- 4.执行存储过程 CALL copy_data (); -- 5. 同步剩下的数据，并修改表名称，将临时表修改为新表 START TRANSACTION; INSERT INTO user_tmp (`id`, `username`, `age`) SELECT `id`, `username`, `age` FROM USER WHERE id \u0026gt;= 1265265; ALTER TABLE USER RENAME user_old; ALTER TABLE user_tmp RENAME USER; COMMIT;  方案2：新建一个表与旧表进行字段关联 略\u0026hellip;\n注意点 如果项目使用了Hibernate，需要关闭hibernate ddl(删掉hibernate.hbm2ddl.auto)。hibernate.cfg.xml 中hibernate.hbm2ddl.auto配置节点：\n\u0026lt;property name=\u0026quot;hibernate.hbm2ddl.auto\u0026quot; value=\u0026quot;create\u0026quot; /\u0026gt;   hibernate.hbm2ddl.auto参数的作用主要用于：自动创建|更新|验证数据库表结构\n create  每次加载hibernate时都会删除上一次的生成的表，然后根据你的model类再重新来生成新表\n create-drop  每次加载hibernate时根据model类生成表，但是sessionFactory一关闭,表就自动删除。\n update  第一次加载hibernate时根据model类会自动建立起表的结构（前提是先建立好数据库），以后加载hibernate时根据 model类自动更新表结构，即使表结构改变了但表中的行仍然存在不会删除以前的行。\n validate  每次加载hibernate时，验证创建数据库表结构，只会和数据库中的表进行比较，不会创建新表，但是会插入新值。\n "
            }
    
        ,
            {
                "id": 68,
                "href": "https://chinalhr.github.io/post/git-basic/",
                "title": "Git记录",
                "section": "post",
                "date" : "2018.07.08",
                "body": " 记录Git操作\n  git 基础操作  ## git创建分支并提交到远程 git branch -r [命令查看远端库的分支情况] git checkout -b name [从已有的分支创建名为name的分支] git push origin name [推送新的本地分支到远程] ## git回退commit版本 git reset --hard HEAD^ [回退到上一个版本] git reset --hard HEAD^^ [回退到上上个版本] git reset --hard HEAD~100 [回退到上100个版本] git reset --hard commitID [回退到指定commit版本] ## git恢复commit版本 git reflog [恢复到之前commit的版本] ## pull远程分支到本地 git pull origin master [更新master分支代码] ## 更新远程分支列表到本地 git remote update origin --prune git remote update origin --p ## 将远程仓库所有分支的最新版本全部取回到本地 git fetch \u0026lt;远程仓库的别名\u0026gt; ## IntelliJ IDEA 对比当前分支代码与远程master分支代码 IDEA右下角选择当前分支，点击Remote Branches下的origin/master,选择Comparing With进行对比 ## IntelliJ IDEA Merge master到当前分支 IDEA右下角选择当前分支，点击Remote Branches下的origin/master,选择 Merge into Current ## 合并操作 add与commit合并：git commit -am \u0026#34;合并提交\u0026#34; ## 删除远程分支\u0026amp;本地分支 git branch -r -d origin/branch-name git push origin :branch-name ## 切换远程分支 git checkout -b dev origin/dev ## git清空修改（未使用add添加到暂存区） git checkout . ## git清空新增文件（未使用add添加到暂存区） rm -rf *** ## git重置命令（清空掉已经add到暂存区的修改） git reset . ## git关联远程仓库 git remote add origin git@github.com:***/***.git ## git退出merge、rebase\tgit rebase --abort git merge --abort  git submodule 操作  ## 添加子仓库,添加成功后，在父仓库根目录增加了.gitmodule文件,并且在父仓库的git 配置文件中加入了submodule段 git submodule add \u0026lt;仓库地址\u0026gt; \u0026lt;本地路径\u0026gt; # 更新子仓库，克隆一个包含子仓库的仓库目录，并不会clone下子仓库的文件，只是会克隆下.gitmodule描述文件，需要进一步克隆子仓库文件 ## 初始化本地配置文件 git submodule init ## 检出父仓库列出的commit git submodule update ## 更新子仓库head，在子仓库目录checkout到指定分支，pull更新代码后 到主仓库目录 commit提交，push更新代码 git checkout xx \u0026amp;\u0026amp; git pull origin xx "
            }
    
        ,
            {
                "id": 69,
                "href": "https://chinalhr.github.io/post/mysql-proxy/",
                "title": "Mysql主从复制-读写分离",
                "section": "post",
                "date" : "2018.06.27",
                "body": " Mysql主从复制，读写分离相关资料整理记录\n 主从复制 解决的问题  数据分布 负载均衡 备份|故障切换  复制工作原理  从主库上把数据更新到二进制日志(Binary Log)中。 备库将主库上的日志复制到自己的中继日志(Relay Log)中。 备库读取中继日志中的事件，重放到备库数据之上。  架构优点:实现了获取事件和重放事件的解耦，两个过程异步执行，I/O线程能够独立于SQL线程之外工作。\nmysql支持的复制类型  基于语句的复制(在服务器上执行sql语句，在从服务器上执行同样的语句)[Default] 基于行的复制(把改变的内容复制过去) 混合类型的复制(默认采用基于语句的复制，一旦发现基于语句无法精确复制时，就会采用基于行的复制)  复制用到的文件  二进制日志文件，中继日志文件 mysql-bin.index:用于记录磁盘上的二进制日志文件的文件名 mysql-relay-bin-index：用于记录磁盘上的中继日志文件的文件名 master.info:保存备库连接到主库所需要的信息 relay-log.info:保存当前备库复制的二进制日志和中继日志坐标  配置步骤  在每台服务器上创建复制账号 配置主库和备库 通知备库连接到主库并从主库复制数据  让备库变成其他服务器的主库 log_slave_updates选项可以让备库变成其他服务器的主库\n复制拓扑   一主库多备库\n一台主库对应堕胎备库 适用场景：少量写，大量读。\n  主-主复制(主动模式)\n两台服务器，每一台都配置成对方的主库和备库。 适用场景：特殊目的，需要双写场景，会产生冲突。\n  主-主复制(被动模式)\n两台服务器，每一台都配置成对方的主库和备库，配置其中一台为只读的被动服务器。 解决了主-主复制冲突的问题\n  拥有备库的主-主复制\n  环形复制\n三个或者更多的主库，每个服务器都是他之前的服务器的备库，之后的服务器的主库。\n  读写分离 关于读写分离 读写分离，基本的原理是让主数据库处理事务性增、改、删操作（INSERT、UPDATE、DELETE），而从数据库处理SELECT查询操作。数据库复制被用来把事务性操作导致的变更同步到集群中的从数据库。\n原因 避免对数据库的写入影响了查询的效率。\n场景 如果程序使用数据库较多时，而更新少，查询多的情况下会考虑使用，利用数据库 主从同步 。可以减少数据库压力，提高性能。\n读写分离与主从复制的关系 通过对数据库进行主从复制的方式来同步数据，实现对主数据进行写操作，对从数据库进行读操作，达到读写分离的目的。\n实现 读写分离就是在主服务器上修改，数据会同步到从服务器，从服务器只能提供读取数据，不能写入，实现备份的同时也实现了数据库性能的优化，以及提升了服务器安全。\n 基于程序代码内部控制,在代码中根据select 、insert进行路由分类 基于中间代理层实现,代理数据库服务器接收到应用服务器的请求后根据判断后转发到后端数据库(mysql_proxy|Atlas|Amoeba)  "
            }
    
        ,
            {
                "id": 70,
                "href": "https://chinalhr.github.io/post/java-system-structure/",
                "title": "Java项目中的系统结构分层",
                "section": "post",
                "date" : "2018.06.25",
                "body": " 公司Java项目中的系统结构分层\n 分层结构  common 公用组件，包括config,exception,filter dao 管理数据源, 包括mysql,redis(Dao接口，Mapper) service 操作实体数据, 严禁service间相互调用 biz 业务层, 向下调用service，一般一个biz向下调用多个Service task 异步调用，定时任务层(Mq,Task) web web入口  领域模型  dao层对象：xxxDO，xxx即为数据表名。查询对象一般用Param后缀 service/biz层对象：xxxDTO，xxx为业务领域相关的名称。 查询对象一般用 Req后缀 web对象：xxxVO，xxx一般为网页名称。 "
            }
    
        ,
            {
                "id": 71,
                "href": "https://chinalhr.github.io/post/java-springboot-component/",
                "title": "Spring Boot组件-Retry|Scheduled",
                "section": "post",
                "date" : "2018.06.24",
                "body": " Spring Retry与Scheduled使用记录\n Spring Boot Retry(重试机制) 配置 pom文件中添加依赖\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.retry\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-retry\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectjweaver\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;  在Application类中增加@EnableRetry注解启用\n使用 在需要重试的方法上面加入@Retryable注解\n @Retryable(value = Exception.class,maxAttempts = 3,backoff = @Backoff(delay = 2000,multiplier = 1.5)) 1.value：哪些异常出现的时候触发重试 2.maxAttempts：最大重试次数 3.delay：重试延迟时间 4.multiplier：上一次延时时间是这一次的倍数（1.5 第一次2s，第二次3s...）  重试到最后一次失败的时候会抛出异常或者执行在同一个类里@Recover注解了的回调方法\n@Recover public int recover(Exception e){ //执行回调 }  Spring Boot Scheduled(定时任务) 配置 在Application类中增加@EnableScheduling注解启用\n使用 在需要定时调用的方法上面加入@Scheduled注解\n/** * 使用cron表达式实现每5秒打印一次 */ @Scheduled(cron=\u0026quot;0/5 * * * * ? \u0026quot;) public void cronPrintTask(){ System.out.println(\u0026quot;cronPrintTask ：\u0026quot;+new Date(System.currentTimeMillis())); } /** * fixedRate含义是上一个调用开始后再次调用的延时（不用等待上一次调用完成） * 这样就会存在重复执行的问题与时间不准确问题 */ @Scheduled(fixedRate = 1000 * 1) public void fixedRatePrintTask(){ //执行方法 } /** * fixedDelay与fixedRate则是相反的， * 配置了该属性后会等到方法执行完成后延迟配置的时间再次执行该方法 */ @Scheduled(fixedDelay = 1000 * 1) public void fixedDelay() throws InterruptedException { //执行方法 } /** * initialDelay表示第一次执行延迟时间，只是做延迟的设定，并不会控制其他逻辑 * 需要配合fixedDelay... */ @Scheduled(initialDelay = 1000 * 10,fixedDelay = 1000 * 1) public void initialDelayPrintTask(){ //执行方法 }  cron表达式 参考http://cron.qqe2.com/\n"
            }
    
        ,
            {
                "id": 72,
                "href": "https://chinalhr.github.io/post/java-practice/",
                "title": "基于SSM项目的个人编码规范",
                "section": "post",
                "date" : "2018.02.19",
                "body": " 仅记录一个多月野蛮生长过程的个人编码规范\n API接口定义  统一返回格式为一个ResultBean，使用状态码对请求的状态做出响应 ResultBean为Controller专用，不能由Service层或者Dao层上传 在Controller层进行aop切面，（或者使用行为参数化传递代码并用lamdba进行内部类的简化书写）对业务进行统一处理业务层异常捕获处理并返回，减少Controller的try-catch代码块统一的日志打印  异常处理  在service层尽量减少不必要的异常捕获与null的判断，可以抛出异常到Controller层，在AOP切面中进行统一处理 对具体业务异常进行统一定义，继承RuntimeException  日志处理  使用门面模式的日志框架：SLF4J  Dao与Service的抽象  使用模板方法对简单操作CRUD进行抽象到统一接口或者父类中，减少代码冗余  MyBatis插件  使用通用Mapper3简化单表CRUD操作 使用PageHelper简化MyBatis的翻页查询  JDK8 Optional优雅处理NullPointerException  of  of方法通过工厂方法创建Optional实例，如果传入的值为null，会抛出异常。\n// 给与一个非空值 Optional\u0026lt;String\u0026gt; username = Optional.of(\u0026#34;lhr\u0026#34;);  ofNullable  与of类似，但如果传入的值为null，则返回一个空的Optional。\nOptional empty = Optional.ofNullable(null);  isPresent  如果Optional值存在返回true,否则返回false\nif (user.isPresent()) { *** }  get  如果Optional有值则将其返回，否则抛出NoSuchElementException。\n ifPresent  如果Optional实例有值则为其调用consumer ,否则不做处理。\nuser.ifPresent((value) -\u0026gt; { *** });  orElse  如果有值则将其返回，否则返回指定的其它值。\nSystem.out.println(user.orElse(new User(\u0026#34;zhangsan\u0026#34;)));  orElseGet  orElseGet与orElse方法类似，区别在于得到的默认值。orElse传入的是默认值，orElseGet可以接受一个Supplier生成默认值。\nSystem.out.println(user.orElseGet(() -\u0026gt; new User(\u0026#34;zhangsan\u0026#34;)));  orElseThrow  如果有值则将其返回，否则抛出supplier接口创建的异常。\n使用Futurn/CompletableFuture在并发的层面充分利用单一CPU，避免等待远程服务返回而浪费资源  在运行不需要立即获取结果的操作时，使用Future进行异步操作 尽量使用可超时的get获取,避免无限阻塞等待的情况 "
            }
    
]
